[('linux-2.6.23.orig/Documentation/sched-design-CFS.txt',
  'linux-2.6.23/Documentation/sched-design-CFS.txt',
  'Index: linux-2.6.23/Documentation/sched-design-CFS.txt\n===================================================================\n--- linux-2.6.23.orig/Documentation/sched-design-CFS.txt\n+++ linux-2.6.23/Documentation/sched-design-CFS.txt\n@@ -117,3 +117,70 @@ Some implementation details:\niterators of the scheduling modules are used. The balancing code got\nquite a bit simpler as a result.\n\n+\n+Group scheduler extension to CFS\n+================================\n+\n+Normally the scheduler operates on individual tasks and strives to provide\n+fair CPU time to each task. Sometimes, it may be desirable to group tasks\n+and provide fair CPU time to each such task group. For example, it may\n+be desirable to first provide fair CPU time to each user on the system\n+and then to each task belonging to a user.\n+\n+CONFIG_FAIR_GROUP_SCHED strives to achieve exactly that. It lets\n+SCHED_NORMAL/BATCH tasks be be grouped and divides CPU time fairly among such\n+groups. At present, there are two (mutually exclusive) mechanisms to group\n+tasks for CPU bandwidth control purpose:\n+\n+\t- Based on user id (CONFIG_FAIR_USER_SCHED)\n+\t\tIn this option, tasks are grouped according to their user id.\n+\t- Based on "cgroup" pseudo filesystem (CONFIG_FAIR_CGROUP_SCHED)\n+\t\tThis options lets the administrator create arbitrary groups\n+\t\tof tasks, using the "cgroup" pseudo filesystem. See\n+\t\tDocumentation/cgroups.txt for more information about this\n+\t\tfilesystem.\n+\n+Only one of these options to group tasks can be chosen and not both.\n+\n+Group scheduler tunables:\n+\n+When CONFIG_FAIR_USER_SCHED is defined, a directory is created in sysfs for\n+each new user and a "cpu_share" file is added in that directory.\n+\n+\t# cd /sys/kernel/uids\n+\t# cat 512/cpu_share\t\t# Display user 512\'s CPU share\n+\t1024\n+\t# echo 2048 > 512/cpu_share\t# Modify user 512\'s CPU share\n+\t# cat 512/cpu_share\t\t# Display user 512\'s CPU share\n+\t2048\n+\t#\n+\n+CPU bandwidth between two users are divided in the ratio of their CPU shares.\n+For ex: if you would like user "root" to get twice the bandwidth of user\n+"guest", then set the cpu_share for both the users such that "root"\'s\n+cpu_share is twice "guest"\'s cpu_share\n+\n+\n+When CONFIG_FAIR_CGROUP_SCHED is defined, a "cpu.shares" file is created\n+for each group created using the pseudo filesystem. See example steps\n+below to create task groups and modify their CPU share using the "cgroups"\n+pseudo filesystem\n+\n+\t# mkdir /dev/cpuctl\n+\t# mount -t cgroup -ocpu none /dev/cpuctl\n+\t# cd /dev/cpuctl\n+\n+\t# mkdir multimedia\t# create "multimedia" group of tasks\n+\t# mkdir browser\t\t# create "browser" group of tasks\n+\n+\t# #Configure the multimedia group to receive twice the CPU bandwidth\n+\t# #that of browser group\n+\n+\t# echo 2048 > multimedia/cpu.shares\n+\t# echo 1024 > browser/cpu.shares\n+\n+\t# firefox &\t# Launch firefox and move it to "browser" group\n+\t# echo <firefox_pid> > browser/tasks\n+\n+\t# #Launch gmplayer (or your favourite movie player)\n+\t# echo <movie_player_pid> > multimedia/tasks'),
 ('linux-2.6.23.orig/arch/i386/Kconfig',
  'linux-2.6.23/arch/i386/Kconfig',
  'Index: linux-2.6.23/arch/i386/Kconfig\n===================================================================\n--- linux-2.6.23.orig/arch/i386/Kconfig\n+++ linux-2.6.23/arch/i386/Kconfig\n@@ -214,6 +214,17 @@ config X86_ES7000\n\nendchoice\n\n+config SCHED_NO_NO_OMIT_FRAME_POINTER\n+\tbool "Single-depth WCHAN output"\n+\tdefault y\n+\thelp\n+\t  Calculate simpler /proc/<PID>/wchan values. If this option\n+\t  is disabled then wchan values will recurse back to the\n+\t  caller function. This provides more accurate wchan values,\n+\t  at the expense of slightly more scheduling overhead.\n+\n+\t  If in doubt, say "Y".\n+\nconfig PARAVIRT\nbool "Paravirtualization support (EXPERIMENTAL)"\ndepends on EXPERIMENTAL'),
 ('linux-2.6.23.orig/drivers/kvm/kvm.h',
  'linux-2.6.23/drivers/kvm/kvm.h',
  'Index: linux-2.6.23/drivers/kvm/kvm.h\n===================================================================\n--- linux-2.6.23.orig/drivers/kvm/kvm.h\n+++ linux-2.6.23/drivers/kvm/kvm.h\n@@ -625,6 +625,16 @@ void kvm_mmu_unload(struct kvm_vcpu *vcp\n\nint kvm_hypercall(struct kvm_vcpu *vcpu, struct kvm_run *run);\n\n+static inline void kvm_guest_enter(void)\n+{\n+\tcurrent->flags |= PF_VCPU;\n+}\n+\n+static inline void kvm_guest_exit(void)\n+{\n+\tcurrent->flags &= ~PF_VCPU;\n+}\n+\nstatic inline int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva,\nu32 error_code)\n{'),
 ('linux-2.6.23.orig/fs/pipe.c',
  'linux-2.6.23/fs/pipe.c',
  'Index: linux-2.6.23/fs/pipe.c\n===================================================================\n--- linux-2.6.23.orig/fs/pipe.c\n+++ linux-2.6.23/fs/pipe.c\n@@ -45,8 +45,7 @@ void pipe_wait(struct pipe_inode_info *p\n* Pipes are system-local resources, so sleeping on them\n* is considered a noninteractive wait:\n*/\n-\tprepare_to_wait(&pipe->wait, &wait,\n-\t\t\tTASK_INTERRUPTIBLE | TASK_NONINTERACTIVE);\n+\tprepare_to_wait(&pipe->wait, &wait, TASK_INTERRUPTIBLE);\nif (pipe->inode)\nmutex_unlock(&pipe->inode->i_mutex);\nschedule();\n@@ -383,7 +382,7 @@ redo:\n\n/* Signal writers asynchronously that there is more room. */\nif (do_wakeup) {\n-\t\twake_up_interruptible(&pipe->wait);\n+\t\twake_up_interruptible_sync(&pipe->wait);\nkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n}\nif (ret > 0)\n@@ -556,7 +555,7 @@ redo2:\nout:\nmutex_unlock(&inode->i_mutex);\nif (do_wakeup) {\n-\t\twake_up_interruptible(&pipe->wait);\n+\t\twake_up_interruptible_sync(&pipe->wait);\nkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n}\nif (ret > 0)\n@@ -650,7 +649,7 @@ pipe_release(struct inode *inode, int de\nif (!pipe->readers && !pipe->writers) {\nfree_pipe_info(inode);\n} else {\n-\t\twake_up_interruptible(&pipe->wait);\n+\t\twake_up_interruptible_sync(&pipe->wait);\nkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\nkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n}'),
 ('linux-2.6.23.orig/fs/proc/array.c',
  'linux-2.6.23/fs/proc/array.c',
  'Index: linux-2.6.23/fs/proc/array.c\n===================================================================\n--- linux-2.6.23.orig/fs/proc/array.c\n+++ linux-2.6.23/fs/proc/array.c\n@@ -367,11 +367,18 @@ static cputime_t task_stime(struct task_\nstime = nsec_to_clock_t(p->se.sum_exec_runtime) -\ncputime_to_clock_t(task_utime(p));\n\n-\tp->prev_stime = max(p->prev_stime, clock_t_to_cputime(stime));\n+\tif (stime >= 0)\n+\t\tp->prev_stime = max(p->prev_stime, clock_t_to_cputime(stime));\n+\nreturn p->prev_stime;\n}\n#endif\n\n+static cputime_t task_gtime(struct task_struct *p)\n+{\n+\treturn p->gtime;\n+}\n+\nstatic int do_task_stat(struct task_struct *task, char *buffer, int whole)\n{\nunsigned long vsize, eip, esp, wchan = ~0UL;\n@@ -387,6 +394,7 @@ static int do_task_stat(struct task_stru\nunsigned long cmin_flt = 0, cmaj_flt = 0;\nunsigned long  min_flt = 0,  maj_flt = 0;\ncputime_t cutime, cstime, utime, stime;\n+\tcputime_t cgtime, gtime;\nunsigned long rsslim = 0;\nchar tcomm[sizeof(task->comm)];\nunsigned long flags;\n@@ -405,6 +413,7 @@ static int do_task_stat(struct task_stru\nsigemptyset(&sigign);\nsigemptyset(&sigcatch);\ncutime = cstime = utime = stime = cputime_zero;\n+\tcgtime = gtime = cputime_zero;\n\nrcu_read_lock();\nif (lock_task_sighand(task, &flags)) {\n@@ -422,6 +431,7 @@ static int do_task_stat(struct task_stru\ncmaj_flt = sig->cmaj_flt;\ncutime = sig->cutime;\ncstime = sig->cstime;\n+\t\tcgtime = sig->cgtime;\nrsslim = sig->rlim[RLIMIT_RSS].rlim_cur;\n\n/* add up live thread stats at the group level */\n@@ -432,6 +442,7 @@ static int do_task_stat(struct task_stru\nmaj_flt += t->maj_flt;\nutime = cputime_add(utime, task_utime(t));\nstime = cputime_add(stime, task_stime(t));\n+\t\t\t\tgtime = cputime_add(gtime, task_gtime(t));\nt = next_thread(t);\n} while (t != task);\n\n@@ -439,6 +450,7 @@ static int do_task_stat(struct task_stru\nmaj_flt += sig->maj_flt;\nutime = cputime_add(utime, sig->utime);\nstime = cputime_add(stime, sig->stime);\n+\t\t\tgtime = cputime_add(gtime, sig->gtime);\n}\n\nsid = signal_session(sig);\n@@ -456,6 +468,7 @@ static int do_task_stat(struct task_stru\nmaj_flt = task->maj_flt;\nutime = task_utime(task);\nstime = task_stime(task);\n+\t\tgtime = task_gtime(task);\n}\n\n/* scale priority and nice values from timeslices to -20..20 */\n@@ -473,7 +486,7 @@ static int do_task_stat(struct task_stru\n\nres = sprintf(buffer, "%d (%s) %c %d %d %d %d %d %u %lu \\\n%lu %lu %lu %lu %lu %ld %ld %ld %ld %d 0 %llu %lu %ld %lu %lu %lu %lu %lu \\\n-%lu %lu %lu %lu %lu %lu %lu %lu %d %d %u %u %llu\\n",\n+%lu %lu %lu %lu %lu %lu %lu %lu %d %d %u %u %llu %lu %ld\\n",\ntask->pid,\ntcomm,\nstate,\n@@ -518,7 +531,9 @@ static int do_task_stat(struct task_stru\ntask_cpu(task),\ntask->rt_priority,\ntask->policy,\n-\t\t(unsigned long long)delayacct_blkio_ticks(task));\n+\t\t(unsigned long long)delayacct_blkio_ticks(task),\n+\t\tcputime_to_clock_t(gtime),\n+\t\tcputime_to_clock_t(cgtime));\nif (mm)\nmmput(mm);\nreturn res;'),
 ('linux-2.6.23.orig/fs/proc/base.c',
  'linux-2.6.23/fs/proc/base.c',
  'Index: linux-2.6.23/fs/proc/base.c\n===================================================================\n--- linux-2.6.23.orig/fs/proc/base.c\n+++ linux-2.6.23/fs/proc/base.c\n@@ -304,7 +304,7 @@ static int proc_pid_schedstat(struct tas\nreturn sprintf(buffer, "%llu %llu %lu\\n",\ntask->sched_info.cpu_time,\ntask->sched_info.run_delay,\n-\t\t\ttask->sched_info.pcnt);\n+\t\t\ttask->sched_info.pcount);\n}\n#endif\n'),
 ('linux-2.6.23.orig/fs/proc/proc_misc.c',
  'linux-2.6.23/fs/proc/proc_misc.c',
  'Index: linux-2.6.23/fs/proc/proc_misc.c\n===================================================================\n--- linux-2.6.23.orig/fs/proc/proc_misc.c\n+++ linux-2.6.23/fs/proc/proc_misc.c\n@@ -443,6 +443,7 @@ static int show_stat(struct seq_file *p,\nint i;\nunsigned long jif;\ncputime64_t user, nice, system, idle, iowait, irq, softirq, steal;\n+\tcputime64_t guest;\nu64 sum = 0;\nstruct timespec boottime;\nunsigned int *per_irq_sum;\n@@ -453,6 +454,7 @@ static int show_stat(struct seq_file *p,\n\nuser = nice = system = idle = iowait =\nirq = softirq = steal = cputime64_zero;\n+\tguest = cputime64_zero;\ngetboottime(&boottime);\njif = boottime.tv_sec;\n\n@@ -467,6 +469,7 @@ static int show_stat(struct seq_file *p,\nirq = cputime64_add(irq, kstat_cpu(i).cpustat.irq);\nsoftirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);\nsteal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);\n+\t\tguest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);\nfor (j = 0; j < NR_IRQS; j++) {\nunsigned int temp = kstat_cpu(i).irqs[j];\nsum += temp;\n@@ -474,7 +477,7 @@ static int show_stat(struct seq_file *p,\n}\n}\n\n-\tseq_printf(p, "cpu  %llu %llu %llu %llu %llu %llu %llu %llu\\n",\n+\tseq_printf(p, "cpu  %llu %llu %llu %llu %llu %llu %llu %llu %llu\\n",\n(unsigned long long)cputime64_to_clock_t(user),\n(unsigned long long)cputime64_to_clock_t(nice),\n(unsigned long long)cputime64_to_clock_t(system),\n@@ -482,7 +485,8 @@ static int show_stat(struct seq_file *p,\n(unsigned long long)cputime64_to_clock_t(iowait),\n(unsigned long long)cputime64_to_clock_t(irq),\n(unsigned long long)cputime64_to_clock_t(softirq),\n-\t\t(unsigned long long)cputime64_to_clock_t(steal));\n+\t\t(unsigned long long)cputime64_to_clock_t(steal),\n+\t\t(unsigned long long)cputime64_to_clock_t(guest));\nfor_each_online_cpu(i) {\n\n/* Copy values here to work around gcc-2.95.3, gcc-2.96 */\n@@ -494,7 +498,9 @@ static int show_stat(struct seq_file *p,\nirq = kstat_cpu(i).cpustat.irq;\nsoftirq = kstat_cpu(i).cpustat.softirq;\nsteal = kstat_cpu(i).cpustat.steal;\n-\t\tseq_printf(p, "cpu%d %llu %llu %llu %llu %llu %llu %llu %llu\\n",\n+\t\tguest = kstat_cpu(i).cpustat.guest;\n+\t\tseq_printf(p,\n+\t\t\t"cpu%d %llu %llu %llu %llu %llu %llu %llu %llu %llu\\n",\ni,\n(unsigned long long)cputime64_to_clock_t(user),\n(unsigned long long)cputime64_to_clock_t(nice),\n@@ -503,7 +509,8 @@ static int show_stat(struct seq_file *p,\n(unsigned long long)cputime64_to_clock_t(iowait),\n(unsigned long long)cputime64_to_clock_t(irq),\n(unsigned long long)cputime64_to_clock_t(softirq),\n-\t\t\t(unsigned long long)cputime64_to_clock_t(steal));\n+\t\t\t(unsigned long long)cputime64_to_clock_t(steal),\n+\t\t\t(unsigned long long)cputime64_to_clock_t(guest));\n}\nseq_printf(p, "intr %llu", (unsigned long long)sum);\n'),
 ('dev/null',
  'linux-2.6.23/include/linux/cgroup.h',
  'Index: linux-2.6.23/include/linux/cgroup.h\n===================================================================\n--- dev/null\n+++ linux-2.6.23/include/linux/cgroup.h\n@@ -0,0 +1,12 @@\n+#ifndef _LINUX_CGROUP_H\n+#define _LINUX_CGROUP_H\n+\n+/*\n+ * Control groups are not backported - we use a few compatibility\n+ * defines to be able to use the upstream sched.c as-is:\n+ */\n+#define task_pid_nr(task)\t\t(task)->pid\n+#define task_pid_vnr(task)\t\t(task)->pid\n+#define find_task_by_vpid(pid)\t\tfind_task_by_pid(pid)\n+\n+#endif'),
 ('linux-2.6.23.orig/include/linux/cpuset.h',
  'linux-2.6.23/include/linux/cpuset.h',
  'Index: linux-2.6.23/include/linux/cpuset.h\n===================================================================\n--- linux-2.6.23.orig/include/linux/cpuset.h\n+++ linux-2.6.23/include/linux/cpuset.h\n@@ -146,6 +146,11 @@ static inline int cpuset_do_slab_mem_spr\n\nstatic inline void cpuset_track_online_nodes(void) {}\n\n+static inline cpumask_t cpuset_cpus_allowed_locked(struct task_struct *p)\n+{\n+\treturn cpu_possible_map;\n+}\n+\n#endif /* !CONFIG_CPUSETS */\n\n#endif /* _LINUX_CPUSET_H */'),
 ('linux-2.6.23.orig/include/linux/kernel.h',
  'linux-2.6.23/include/linux/kernel.h',
  'Index: linux-2.6.23/include/linux/kernel.h\n===================================================================\n--- linux-2.6.23.orig/include/linux/kernel.h\n+++ linux-2.6.23/include/linux/kernel.h\n@@ -62,6 +62,13 @@ extern const char linux_proc_banner[];\n#define\tKERN_INFO\t"<6>"\t/* informational\t\t\t*/\n#define\tKERN_DEBUG\t"<7>"\t/* debug-level messages\t\t\t*/\n\n+/*\n+ * Annotation for a "continued" line of log printout (only done after a\n+ * line that had no enclosing \\n). Only to be used by core/arch code\n+ * during early bootup (a continued line is not SMP-safe otherwise).\n+ */\n+#define\tKERN_CONT\t""\n+\nextern int console_printk[];\n\n#define console_loglevel (console_printk[0])'),
 ('linux-2.6.23.orig/include/linux/kernel_stat.h',
  'linux-2.6.23/include/linux/kernel_stat.h',
  'Index: linux-2.6.23/include/linux/kernel_stat.h\n===================================================================\n--- linux-2.6.23.orig/include/linux/kernel_stat.h\n+++ linux-2.6.23/include/linux/kernel_stat.h\n@@ -23,6 +23,7 @@ struct cpu_usage_stat {\ncputime64_t idle;\ncputime64_t iowait;\ncputime64_t steal;\n+\tcputime64_t guest;\n};\n\nstruct kernel_stat {\n@@ -52,7 +53,9 @@ static inline int kstat_irqs(int irq)\n}\n\nextern void account_user_time(struct task_struct *, cputime_t);\n+extern void account_user_time_scaled(struct task_struct *, cputime_t);\nextern void account_system_time(struct task_struct *, int, cputime_t);\n+extern void account_system_time_scaled(struct task_struct *, cputime_t);\nextern void account_steal_time(struct task_struct *, cputime_t);\n\n#endif /* _LINUX_KERNEL_STAT_H */'),
 ('linux-2.6.23.orig/include/linux/nodemask.h',
  'linux-2.6.23/include/linux/nodemask.h',
  'Index: linux-2.6.23/include/linux/nodemask.h\n===================================================================\n--- linux-2.6.23.orig/include/linux/nodemask.h\n+++ linux-2.6.23/include/linux/nodemask.h\n@@ -338,31 +338,88 @@ static inline void __nodes_remap(nodemas\n#endif /* MAX_NUMNODES */\n\n/*\n+ * Bitmasks that are kept for all the nodes.\n+ */\n+enum node_states {\n+\tN_POSSIBLE,\t\t/* The node could become online at some point */\n+\tN_ONLINE,\t\t/* The node is online */\n+\tN_NORMAL_MEMORY,\t/* The node has regular memory */\n+#ifdef CONFIG_HIGHMEM\n+\tN_HIGH_MEMORY,\t\t/* The node has regular or high memory */\n+#else\n+\tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n+#endif\n+\tN_CPU,\t\t/* The node has one or more cpus */\n+\tNR_NODE_STATES\n+};\n+\n+/*\n* The following particular system nodemasks and operations\n* on them manage all possible and online nodes.\n*/\n\n-extern nodemask_t node_online_map;\n-extern nodemask_t node_possible_map;\n+extern nodemask_t node_states[NR_NODE_STATES];\n\n#if MAX_NUMNODES > 1\n-#define num_online_nodes()\tnodes_weight(node_online_map)\n-#define num_possible_nodes()\tnodes_weight(node_possible_map)\n-#define node_online(node)\tnode_isset((node), node_online_map)\n-#define node_possible(node)\tnode_isset((node), node_possible_map)\n-#define first_online_node\tfirst_node(node_online_map)\n-#define next_online_node(nid)\tnext_node((nid), node_online_map)\n+static inline int node_state(int node, enum node_states state)\n+{\n+\treturn node_isset(node, node_states[state]);\n+}\n+\n+static inline void node_set_state(int node, enum node_states state)\n+{\n+\t__node_set(node, &node_states[state]);\n+}\n+\n+static inline void node_clear_state(int node, enum node_states state)\n+{\n+\t__node_clear(node, &node_states[state]);\n+}\n+\n+static inline int num_node_state(enum node_states state)\n+{\n+\treturn nodes_weight(node_states[state]);\n+}\n+\n+#define for_each_node_state(__node, __state) \\\n+\tfor_each_node_mask((__node), node_states[__state])\n+\n+#define first_online_node\tfirst_node(node_states[N_ONLINE])\n+#define next_online_node(nid)\tnext_node((nid), node_states[N_ONLINE])\n+\nextern int nr_node_ids;\n#else\n-#define num_online_nodes()\t1\n-#define num_possible_nodes()\t1\n-#define node_online(node)\t((node) == 0)\n-#define node_possible(node)\t((node) == 0)\n+\n+static inline int node_state(int node, enum node_states state)\n+{\n+\treturn node == 0;\n+}\n+\n+static inline void node_set_state(int node, enum node_states state)\n+{\n+}\n+\n+static inline void node_clear_state(int node, enum node_states state)\n+{\n+}\n+\n+static inline int num_node_state(enum node_states state)\n+{\n+\treturn 1;\n+}\n+\n+#define for_each_node_state(node, __state) \\\n+\tfor ( (node) = 0; (node) == 0; (node) = 1)\n+\n#define first_online_node\t0\n#define next_online_node(nid)\t(MAX_NUMNODES)\n#define nr_node_ids\t\t1\n+\n#endif\n\n+#define node_online_map \tnode_states[N_ONLINE]\n+#define node_possible_map \tnode_states[N_POSSIBLE]\n+\n#define any_online_node(mask)\t\t\t\\\n({\t\t\t\t\t\t\\\nint node;\t\t\t\t\\\n@@ -372,10 +429,15 @@ extern int nr_node_ids;\nnode;\t\t\t\t\t\\\n})\n\n-#define node_set_online(node)\t   set_bit((node), node_online_map.bits)\n-#define node_set_offline(node)\t   clear_bit((node), node_online_map.bits)\n+#define num_online_nodes()\tnum_node_state(N_ONLINE)\n+#define num_possible_nodes()\tnum_node_state(N_POSSIBLE)\n+#define node_online(node)\tnode_state((node), N_ONLINE)\n+#define node_possible(node)\tnode_state((node), N_POSSIBLE)\n+\n+#define node_set_online(node)\t   node_set_state((node), N_ONLINE)\n+#define node_set_offline(node)\t   node_clear_state((node), N_ONLINE)\n\n-#define for_each_node(node)\t   for_each_node_mask((node), node_possible_map)\n-#define for_each_online_node(node) for_each_node_mask((node), node_online_map)\n+#define for_each_node(node)\t   for_each_node_state(node, N_POSSIBLE)\n+#define for_each_online_node(node) for_each_node_state(node, N_ONLINE)\n\n#endif /* __LINUX_NODEMASK_H */'),
 ('linux-2.6.23.orig/include/linux/sched.h',
  'linux-2.6.23/include/linux/sched.h',
  'Index: linux-2.6.23/include/linux/sched.h\n===================================================================\n--- linux-2.6.23.orig/include/linux/sched.h\n+++ linux-2.6.23/include/linux/sched.h\n@@ -3,6 +3,17 @@\n\n#include <linux/auxvec.h>\t/* For AT_VECTOR_SIZE */\n\n+/* backporting helper macro: */\n+#define cpu_sibling_map(cpu) cpu_sibling_map[cpu]\n+\n+/*\n+ *  * Control groups are not backported - we use a few compatibility\n+ *   * defines to be able to use the upstream sched.c as-is:\n+ *    */\n+#define task_pid_nr(task)               (task)->pid\n+#define task_pid_vnr(task)              (task)->pid\n+#define find_task_by_vpid(pid)          find_task_by_pid(pid)\n+\n/*\n* cloning flags:\n*/\n@@ -86,6 +97,7 @@ struct sched_param {\n#include <linux/timer.h>\n#include <linux/hrtimer.h>\n#include <linux/task_io_accounting.h>\n+#include <linux/kobject.h>\n\n#include <asm/processor.h>\n\n@@ -135,6 +147,7 @@ extern unsigned long weighted_cpuload(co\n\nstruct seq_file;\nstruct cfs_rq;\n+struct task_group;\n#ifdef CONFIG_SCHED_DEBUG\nextern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);\nextern void proc_sched_set_task(struct task_struct *p);\n@@ -173,8 +186,7 @@ print_cfs_rq(struct seq_file *m, int cpu\n#define EXIT_ZOMBIE\t\t16\n#define EXIT_DEAD\t\t32\n/* in tsk->state again */\n-#define TASK_NONINTERACTIVE\t64\n-#define TASK_DEAD\t\t128\n+#define TASK_DEAD\t\t64\n\n#define __set_task_state(tsk, state_value)\t\t\\\ndo { (tsk)->state = (state_value); } while (0)\n@@ -278,6 +290,10 @@ static inline void touch_all_softlockup_\n\n/* Attach to any functions which should be ignored in wchan output. */\n#define __sched\t\t__attribute__((__section__(".sched.text")))\n+\n+/* Linker adds these: start and end of __sched functions */\n+extern char __sched_text_start[], __sched_text_end[];\n+\n/* Is this address in the __sched functions? */\nextern int in_sched_functions(unsigned long addr);\n\n@@ -515,6 +531,8 @@ struct signal_struct {\n* in __exit_signal, except for the group leader.\n*/\ncputime_t utime, stime, cutime, cstime;\n+\tcputime_t gtime;\n+\tcputime_t cgtime;\nunsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;\nunsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;\nunsigned long inblock, oublock, cinblock, coublock;\n@@ -595,8 +613,23 @@ struct user_struct {\n/* Hash table maintenance information */\nstruct hlist_node uidhash_node;\nuid_t uid;\n+\n+#ifdef CONFIG_FAIR_USER_SCHED\n+\tstruct task_group *tg;\n+#ifdef CONFIG_SYSFS\n+\tstruct kset kset;\n+\tstruct subsys_attribute user_attr;\n+\tstruct work_struct work;\n+#endif\n+#endif\n};\n\n+#ifdef CONFIG_FAIR_USER_SCHED\n+extern int uids_kobject_init(void);\n+#else\n+static inline int uids_kobject_init(void) { return 0; }\n+#endif\n+\nextern struct user_struct *find_user(uid_t);\n\nextern struct user_struct root_user;\n@@ -608,13 +641,17 @@ struct reclaim_state;\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\nstruct sched_info {\n/* cumulative counters */\n-\tunsigned long pcnt;\t      /* # of times run on this cpu */\n+\tunsigned long pcount;\t      /* # of times run on this cpu */\nunsigned long long cpu_time,  /* time spent on the cpu */\nrun_delay; /* time spent waiting on a runqueue */\n\n/* timestamps */\nunsigned long long last_arrival,/* when we last ran on a cpu */\nlast_queued;\t/* when we were last queued to run */\n+#ifdef CONFIG_SCHEDSTATS\n+\t/* BKL stats */\n+\tunsigned int bkl_count;\n+#endif\n};\n#endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */\n\n@@ -749,39 +786,38 @@ struct sched_domain {\n\n#ifdef CONFIG_SCHEDSTATS\n/* load_balance() stats */\n-\tunsigned long lb_cnt[CPU_MAX_IDLE_TYPES];\n-\tunsigned long lb_failed[CPU_MAX_IDLE_TYPES];\n-\tunsigned long lb_balanced[CPU_MAX_IDLE_TYPES];\n-\tunsigned long lb_imbalance[CPU_MAX_IDLE_TYPES];\n-\tunsigned long lb_gained[CPU_MAX_IDLE_TYPES];\n-\tunsigned long lb_hot_gained[CPU_MAX_IDLE_TYPES];\n-\tunsigned long lb_nobusyg[CPU_MAX_IDLE_TYPES];\n-\tunsigned long lb_nobusyq[CPU_MAX_IDLE_TYPES];\n+\tunsigned int lb_count[CPU_MAX_IDLE_TYPES];\n+\tunsigned int lb_failed[CPU_MAX_IDLE_TYPES];\n+\tunsigned int lb_balanced[CPU_MAX_IDLE_TYPES];\n+\tunsigned int lb_imbalance[CPU_MAX_IDLE_TYPES];\n+\tunsigned int lb_gained[CPU_MAX_IDLE_TYPES];\n+\tunsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];\n+\tunsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];\n+\tunsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];\n\n/* Active load balancing */\n-\tunsigned long alb_cnt;\n-\tunsigned long alb_failed;\n-\tunsigned long alb_pushed;\n+\tunsigned int alb_count;\n+\tunsigned int alb_failed;\n+\tunsigned int alb_pushed;\n\n/* SD_BALANCE_EXEC stats */\n-\tunsigned long sbe_cnt;\n-\tunsigned long sbe_balanced;\n-\tunsigned long sbe_pushed;\n+\tunsigned int sbe_count;\n+\tunsigned int sbe_balanced;\n+\tunsigned int sbe_pushed;\n\n/* SD_BALANCE_FORK stats */\n-\tunsigned long sbf_cnt;\n-\tunsigned long sbf_balanced;\n-\tunsigned long sbf_pushed;\n+\tunsigned int sbf_count;\n+\tunsigned int sbf_balanced;\n+\tunsigned int sbf_pushed;\n\n/* try_to_wake_up() stats */\n-\tunsigned long ttwu_wake_remote;\n-\tunsigned long ttwu_move_affine;\n-\tunsigned long ttwu_move_balance;\n+\tunsigned int ttwu_wake_remote;\n+\tunsigned int ttwu_move_affine;\n+\tunsigned int ttwu_move_balance;\n#endif\n};\n\n-extern int partition_sched_domains(cpumask_t *partition1,\n-\t\t\t\t    cpumask_t *partition2);\n+extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new);\n\n#endif\t/* CONFIG_SMP */\n\n@@ -853,23 +889,28 @@ struct rq;\nstruct sched_domain;\n\nstruct sched_class {\n-\tstruct sched_class *next;\n+\tconst struct sched_class *next;\n\nvoid (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);\nvoid (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);\n-\tvoid (*yield_task) (struct rq *rq, struct task_struct *p);\n+\tvoid (*yield_task) (struct rq *rq);\n\nvoid (*check_preempt_curr) (struct rq *rq, struct task_struct *p);\n\nstruct task_struct * (*pick_next_task) (struct rq *rq);\nvoid (*put_prev_task) (struct rq *rq, struct task_struct *p);\n\n+#ifdef CONFIG_SMP\nunsigned long (*load_balance) (struct rq *this_rq, int this_cpu,\n-\t\t\tstruct rq *busiest,\n-\t\t\tunsigned long max_nr_move, unsigned long max_load_move,\n+\t\t\tstruct rq *busiest, unsigned long max_load_move,\nstruct sched_domain *sd, enum cpu_idle_type idle,\nint *all_pinned, int *this_best_prio);\n\n+\tint (*move_one_task) (struct rq *this_rq, int this_cpu,\n+\t\t\t      struct rq *busiest, struct sched_domain *sd,\n+\t\t\t      enum cpu_idle_type idle);\n+#endif\n+\nvoid (*set_curr_task) (struct rq *rq);\nvoid (*task_tick) (struct rq *rq, struct task_struct *p);\nvoid (*task_new) (struct rq *rq, struct task_struct *p);\n@@ -887,31 +928,21 @@ struct load_weight {\n*     4 se->block_start\n*     4 se->run_node\n*     4 se->sleep_start\n- *     4 se->sleep_start_fair\n*     6 se->load.weight\n- *     7 se->delta_fair\n- *    15 se->wait_runtime\n*/\nstruct sched_entity {\n-\tlong\t\t\twait_runtime;\n-\tunsigned long\t\tdelta_fair_run;\n-\tunsigned long\t\tdelta_fair_sleep;\n-\tunsigned long\t\tdelta_exec;\n-\ts64\t\t\tfair_key;\nstruct load_weight\tload;\t\t/* for load-balancing */\nstruct rb_node\t\trun_node;\nunsigned int\t\ton_rq;\n\nu64\t\t\texec_start;\nu64\t\t\tsum_exec_runtime;\n+\tu64\t\t\tvruntime;\nu64\t\t\tprev_sum_exec_runtime;\n-\tu64\t\t\twait_start_fair;\n-\tu64\t\t\tsleep_start_fair;\n\n#ifdef CONFIG_SCHEDSTATS\nu64\t\t\twait_start;\nu64\t\t\twait_max;\n-\ts64\t\t\tsum_wait_runtime;\n\nu64\t\t\tsleep_start;\nu64\t\t\tsleep_max;\n@@ -920,9 +951,25 @@ struct sched_entity {\nu64\t\t\tblock_start;\nu64\t\t\tblock_max;\nu64\t\t\texec_max;\n+\tu64\t\t\tslice_max;\n\n-\tunsigned long\t\twait_runtime_overruns;\n-\tunsigned long\t\twait_runtime_underruns;\n+\tu64\t\t\tnr_migrations;\n+\tu64\t\t\tnr_migrations_cold;\n+\tu64\t\t\tnr_failed_migrations_affine;\n+\tu64\t\t\tnr_failed_migrations_running;\n+\tu64\t\t\tnr_failed_migrations_hot;\n+\tu64\t\t\tnr_forced_migrations;\n+\tu64\t\t\tnr_forced2_migrations;\n+\n+\tu64\t\t\tnr_wakeups;\n+\tu64\t\t\tnr_wakeups_sync;\n+\tu64\t\t\tnr_wakeups_migrate;\n+\tu64\t\t\tnr_wakeups_local;\n+\tu64\t\t\tnr_wakeups_remote;\n+\tu64\t\t\tnr_wakeups_affine;\n+\tu64\t\t\tnr_wakeups_affine_attempts;\n+\tu64\t\t\tnr_wakeups_passive;\n+\tu64\t\t\tnr_wakeups_idle;\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n@@ -951,7 +998,7 @@ struct task_struct {\n\nint prio, static_prio, normal_prio;\nstruct list_head run_list;\n-\tstruct sched_class *sched_class;\n+\tconst struct sched_class *sched_class;\nstruct sched_entity se;\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n@@ -1021,7 +1068,8 @@ struct task_struct {\nint __user *clear_child_tid;\t\t/* CLONE_CHILD_CLEARTID */\n\nunsigned int rt_priority;\n-\tcputime_t utime, stime;\n+\tcputime_t utime, stime, utimescaled, stimescaled;\n+\tcputime_t gtime;\ncputime_t prev_utime, prev_stime;\nunsigned long nvcsw, nivcsw; /* context switch counts */\nstruct timespec start_time; \t\t/* monotonic time */\n@@ -1314,6 +1362,7 @@ static inline void put_task_struct(struc\n#define PF_STARTING\t0x00000002\t/* being created */\n#define PF_EXITING\t0x00000004\t/* getting shut down */\n#define PF_EXITPIDONE\t0x00000008\t/* pi exit done on shut down */\n+#define PF_VCPU\t\t0x00000010\t/* I\'m a virtual CPU */\n#define PF_FORKNOEXEC\t0x00000040\t/* forked but didn\'t exec */\n#define PF_SUPERPRIV\t0x00000100\t/* used super-user privileges */\n#define PF_DUMPCORE\t0x00000200\t/* dumped core */\n@@ -1401,15 +1450,26 @@ static inline void idle_task_exit(void)\n\nextern void sched_idle_next(void);\n\n+#ifdef CONFIG_SCHED_DEBUG\nextern unsigned int sysctl_sched_latency;\nextern unsigned int sysctl_sched_min_granularity;\nextern unsigned int sysctl_sched_wakeup_granularity;\nextern unsigned int sysctl_sched_batch_wakeup_granularity;\n-extern unsigned int sysctl_sched_stat_granularity;\n-extern unsigned int sysctl_sched_runtime_limit;\n-extern unsigned int sysctl_sched_compat_yield;\nextern unsigned int sysctl_sched_child_runs_first;\nextern unsigned int sysctl_sched_features;\n+extern unsigned int sysctl_sched_migration_cost;\n+extern unsigned int sysctl_sched_nr_migrate;\n+#ifdef CONFIG_FAIR_GROUP_SCHED\n+extern unsigned int sysctl_sched_min_bal_int_shares;\n+extern unsigned int sysctl_sched_max_bal_int_shares;\n+#endif\n+\n+int sched_nr_latency_handler(struct ctl_table *table, int write,\n+\t\tstruct file *file, void __user *buffer, size_t *length,\n+\t\tloff_t *ppos);\n+#endif\n+\n+extern unsigned int sysctl_sched_compat_yield;\n\n#ifdef CONFIG_RT_MUTEXES\nextern int rt_mutex_getprio(struct task_struct *p);\n@@ -1843,6 +1903,18 @@ extern int sched_mc_power_savings, sched\n\nextern void normalize_rt_tasks(void);\n\n+#ifdef CONFIG_FAIR_GROUP_SCHED\n+\n+extern struct task_group init_task_group;\n+\n+extern struct task_group *sched_create_group(void);\n+extern void sched_destroy_group(struct task_group *tg);\n+extern void sched_move_task(struct task_struct *tsk);\n+extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);\n+extern unsigned long sched_group_shares(struct task_group *tg);\n+\n+#endif\n+\n#ifdef CONFIG_TASK_XACCT\nstatic inline void add_rchar(struct task_struct *tsk, ssize_t amt)\n{\n@@ -1884,6 +1956,14 @@ static inline void inc_syscw(struct task\nextern void clear_kernel_trace_flag_all_tasks(void);\nextern void set_kernel_trace_flag_all_tasks(void);\n\n+#ifdef CONFIG_SMP\n+void migration_init(void);\n+#else\n+static inline void migration_init(void)\n+{\n+}\n+#endif\n+\n#endif /* __KERNEL__ */\n\n#endif'),
 ('linux-2.6.23.orig/include/linux/taskstats.h',
  'linux-2.6.23/include/linux/taskstats.h',
  'Index: linux-2.6.23/include/linux/taskstats.h\n===================================================================\n--- linux-2.6.23.orig/include/linux/taskstats.h\n+++ linux-2.6.23/include/linux/taskstats.h\n@@ -31,7 +31,7 @@\n*/\n\n\n-#define TASKSTATS_VERSION\t5\n+#define TASKSTATS_VERSION\t6\n#define TS_COMM_LEN\t\t32\t/* should be >= TASK_COMM_LEN\n* in linux/sched.h */\n\n@@ -152,6 +152,11 @@ struct taskstats {\n\n__u64  nvcsw;\t\t\t/* voluntary_ctxt_switches */\n__u64  nivcsw;\t\t\t/* nonvoluntary_ctxt_switches */\n+\n+\t/* time accounting for SMT machines */\n+\t__u64\tac_utimescaled;\t\t/* utime scaled on frequency etc */\n+\t__u64\tac_stimescaled;\t\t/* stime scaled on frequency etc */\n+\t__u64\tcpu_scaled_run_real_total; /* scaled cpu_run_real_total */\n};\n\n'),
 ('linux-2.6.23.orig/include/linux/topology.h',
  'linux-2.6.23/include/linux/topology.h',
  'Index: linux-2.6.23/include/linux/topology.h\n===================================================================\n--- linux-2.6.23.orig/include/linux/topology.h\n+++ linux-2.6.23/include/linux/topology.h\n@@ -159,15 +159,14 @@\n.imbalance_pct\t\t= 125,\t\t\t\\\n.cache_nice_tries\t= 1,\t\t\t\\\n.busy_idx\t\t= 2,\t\t\t\\\n-\t.idle_idx\t\t= 0,\t\t\t\\\n-\t.newidle_idx\t\t= 0,\t\t\t\\\n+\t.idle_idx\t\t= 1,\t\t\t\\\n+\t.newidle_idx\t\t= 2,\t\t\t\\\n.wake_idx\t\t= 1,\t\t\t\\\n.forkexec_idx\t\t= 1,\t\t\t\\\n.flags\t\t\t= SD_LOAD_BALANCE\t\\\n| SD_BALANCE_NEWIDLE\t\\\n| SD_BALANCE_EXEC\t\\\n| SD_WAKE_AFFINE\t\\\n-\t\t\t\t| SD_WAKE_IDLE\t\t\\\n| BALANCE_FOR_PKG_POWER,\\\n.last_balance\t\t= jiffies,\t\t\\\n.balance_interval\t= 1,\t\t\t\\'),
 ('linux-2.6.23.orig/init/Kconfig',
  'linux-2.6.23/init/Kconfig',
  'Index: linux-2.6.23/init/Kconfig\n===================================================================\n--- linux-2.6.23.orig/init/Kconfig\n+++ linux-2.6.23/init/Kconfig\n@@ -273,6 +273,11 @@ config LOG_BUF_SHIFT\nconfig CPUSETS\nbool "Cpuset support"\ndepends on SMP\n+\t#\n+\t# disabled for now - depends on control groups, which\n+\t# are hard to backport:\n+\t#\n+\tdepends on 0\nhelp\nThis option will let you create and manage CPUSETs which\nallow dynamically partitioning a system into sets of CPUs and\n@@ -281,6 +286,27 @@ config CPUSETS\n\nSay N if unsure.\n\n+config FAIR_GROUP_SCHED\n+\tbool "Fair group CPU scheduler"\n+\tdefault y\n+\tdepends on EXPERIMENTAL\n+\thelp\n+\t  This feature lets CPU scheduler recognize task groups and control CPU\n+\t  bandwidth allocation to such task groups.\n+\n+choice\n+\tdepends on FAIR_GROUP_SCHED\n+\tprompt "Basis for grouping tasks"\n+\tdefault FAIR_USER_SCHED\n+\n+config FAIR_USER_SCHED\n+\tbool "user id"\n+\thelp\n+\t  This option will choose userid as the basis for grouping\n+\t  tasks, thus providing equal CPU bandwidth to each user.\n+\n+endchoice\n+\nconfig SYSFS_DEPRECATED\nbool "Create deprecated sysfs files"\ndefault y'),
 ('linux-2.6.23.orig/init/main.c',
  'linux-2.6.23/init/main.c',
  'Index: linux-2.6.23/init/main.c\n===================================================================\n--- linux-2.6.23.orig/init/main.c\n+++ linux-2.6.23/init/main.c\n@@ -794,11 +794,8 @@ __setup("nosoftlockup", nosoftlockup_set\nstatic void __init do_pre_smp_initcalls(void)\n{\nextern int spawn_ksoftirqd(void);\n-#ifdef CONFIG_SMP\n-\textern int migration_init(void);\n\nmigration_init();\n-#endif\nspawn_ksoftirqd();\nif (!nosoftlockup)\nspawn_softlockup_task();'),
 ('linux-2.6.23.orig/kernel/delayacct.c',
  'linux-2.6.23/kernel/delayacct.c',
  'Index: linux-2.6.23/kernel/delayacct.c\n===================================================================\n--- linux-2.6.23.orig/kernel/delayacct.c\n+++ linux-2.6.23/kernel/delayacct.c\n@@ -115,11 +115,17 @@ int __delayacct_add_tsk(struct taskstats\ntmp += timespec_to_ns(&ts);\nd->cpu_run_real_total = (tmp < (s64)d->cpu_run_real_total) ? 0 : tmp;\n\n+\ttmp = (s64)d->cpu_scaled_run_real_total;\n+\tcputime_to_timespec(tsk->utimescaled + tsk->stimescaled, &ts);\n+\ttmp += timespec_to_ns(&ts);\n+\td->cpu_scaled_run_real_total =\n+\t\t(tmp < (s64)d->cpu_scaled_run_real_total) ? 0 : tmp;\n+\n/*\n* No locking available for sched_info (and too expensive to add one)\n* Mitigate by taking snapshot of values\n*/\n-\tt1 = tsk->sched_info.pcnt;\n+\tt1 = tsk->sched_info.pcount;\nt2 = tsk->sched_info.run_delay;\nt3 = tsk->sched_info.cpu_time;\n'),
 ('linux-2.6.23.orig/kernel/exit.c',
  'linux-2.6.23/kernel/exit.c',
  'Index: linux-2.6.23/kernel/exit.c\n===================================================================\n--- linux-2.6.23.orig/kernel/exit.c\n+++ linux-2.6.23/kernel/exit.c\n@@ -111,6 +111,7 @@ static void __exit_signal(struct task_st\n*/\nsig->utime = cputime_add(sig->utime, tsk->utime);\nsig->stime = cputime_add(sig->stime, tsk->stime);\n+\t\tsig->gtime = cputime_add(sig->gtime, tsk->gtime);\nsig->min_flt += tsk->min_flt;\nsig->maj_flt += tsk->maj_flt;\nsig->nvcsw += tsk->nvcsw;\n@@ -1245,6 +1246,11 @@ static int wait_task_zombie(struct task_\ncputime_add(p->stime,\ncputime_add(sig->stime,\nsig->cstime)));\n+\t\tpsig->cgtime =\n+\t\t\tcputime_add(psig->cgtime,\n+\t\t\tcputime_add(p->gtime,\n+\t\t\tcputime_add(sig->gtime,\n+\t\t\t\t    sig->cgtime)));\npsig->cmin_flt +=\np->min_flt + sig->min_flt + sig->cmin_flt;\npsig->cmaj_flt +='),
 ('linux-2.6.23.orig/kernel/fork.c',
  'linux-2.6.23/kernel/fork.c',
  'Index: linux-2.6.23/kernel/fork.c\n===================================================================\n--- linux-2.6.23.orig/kernel/fork.c\n+++ linux-2.6.23/kernel/fork.c\n@@ -878,6 +878,8 @@ static inline int copy_signal(unsigned l\nsig->tty_old_pgrp = NULL;\n\nsig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;\n+\tsig->gtime = cputime_zero;\n+\tsig->cgtime = cputime_zero;\nsig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;\nsig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;\nsig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;\n@@ -1048,6 +1050,9 @@ static struct task_struct *copy_process(\np->stime = cputime_zero;\np->prev_utime = cputime_zero;\np->prev_stime = cputime_zero;\n+\tp->gtime = cputime_zero;\n+\tp->utimescaled = cputime_zero;\n+\tp->stimescaled = cputime_zero;\n\n#ifdef CONFIG_TASK_XACCT\np->rchar = 0;\t\t/* I/O counter: bytes read */'),
 ('linux-2.6.23.orig/kernel/ksysfs.c',
  'linux-2.6.23/kernel/ksysfs.c',
  'Index: linux-2.6.23/kernel/ksysfs.c\n===================================================================\n--- linux-2.6.23.orig/kernel/ksysfs.c\n+++ linux-2.6.23/kernel/ksysfs.c\n@@ -14,6 +14,7 @@\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/kexec.h>\n+#include <linux/sched.h>\n\n#define KERNEL_ATTR_RO(_name) \\\nstatic struct subsys_attribute _name##_attr = __ATTR_RO(_name)\n@@ -116,6 +117,13 @@ static int __init ksysfs_init(void)\n&notes_attr);\n}\n\n+\t/*\n+\t * Create "/sys/kernel/uids" directory and corresponding root user\'s\n+\t * directory under it.\n+\t */\n+\tif (!error)\n+\t\terror = uids_kobject_init();\n+\nreturn error;\n}\n'),
 ('linux-2.6.23.orig/kernel/sched.c',
  'linux-2.6.23/kernel/sched.c',
  'Index: linux-2.6.23/kernel/sched.c\n===================================================================\n--- linux-2.6.23.orig/kernel/sched.c\n+++ linux-2.6.23/kernel/sched.c\n@@ -44,6 +44,7 @@\n#include <linux/vmalloc.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n+#include <linux/pid_namespace.h>\n#include <linux/smp.h>\n#include <linux/threads.h>\n#include <linux/timer.h>\n@@ -62,8 +63,10 @@\n#include <linux/delayacct.h>\n#include <linux/reciprocal_div.h>\n#include <linux/unistd.h>\n+#include <linux/pagemap.h>\n\n#include <asm/tlb.h>\n+#include <asm/irq_regs.h>\n\n/*\n* Scheduler clock - returns current time in nanosec units.\n@@ -72,7 +75,7 @@\n*/\nunsigned long long __attribute__((weak)) sched_clock(void)\n{\n-\treturn (unsigned long long)jiffies * (1000000000 / HZ);\n+\treturn (unsigned long long)jiffies * (NSEC_PER_SEC / HZ);\n}\n\n/*\n@@ -96,8 +99,8 @@ unsigned long long __attribute__((weak))\n/*\n* Some helpers for converting nanosecond timing to jiffy resolution\n*/\n-#define NS_TO_JIFFIES(TIME)\t((TIME) / (1000000000 / HZ))\n-#define JIFFIES_TO_NS(TIME)\t((TIME) * (1000000000 / HZ))\n+#define NS_TO_JIFFIES(TIME)\t((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))\n+#define JIFFIES_TO_NS(TIME)\t((TIME) * (NSEC_PER_SEC / HZ))\n\n#define NICE_0_LOAD\t\tSCHED_LOAD_SCALE\n#define NICE_0_SHIFT\t\tSCHED_LOAD_SHIFT\n@@ -105,11 +108,9 @@ unsigned long long __attribute__((weak))\n/*\n* These are the \'tuning knobs\' of the scheduler:\n*\n- * Minimum timeslice is 5 msecs (or 1 jiffy, whichever is larger),\n- * default timeslice is 100 msecs, maximum timeslice is 800 msecs.\n+ * default timeslice is 100 msecs (used only for SCHED_RR tasks).\n* Timeslices get refilled after they expire.\n*/\n-#define MIN_TIMESLICE\t\tmax(5 * HZ / 1000, 1)\n#define DEF_TIMESLICE\t\t(100 * HZ / 1000)\n\n#ifdef CONFIG_SMP\n@@ -133,24 +134,6 @@ static inline void sg_inc_cpu_power(stru\n}\n#endif\n\n-#define SCALE_PRIO(x, prio) \\\n-\tmax(x * (MAX_PRIO - prio) / (MAX_USER_PRIO / 2), MIN_TIMESLICE)\n-\n-/*\n- * static_prio_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]\n- * to time slice values: [800ms ... 100ms ... 5ms]\n- */\n-static unsigned int static_prio_timeslice(int static_prio)\n-{\n-\tif (static_prio == NICE_TO_PRIO(19))\n-\t\treturn 1;\n-\n-\tif (static_prio < NICE_TO_PRIO(0))\n-\t\treturn SCALE_PRIO(DEF_TIMESLICE * 4, static_prio);\n-\telse\n-\t\treturn SCALE_PRIO(DEF_TIMESLICE, static_prio);\n-}\n-\nstatic inline int rt_policy(int policy)\n{\nif (unlikely(policy == SCHED_FIFO) || unlikely(policy == SCHED_RR))\n@@ -171,41 +154,111 @@ struct rt_prio_array {\nstruct list_head queue[MAX_RT_PRIO];\n};\n\n-struct load_stat {\n-\tstruct load_weight load;\n-\tu64 load_update_start, load_update_last;\n-\tunsigned long delta_fair, delta_exec, delta_stat;\n+#ifdef CONFIG_FAIR_GROUP_SCHED\n+\n+#include <linux/cgroup.h>\n+\n+struct cfs_rq;\n+\n+/* task group related information */\n+struct task_group {\n+#ifdef CONFIG_FAIR_CGROUP_SCHED\n+\tstruct cgroup_subsys_state css;\n+#endif\n+\t/* schedulable entities of this group on each cpu */\n+\tstruct sched_entity **se;\n+\t/* runqueue "owned" by this group on each cpu */\n+\tstruct cfs_rq **cfs_rq;\n+\tunsigned long shares;\n+\t/* spinlock to serialize modification to shares */\n+\tspinlock_t lock;\n+\tstruct rcu_head rcu;\n+};\n+\n+/* Default task group\'s sched entity on each cpu */\n+static DEFINE_PER_CPU(struct sched_entity, init_sched_entity);\n+/* Default task group\'s cfs_rq on each cpu */\n+static DEFINE_PER_CPU(struct cfs_rq, init_cfs_rq) ____cacheline_aligned_in_smp;\n+\n+static struct sched_entity *init_sched_entity_p[NR_CPUS];\n+static struct cfs_rq *init_cfs_rq_p[NR_CPUS];\n+\n+/* Default task group.\n+ *\tEvery task in system belong to this group at bootup.\n+ */\n+struct task_group init_task_group = {\n+\t.se     = init_sched_entity_p,\n+\t.cfs_rq = init_cfs_rq_p,\n};\n\n+#ifdef CONFIG_FAIR_USER_SCHED\n+# define INIT_TASK_GRP_LOAD\t2*NICE_0_LOAD\n+#else\n+# define INIT_TASK_GRP_LOAD\tNICE_0_LOAD\n+#endif\n+\n+static int init_task_group_load = INIT_TASK_GRP_LOAD;\n+\n+/* return group to which a task belongs */\n+static inline struct task_group *task_group(struct task_struct *p)\n+{\n+\tstruct task_group *tg;\n+\n+#ifdef CONFIG_FAIR_USER_SCHED\n+\ttg = p->user->tg;\n+#elif defined(CONFIG_FAIR_CGROUP_SCHED)\n+\ttg = container_of(task_subsys_state(p, cpu_cgroup_subsys_id),\n+\t\t\t\tstruct task_group, css);\n+#else\n+\ttg = &init_task_group;\n+#endif\n+\treturn tg;\n+}\n+\n+/* Change a task\'s cfs_rq and parent entity if it moves across CPUs/groups */\n+static inline void set_task_cfs_rq(struct task_struct *p, unsigned int cpu)\n+{\n+\tp->se.cfs_rq = task_group(p)->cfs_rq[cpu];\n+\tp->se.parent = task_group(p)->se[cpu];\n+}\n+\n+#else\n+\n+static inline void set_task_cfs_rq(struct task_struct *p, unsigned int cpu) { }\n+\n+#endif\t/* CONFIG_FAIR_GROUP_SCHED */\n+\n/* CFS-related fields in a runqueue */\nstruct cfs_rq {\nstruct load_weight load;\nunsigned long nr_running;\n\n-\ts64 fair_clock;\nu64 exec_clock;\n-\ts64 wait_runtime;\n-\tu64 sleeper_bonus;\n-\tunsigned long wait_runtime_overruns, wait_runtime_underruns;\n+\tu64 min_vruntime;\n\nstruct rb_root tasks_timeline;\nstruct rb_node *rb_leftmost;\nstruct rb_node *rb_load_balance_curr;\n-#ifdef CONFIG_FAIR_GROUP_SCHED\n/* \'curr\' points to currently running entity on this cfs_rq.\n* It is set to NULL otherwise (i.e when none are currently running).\n*/\nstruct sched_entity *curr;\n+\n+\tunsigned long nr_spread_over;\n+\n+#ifdef CONFIG_FAIR_GROUP_SCHED\nstruct rq *rq;\t/* cpu runqueue to which this cfs_rq is attached */\n\n-\t/* leaf cfs_rqs are those that hold tasks (lowest schedulable entity in\n+\t/*\n+\t * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in\n* a hierarchy). Non-leaf lrqs hold other higher schedulable entities\n* (like users, containers etc.)\n*\n* leaf_cfs_rq_list ties together list of leaf cfs_rq\'s in a cpu. This\n* list is used during load balance.\n*/\n-\tstruct list_head leaf_cfs_rq_list; /* Better name : task_cfs_rq_list? */\n+\tstruct list_head leaf_cfs_rq_list;\n+\tstruct task_group *tg;\t/* group that "owns" this runqueue */\n#endif\n};\n\n@@ -224,7 +277,8 @@ struct rt_rq {\n* acquire operations must be ordered by ascending &runqueue.\n*/\nstruct rq {\n-\tspinlock_t lock;\t/* runqueue lock */\n+\t/* runqueue lock: */\n+\tspinlock_t lock;\n\n/*\n* nr_running and cpu_load should be in the same cacheline because\n@@ -237,15 +291,17 @@ struct rq {\n#ifdef CONFIG_NO_HZ\nunsigned char in_nohz_recently;\n#endif\n-\tstruct load_stat ls;\t/* capture load from *all* tasks on this cpu */\n+\t/* capture load from *all* tasks on this cpu: */\n+\tstruct load_weight load;\nunsigned long nr_load_updates;\nu64 nr_switches;\n\nstruct cfs_rq cfs;\n#ifdef CONFIG_FAIR_GROUP_SCHED\n-\tstruct list_head leaf_cfs_rq_list; /* list of leaf cfs_rq on this cpu */\n+\t/* list of leaf cfs_rq on this cpu: */\n+\tstruct list_head leaf_cfs_rq_list;\n#endif\n-\tstruct rt_rq  rt;\n+\tstruct rt_rq rt;\n\n/*\n* This is part of a global counter where only the total sum\n@@ -275,7 +331,8 @@ struct rq {\n/* For active balancing */\nint active_balance;\nint push_cpu;\n-\tint cpu;\t\t/* cpu of this runqueue */\n+\t/* cpu of this runqueue: */\n+\tint cpu;\n\nstruct task_struct *migration_thread;\nstruct list_head migration_queue;\n@@ -286,19 +343,22 @@ struct rq {\nstruct sched_info rq_sched_info;\n\n/* sys_sched_yield() stats */\n-\tunsigned long yld_exp_empty;\n-\tunsigned long yld_act_empty;\n-\tunsigned long yld_both_empty;\n-\tunsigned long yld_cnt;\n+\tunsigned int yld_exp_empty;\n+\tunsigned int yld_act_empty;\n+\tunsigned int yld_both_empty;\n+\tunsigned int yld_count;\n\n/* schedule() stats */\n-\tunsigned long sched_switch;\n-\tunsigned long sched_cnt;\n-\tunsigned long sched_goidle;\n+\tunsigned int sched_switch;\n+\tunsigned int sched_count;\n+\tunsigned int sched_goidle;\n\n/* try_to_wake_up() stats */\n-\tunsigned long ttwu_cnt;\n-\tunsigned long ttwu_local;\n+\tunsigned int ttwu_count;\n+\tunsigned int ttwu_local;\n+\n+\t/* BKL stats */\n+\tunsigned int bkl_count;\n#endif\nstruct lock_class_key rq_lock_key;\n};\n@@ -383,6 +443,41 @@ static void update_rq_clock(struct rq *r\n#define cpu_curr(cpu)\t\t(cpu_rq(cpu)->curr)\n\n/*\n+ * Tunables that become constants when CONFIG_SCHED_DEBUG is off:\n+ */\n+#ifdef CONFIG_SCHED_DEBUG\n+# define const_debug __read_mostly\n+#else\n+# define const_debug static const\n+#endif\n+\n+/*\n+ * Debugging: various feature bits\n+ */\n+enum {\n+\tSCHED_FEAT_NEW_FAIR_SLEEPERS\t= 1,\n+\tSCHED_FEAT_WAKEUP_PREEMPT\t= 2,\n+\tSCHED_FEAT_START_DEBIT\t\t= 4,\n+\tSCHED_FEAT_TREE_AVG\t\t= 8,\n+\tSCHED_FEAT_APPROX_AVG\t\t= 16,\n+};\n+\n+const_debug unsigned int sysctl_sched_features =\n+\t\tSCHED_FEAT_NEW_FAIR_SLEEPERS\t* 1 |\n+\t\tSCHED_FEAT_WAKEUP_PREEMPT\t* 1 |\n+\t\tSCHED_FEAT_START_DEBIT\t\t* 1 |\n+\t\tSCHED_FEAT_TREE_AVG\t\t* 0 |\n+\t\tSCHED_FEAT_APPROX_AVG\t\t* 0;\n+\n+#define sched_feat(x) (sysctl_sched_features & SCHED_FEAT_##x)\n+\n+/*\n+ * Number of tasks to iterate in a single balance run.\n+ * Limited because this is done with IRQs disabled.\n+ */\n+const_debug unsigned int sysctl_sched_nr_migrate = 32;\n+\n+/*\n* For kernel-internal use: high-speed (but slightly incorrect) per-cpu\n* clock constructed from sched_clock():\n*/\n@@ -394,24 +489,18 @@ unsigned long long cpu_clock(int cpu)\n\nlocal_irq_save(flags);\nrq = cpu_rq(cpu);\n-\tupdate_rq_clock(rq);\n+\t/*\n+\t * Only call sched_clock() if the scheduler has already been\n+\t * initialized (some code might call cpu_clock() very early):\n+\t */\n+\tif (rq->idle)\n+\t\tupdate_rq_clock(rq);\nnow = rq->clock;\nlocal_irq_restore(flags);\n\nreturn now;\n}\n-\n-#ifdef CONFIG_FAIR_GROUP_SCHED\n-/* Change a task\'s ->cfs_rq if it moves across CPUs */\n-static inline void set_task_cfs_rq(struct task_struct *p)\n-{\n-\tp->se.cfs_rq = &task_rq(p)->cfs;\n-}\n-#else\n-static inline void set_task_cfs_rq(struct task_struct *p)\n-{\n-}\n-#endif\n+EXPORT_SYMBOL_GPL(cpu_clock);\n\n#ifndef prepare_arch_switch\n# define prepare_arch_switch(next)\tdo { } while (0)\n@@ -420,10 +509,15 @@ static inline void set_task_cfs_rq(struc\n# define finish_arch_switch(prev)\tdo { } while (0)\n#endif\n\n+static inline int task_current(struct rq *rq, struct task_struct *p)\n+{\n+\treturn rq->curr == p;\n+}\n+\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\nstatic inline int task_running(struct rq *rq, struct task_struct *p)\n{\n-\treturn rq->curr == p;\n+\treturn task_current(rq, p);\n}\n\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n@@ -452,7 +546,7 @@ static inline int task_running(struct rq\n#ifdef CONFIG_SMP\nreturn p->oncpu;\n#else\n-\treturn rq->curr == p;\n+\treturn task_current(rq, p);\n#endif\n}\n\n@@ -497,21 +591,18 @@ static inline void finish_lock_switch(st\nstatic inline struct rq *__task_rq_lock(struct task_struct *p)\n__acquires(rq->lock)\n{\n-\tstruct rq *rq;\n-\n-repeat_lock_task:\n-\trq = task_rq(p);\n-\tspin_lock(&rq->lock);\n-\tif (unlikely(rq != task_rq(p))) {\n+\tfor (;;) {\n+\t\tstruct rq *rq = task_rq(p);\n+\t\tspin_lock(&rq->lock);\n+\t\tif (likely(rq == task_rq(p)))\n+\t\t\treturn rq;\nspin_unlock(&rq->lock);\n-\t\tgoto repeat_lock_task;\n}\n-\treturn rq;\n}\n\n/*\n* task_rq_lock - lock the runqueue a given task resides on and disable\n- * interrupts.  Note the ordering: we can safely lookup the task_rq without\n+ * interrupts. Note the ordering: we can safely lookup the task_rq without\n* explicitly disabling preemption.\n*/\nstatic struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)\n@@ -519,18 +610,17 @@ static struct rq *task_rq_lock(struct ta\n{\nstruct rq *rq;\n\n-repeat_lock_task:\n-\tlocal_irq_save(*flags);\n-\trq = task_rq(p);\n-\tspin_lock(&rq->lock);\n-\tif (unlikely(rq != task_rq(p))) {\n+\tfor (;;) {\n+\t\tlocal_irq_save(*flags);\n+\t\trq = task_rq(p);\n+\t\tspin_lock(&rq->lock);\n+\t\tif (likely(rq == task_rq(p)))\n+\t\t\treturn rq;\nspin_unlock_irqrestore(&rq->lock, *flags);\n-\t\tgoto repeat_lock_task;\n}\n-\treturn rq;\n}\n\n-static inline void __task_rq_unlock(struct rq *rq)\n+static void __task_rq_unlock(struct rq *rq)\n__releases(rq->lock)\n{\nspin_unlock(&rq->lock);\n@@ -545,7 +635,7 @@ static inline void task_rq_unlock(struct\n/*\n* this_rq_lock - lock this runqueue and disable interrupts.\n*/\n-static inline struct rq *this_rq_lock(void)\n+static struct rq *this_rq_lock(void)\n__acquires(rq->lock)\n{\nstruct rq *rq;\n@@ -579,6 +669,7 @@ void sched_clock_idle_wakeup_event(u64 d\nstruct rq *rq = cpu_rq(smp_processor_id());\nu64 now = sched_clock();\n\n+\ttouch_softlockup_watchdog();\nrq->idle_clock += delta_ns;\n/*\n* Override the previous timestamp and ignore all\n@@ -645,19 +736,6 @@ static inline void resched_task(struct t\n}\n#endif\n\n-static u64 div64_likely32(u64 divident, unsigned long divisor)\n-{\n-#if BITS_PER_LONG == 32\n-\tif (likely(divident <= 0xffffffffULL))\n-\t\treturn (u32)divident / divisor;\n-\tdo_div(divident, divisor);\n-\n-\treturn divident;\n-#else\n-\treturn divident / divisor;\n-#endif\n-}\n-\n#if BITS_PER_LONG == 32\n# define WMULT_CONST\t(~0UL)\n#else\n@@ -699,23 +777,21 @@ calc_delta_fair(unsigned long delta_exec\nreturn calc_delta_mine(delta_exec, NICE_0_LOAD, lw);\n}\n\n-static void update_load_add(struct load_weight *lw, unsigned long inc)\n+static inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\nlw->weight += inc;\n-\tlw->inv_weight = 0;\n}\n\n-static void update_load_sub(struct load_weight *lw, unsigned long dec)\n+static inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\nlw->weight -= dec;\n-\tlw->inv_weight = 0;\n}\n\n/*\n* To aid in avoiding the subversion of "niceness" due to uneven distribution\n* of tasks with abnormal "nice" values across CPUs the contribution that\n* each task makes to its run queue\'s load is weighted according to its\n- * scheduling class and "nice" value.  For SCHED_NORMAL tasks this is just a\n+ * scheduling class and "nice" value. For SCHED_NORMAL tasks this is just a\n* scaled version of the new time slice allocation that they receive on time\n* slice expiry etc.\n*/\n@@ -777,36 +853,40 @@ struct rq_iterator {\nstruct task_struct *(*next)(void *);\n};\n\n-static int balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,\n-\t\t      unsigned long max_nr_move, unsigned long max_load_move,\n-\t\t      struct sched_domain *sd, enum cpu_idle_type idle,\n-\t\t      int *all_pinned, unsigned long *load_moved,\n-\t\t      int *this_best_prio, struct rq_iterator *iterator);\n+#ifdef CONFIG_SMP\n+static unsigned long\n+balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,\n+\t      unsigned long max_load_move, struct sched_domain *sd,\n+\t      enum cpu_idle_type idle, int *all_pinned,\n+\t      int *this_best_prio, struct rq_iterator *iterator);\n+\n+static int\n+iter_move_one_task(struct rq *this_rq, int this_cpu, struct rq *busiest,\n+\t\t   struct sched_domain *sd, enum cpu_idle_type idle,\n+\t\t   struct rq_iterator *iterator);\n+#endif\n+\n+#ifdef CONFIG_CGROUP_CPUACCT\n+static void cpuacct_charge(struct task_struct *tsk, u64 cputime);\n+#else\n+static inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}\n+#endif\n\n#include "sched_stats.h"\n-#include "sched_rt.c"\n-#include "sched_fair.c"\n#include "sched_idletask.c"\n+#include "sched_fair.c"\n+#include "sched_rt.c"\n#ifdef CONFIG_SCHED_DEBUG\n# include "sched_debug.c"\n#endif\n\n#define sched_class_highest (&rt_sched_class)\n\n-static void __update_curr_load(struct rq *rq, struct load_stat *ls)\n-{\n-\tif (rq->curr != rq->idle && ls->load.weight) {\n-\t\tls->delta_exec += ls->delta_stat;\n-\t\tls->delta_fair += calc_delta_fair(ls->delta_stat, &ls->load);\n-\t\tls->delta_stat = 0;\n-\t}\n-}\n-\n/*\n* Update delta_exec, delta_fair fields for rq.\n*\n* delta_fair clock advances at a rate inversely proportional to\n- * total load (rq->ls.load.weight) on the runqueue, while\n+ * total load (rq->load.weight) on the runqueue, while\n* delta_exec advances at the same rate as wall-clock (provided\n* cpu is not idle).\n*\n@@ -814,35 +894,17 @@ static void __update_curr_load(struct rq\n* runqueue over any given interval. This (smoothened) load is used\n* during load balance.\n*\n- * This function is called /before/ updating rq->ls.load\n+ * This function is called /before/ updating rq->load\n* and when switching tasks.\n*/\n-static void update_curr_load(struct rq *rq)\n-{\n-\tstruct load_stat *ls = &rq->ls;\n-\tu64 start;\n-\n-\tstart = ls->load_update_start;\n-\tls->load_update_start = rq->clock;\n-\tls->delta_stat += rq->clock - start;\n-\t/*\n-\t * Stagger updates to ls->delta_fair. Very frequent updates\n-\t * can be expensive.\n-\t */\n-\tif (ls->delta_stat >= sysctl_sched_stat_granularity)\n-\t\t__update_curr_load(rq, ls);\n-}\n-\nstatic inline void inc_load(struct rq *rq, const struct task_struct *p)\n{\n-\tupdate_curr_load(rq);\n-\tupdate_load_add(&rq->ls.load, p->se.load.weight);\n+\tupdate_load_add(&rq->load, p->se.load.weight);\n}\n\nstatic inline void dec_load(struct rq *rq, const struct task_struct *p)\n{\n-\tupdate_curr_load(rq);\n-\tupdate_load_sub(&rq->ls.load, p->se.load.weight);\n+\tupdate_load_sub(&rq->load, p->se.load.weight);\n}\n\nstatic void inc_nr_running(struct task_struct *p, struct rq *rq)\n@@ -859,8 +921,6 @@ static void dec_nr_running(struct task_s\n\nstatic void set_load_weight(struct task_struct *p)\n{\n-\tp->se.wait_runtime = 0;\n-\nif (task_has_rt_policy(p)) {\np->se.load.weight = prio_to_weight[0] * 2;\np->se.load.inv_weight = prio_to_wmult[0] >> 1;\n@@ -952,20 +1012,6 @@ static void activate_task(struct rq *rq,\n}\n\n/*\n- * activate_idle_task - move idle task to the _front_ of runqueue.\n- */\n-static inline void activate_idle_task(struct task_struct *p, struct rq *rq)\n-{\n-\tupdate_rq_clock(rq);\n-\n-\tif (p->state == TASK_UNINTERRUPTIBLE)\n-\t\trq->nr_uninterruptible--;\n-\n-\tenqueue_task(rq, p, 0);\n-\tinc_nr_running(p, rq);\n-}\n-\n-/*\n* deactivate_task - remove a task from the runqueue.\n*/\nstatic void deactivate_task(struct rq *rq, struct task_struct *p, int sleep)\n@@ -989,32 +1035,56 @@ inline int task_curr(const struct task_s\n/* Used instead of source_load when we know the type == 0 */\nunsigned long weighted_cpuload(const int cpu)\n{\n-\treturn cpu_rq(cpu)->ls.load.weight;\n+\treturn cpu_rq(cpu)->load.weight;\n}\n\nstatic inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n+\tset_task_cfs_rq(p, cpu);\n#ifdef CONFIG_SMP\n+\t/*\n+\t * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be\n+\t * successfuly executed on another CPU. We must ensure that updates of\n+\t * per-task data have been completed by this moment.\n+\t */\n+\tsmp_wmb();\ntask_thread_info(p)->cpu = cpu;\n-\tset_task_cfs_rq(p);\n#endif\n}\n\n#ifdef CONFIG_SMP\n\n+/*\n+ * Is this task likely cache-hot:\n+ */\n+static inline int\n+task_hot(struct task_struct *p, u64 now, struct sched_domain *sd)\n+{\n+\ts64 delta;\n+\n+\tif (p->sched_class != &fair_sched_class)\n+\t\treturn 0;\n+\n+\tif (sysctl_sched_migration_cost == -1)\n+\t\treturn 1;\n+\tif (sysctl_sched_migration_cost == 0)\n+\t\treturn 0;\n+\n+\tdelta = now - p->se.exec_start;\n+\n+\treturn delta < (s64)sysctl_sched_migration_cost;\n+}\n+\n+\nvoid set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n{\nint old_cpu = task_cpu(p);\nstruct rq *old_rq = cpu_rq(old_cpu), *new_rq = cpu_rq(new_cpu);\n-\tu64 clock_offset, fair_clock_offset;\n+\tstruct cfs_rq *old_cfsrq = task_cfs_rq(p),\n+\t\t      *new_cfsrq = cpu_cfs_rq(old_cfsrq, new_cpu);\n+\tu64 clock_offset;\n\nclock_offset = old_rq->clock - new_rq->clock;\n-\tfair_clock_offset = old_rq->cfs.fair_clock - new_rq->cfs.fair_clock;\n-\n-\tif (p->se.wait_start_fair)\n-\t\tp->se.wait_start_fair -= fair_clock_offset;\n-\tif (p->se.sleep_start_fair)\n-\t\tp->se.sleep_start_fair -= fair_clock_offset;\n\n#ifdef CONFIG_SCHEDSTATS\nif (p->se.wait_start)\n@@ -1023,7 +1093,14 @@ void set_task_cpu(struct task_struct *p,\np->se.sleep_start -= clock_offset;\nif (p->se.block_start)\np->se.block_start -= clock_offset;\n+\tif (old_cpu != new_cpu) {\n+\t\tschedstat_inc(p, se.nr_migrations);\n+\t\tif (task_hot(p, old_rq->clock, NULL))\n+\t\t\tschedstat_inc(p, se.nr_forced2_migrations);\n+\t}\n#endif\n+\tp->se.vruntime -= old_cfsrq->min_vruntime -\n+\t\t\t\t\t new_cfsrq->min_vruntime;\n\n__set_task_cpu(p, new_cpu);\n}\n@@ -1078,69 +1155,71 @@ void wait_task_inactive(struct task_stru\nint running, on_rq;\nstruct rq *rq;\n\n-repeat:\n-\t/*\n-\t * We do the initial early heuristics without holding\n-\t * any task-queue locks at all. We\'ll only try to get\n-\t * the runqueue lock when things look like they will\n-\t * work out!\n-\t */\n-\trq = task_rq(p);\n+\tfor (;;) {\n+\t\t/*\n+\t\t * We do the initial early heuristics without holding\n+\t\t * any task-queue locks at all. We\'ll only try to get\n+\t\t * the runqueue lock when things look like they will\n+\t\t * work out!\n+\t\t */\n+\t\trq = task_rq(p);\n\n-\t/*\n-\t * If the task is actively running on another CPU\n-\t * still, just relax and busy-wait without holding\n-\t * any locks.\n-\t *\n-\t * NOTE! Since we don\'t hold any locks, it\'s not\n-\t * even sure that "rq" stays as the right runqueue!\n-\t * But we don\'t care, since "task_running()" will\n-\t * return false if the runqueue has changed and p\n-\t * is actually now running somewhere else!\n-\t */\n-\twhile (task_running(rq, p))\n-\t\tcpu_relax();\n+\t\t/*\n+\t\t * If the task is actively running on another CPU\n+\t\t * still, just relax and busy-wait without holding\n+\t\t * any locks.\n+\t\t *\n+\t\t * NOTE! Since we don\'t hold any locks, it\'s not\n+\t\t * even sure that "rq" stays as the right runqueue!\n+\t\t * But we don\'t care, since "task_running()" will\n+\t\t * return false if the runqueue has changed and p\n+\t\t * is actually now running somewhere else!\n+\t\t */\n+\t\twhile (task_running(rq, p))\n+\t\t\tcpu_relax();\n\n-\t/*\n-\t * Ok, time to look more closely! We need the rq\n-\t * lock now, to be *sure*. If we\'re wrong, we\'ll\n-\t * just go back and repeat.\n-\t */\n-\trq = task_rq_lock(p, &flags);\n-\trunning = task_running(rq, p);\n-\ton_rq = p->se.on_rq;\n-\ttask_rq_unlock(rq, &flags);\n+\t\t/*\n+\t\t * Ok, time to look more closely! We need the rq\n+\t\t * lock now, to be *sure*. If we\'re wrong, we\'ll\n+\t\t * just go back and repeat.\n+\t\t */\n+\t\trq = task_rq_lock(p, &flags);\n+\t\trunning = task_running(rq, p);\n+\t\ton_rq = p->se.on_rq;\n+\t\ttask_rq_unlock(rq, &flags);\n\n-\t/*\n-\t * Was it really running after all now that we\n-\t * checked with the proper locks actually held?\n-\t *\n-\t * Oops. Go back and try again..\n-\t */\n-\tif (unlikely(running)) {\n-\t\tcpu_relax();\n-\t\tgoto repeat;\n-\t}\n+\t\t/*\n+\t\t * Was it really running after all now that we\n+\t\t * checked with the proper locks actually held?\n+\t\t *\n+\t\t * Oops. Go back and try again..\n+\t\t */\n+\t\tif (unlikely(running)) {\n+\t\t\tcpu_relax();\n+\t\t\tcontinue;\n+\t\t}\n\n-\t/*\n-\t * It\'s not enough that it\'s not actively running,\n-\t * it must be off the runqueue _entirely_, and not\n-\t * preempted!\n-\t *\n-\t * So if it wa still runnable (but just not actively\n-\t * running right now), it\'s preempted, and we should\n-\t * yield - it could be a while.\n-\t */\n-\tif (unlikely(on_rq)) {\n-\t\tyield();\n-\t\tgoto repeat;\n-\t}\n+\t\t/*\n+\t\t * It\'s not enough that it\'s not actively running,\n+\t\t * it must be off the runqueue _entirely_, and not\n+\t\t * preempted!\n+\t\t *\n+\t\t * So if it wa still runnable (but just not actively\n+\t\t * running right now), it\'s preempted, and we should\n+\t\t * yield - it could be a while.\n+\t\t */\n+\t\tif (unlikely(on_rq)) {\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tcontinue;\n+\t\t}\n\n-\t/*\n-\t * Ahh, all good. It wasn\'t running, and it wasn\'t\n-\t * runnable, which means that it will never become\n-\t * running in the future either. We\'re all done!\n-\t */\n+\t\t/*\n+\t\t * Ahh, all good. It wasn\'t running, and it wasn\'t\n+\t\t * runnable, which means that it will never become\n+\t\t * running in the future either. We\'re all done!\n+\t\t */\n+\t\tbreak;\n+\t}\n}\n\n/***\n@@ -1174,7 +1253,7 @@ void kick_process(struct task_struct *p)\n* We want to under-estimate the load of migration sources, to\n* balance conservatively.\n*/\n-static inline unsigned long source_load(int cpu, int type)\n+static unsigned long source_load(int cpu, int type)\n{\nstruct rq *rq = cpu_rq(cpu);\nunsigned long total = weighted_cpuload(cpu);\n@@ -1189,7 +1268,7 @@ static inline unsigned long source_load(\n* Return a high guess at the load of a migration-target cpu weighted\n* according to the scheduling class and "nice" value.\n*/\n-static inline unsigned long target_load(int cpu, int type)\n+static unsigned long target_load(int cpu, int type)\n{\nstruct rq *rq = cpu_rq(cpu);\nunsigned long total = weighted_cpuload(cpu);\n@@ -1231,7 +1310,7 @@ find_idlest_group(struct sched_domain *s\n\n/* Skip over this group if it has no CPUs allowed */\nif (!cpus_intersects(group->cpumask, p->cpus_allowed))\n-\t\t\tgoto nextgroup;\n+\t\t\tcontinue;\n\nlocal_group = cpu_isset(this_cpu, group->cpumask);\n\n@@ -1259,9 +1338,7 @@ find_idlest_group(struct sched_domain *s\nmin_load = avg_load;\nidlest = group;\n}\n-nextgroup:\n-\t\tgroup = group->next;\n-\t} while (group != sd->groups);\n+\t} while (group = group->next, group != sd->groups);\n\nif (!idlest || 100*this_load < imbalance*min_load)\nreturn NULL;\n@@ -1393,8 +1470,13 @@ static int wake_idle(int cpu, struct tas\nif (sd->flags & SD_WAKE_IDLE) {\ncpus_and(tmp, sd->span, p->cpus_allowed);\nfor_each_cpu_mask(i, tmp) {\n-\t\t\t\tif (idle_cpu(i))\n+\t\t\t\tif (idle_cpu(i)) {\n+\t\t\t\t\tif (i != task_cpu(p)) {\n+\t\t\t\t\t\tschedstat_inc(p,\n+\t\t\t\t\t\t\tse.nr_wakeups_idle);\n+\t\t\t\t\t}\nreturn i;\n+\t\t\t\t}\n}\n} else {\nbreak;\n@@ -1425,7 +1507,7 @@ static inline int wake_idle(int cpu, str\n*/\nstatic int try_to_wake_up(struct task_struct *p, unsigned int state, int sync)\n{\n-\tint cpu, this_cpu, success = 0;\n+\tint cpu, orig_cpu, this_cpu, success = 0;\nunsigned long flags;\nlong old_state;\nstruct rq *rq;\n@@ -1446,6 +1528,7 @@ static int try_to_wake_up(struct task_st\ngoto out_running;\n\ncpu = task_cpu(p);\n+\torig_cpu = cpu;\nthis_cpu = smp_processor_id();\n\n#ifdef CONFIG_SMP\n@@ -1454,7 +1537,7 @@ static int try_to_wake_up(struct task_st\n\nnew_cpu = cpu;\n\n-\tschedstat_inc(rq, ttwu_cnt);\n+\tschedstat_inc(rq, ttwu_count);\nif (cpu == this_cpu) {\nschedstat_inc(rq, ttwu_local);\ngoto out_set_cpu;\n@@ -1489,6 +1572,13 @@ static int try_to_wake_up(struct task_st\nunsigned long tl = this_load;\nunsigned long tl_per_task;\n\n+\t\t\t/*\n+\t\t\t * Attract cache-cold tasks on sync wakeups:\n+\t\t\t */\n+\t\t\tif (sync && !task_hot(p, rq->clock, this_sd))\n+\t\t\t\tgoto out_set_cpu;\n+\n+\t\t\tschedstat_inc(p, se.nr_wakeups_affine_attempts);\ntl_per_task = cpu_avg_load_per_task(this_cpu);\n\n/*\n@@ -1508,6 +1598,7 @@ static int try_to_wake_up(struct task_st\n* there is no bad imbalance.\n*/\nschedstat_inc(this_sd, ttwu_move_affine);\n+\t\t\t\tschedstat_inc(p, se.nr_wakeups_affine);\ngoto out_set_cpu;\n}\n}\n@@ -1519,6 +1610,7 @@ static int try_to_wake_up(struct task_st\nif (this_sd->flags & SD_WAKE_BALANCE) {\nif (imbalance*this_load <= 100*load) {\nschedstat_inc(this_sd, ttwu_move_balance);\n+\t\t\t\tschedstat_inc(p, se.nr_wakeups_passive);\ngoto out_set_cpu;\n}\n}\n@@ -1544,18 +1636,18 @@ out_set_cpu:\n\nout_activate:\n#endif /* CONFIG_SMP */\n+\tschedstat_inc(p, se.nr_wakeups);\n+\tif (sync)\n+\t\tschedstat_inc(p, se.nr_wakeups_sync);\n+\tif (orig_cpu != cpu)\n+\t\tschedstat_inc(p, se.nr_wakeups_migrate);\n+\tif (cpu == this_cpu)\n+\t\tschedstat_inc(p, se.nr_wakeups_local);\n+\telse\n+\t\tschedstat_inc(p, se.nr_wakeups_remote);\nupdate_rq_clock(rq);\nactivate_task(rq, p, 1);\n-\t/*\n-\t * Sync wakeups (i.e. those types of wakeups where the waker\n-\t * has indicated that it will leave the CPU in short order)\n-\t * don\'t trigger a preemption, if the woken up task will run on\n-\t * this cpu. (in this case the \'I will reschedule\' promise of\n-\t * the waker guarantees that the freshly woken up task is going\n-\t * to be considered on this CPU.)\n-\t */\n-\tif (!sync || cpu != this_cpu)\n-\t\tcheck_preempt_curr(rq, p);\n+\tcheck_preempt_curr(rq, p);\nsuccess = 1;\n\nout_running:\n@@ -1586,28 +1678,20 @@ int fastcall wake_up_state(struct task_s\n*/\nstatic void __sched_fork(struct task_struct *p)\n{\n-\tp->se.wait_start_fair\t\t= 0;\np->se.exec_start\t\t= 0;\np->se.sum_exec_runtime\t\t= 0;\np->se.prev_sum_exec_runtime\t= 0;\n-\tp->se.delta_exec\t\t= 0;\n-\tp->se.delta_fair_run\t\t= 0;\n-\tp->se.delta_fair_sleep\t\t= 0;\n-\tp->se.wait_runtime\t\t= 0;\n-\tp->se.sleep_start_fair\t\t= 0;\n\n#ifdef CONFIG_SCHEDSTATS\np->se.wait_start\t\t= 0;\n-\tp->se.sum_wait_runtime\t\t= 0;\np->se.sum_sleep_runtime\t\t= 0;\np->se.sleep_start\t\t= 0;\np->se.block_start\t\t= 0;\np->se.sleep_max\t\t\t= 0;\np->se.block_max\t\t\t= 0;\np->se.exec_max\t\t\t= 0;\n+\tp->se.slice_max\t\t\t= 0;\np->se.wait_max\t\t\t= 0;\n-\tp->se.wait_runtime_overruns\t= 0;\n-\tp->se.wait_runtime_underruns\t= 0;\n#endif\n\nINIT_LIST_HEAD(&p->run_list);\n@@ -1638,12 +1722,14 @@ void sched_fork(struct task_struct *p, i\n#ifdef CONFIG_SMP\ncpu = sched_balance_self(cpu, SD_BALANCE_FORK);\n#endif\n-\t__set_task_cpu(p, cpu);\n+\tset_task_cpu(p, cpu);\n\n/*\n* Make sure we do not leak PI boosting priority to the child:\n*/\np->prio = current->normal_prio;\n+\tif (!rt_prio(p->prio))\n+\t\tp->sched_class = &fair_sched_class;\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\nif (likely(sched_info_on()))\n@@ -1660,12 +1746,6 @@ void sched_fork(struct task_struct *p, i\n}\n\n/*\n- * After fork, child runs first. (default) If set to 0 then\n- * parent will (try to) run first.\n- */\n-unsigned int __read_mostly sysctl_sched_child_runs_first = 1;\n-\n-/*\n* wake_up_new_task - wake up a newly created task for the first time.\n*\n* This function will do some initial scheduler statistics housekeeping\n@@ -1676,26 +1756,16 @@ void fastcall wake_up_new_task(struct ta\n{\nunsigned long flags;\nstruct rq *rq;\n-\tint this_cpu;\n\nrq = task_rq_lock(p, &flags);\ntrace_mark(kernel_sched_wakeup_new_task, "pid %d state %ld",\np->pid, p->state);\nBUG_ON(p->state != TASK_RUNNING);\n-\tthis_cpu = smp_processor_id(); /* parent\'s CPU */\nupdate_rq_clock(rq);\n\np->prio = effective_prio(p);\n\n-\tif (rt_prio(p->prio))\n-\t\tp->sched_class = &rt_sched_class;\n-\telse\n-\t\tp->sched_class = &fair_sched_class;\n-\n-\tif (!p->sched_class->task_new || !sysctl_sched_child_runs_first ||\n-\t\t\t(clone_flags & CLONE_VM) || task_cpu(p) != this_cpu ||\n-\t\t\t!current->se.on_rq) {\n-\n+\tif (!p->sched_class->task_new || !current->se.on_rq) {\nactivate_task(rq, p, 0);\n} else {\n/*\n@@ -1800,11 +1870,11 @@ prepare_task_switch(struct rq *rq, struc\n* and do any other architecture-specific cleanup actions.\n*\n* Note that we may have delayed dropping an mm in context_switch(). If\n- * so, we finish that here outside of the runqueue lock.  (Doing it\n+ * so, we finish that here outside of the runqueue lock. (Doing it\n* with the lock held can cause deadlocks; see schedule() for\n* details.)\n*/\n-static inline void finish_task_switch(struct rq *rq, struct task_struct *prev)\n+static void finish_task_switch(struct rq *rq, struct task_struct *prev)\n__releases(rq->lock)\n{\nstruct mm_struct *mm = rq->prev_mm;\n@@ -1854,7 +1924,7 @@ asmlinkage void schedule_tail(struct tas\npreempt_enable();\n#endif\nif (current->set_child_tid)\n-\t\tput_user(current->pid, current->set_child_tid);\n+\t\tput_user(task_pid_vnr(current), current->set_child_tid);\n}\n\n/*\n@@ -1989,42 +2059,10 @@ unsigned long nr_active(void)\n*/\nstatic void update_cpu_load(struct rq *this_rq)\n{\n-\tu64 fair_delta64, exec_delta64, idle_delta64, sample_interval64, tmp64;\n-\tunsigned long total_load = this_rq->ls.load.weight;\n-\tunsigned long this_load =  total_load;\n-\tstruct load_stat *ls = &this_rq->ls;\n+\tunsigned long this_load = this_rq->load.weight;\nint i, scale;\n\nthis_rq->nr_load_updates++;\n-\tif (unlikely(!(sysctl_sched_features & SCHED_FEAT_PRECISE_CPU_LOAD)))\n-\t\tgoto do_avg;\n-\n-\t/* Update delta_fair/delta_exec fields first */\n-\tupdate_curr_load(this_rq);\n-\n-\tfair_delta64 = ls->delta_fair + 1;\n-\tls->delta_fair = 0;\n-\n-\texec_delta64 = ls->delta_exec + 1;\n-\tls->delta_exec = 0;\n-\n-\tsample_interval64 = this_rq->clock - ls->load_update_last;\n-\tls->load_update_last = this_rq->clock;\n-\n-\tif ((s64)sample_interval64 < (s64)TICK_NSEC)\n-\t\tsample_interval64 = TICK_NSEC;\n-\n-\tif (exec_delta64 > sample_interval64)\n-\t\texec_delta64 = sample_interval64;\n-\n-\tidle_delta64 = sample_interval64 - exec_delta64;\n-\n-\ttmp64 = div64_64(SCHED_LOAD_SCALE * exec_delta64, fair_delta64);\n-\ttmp64 = div64_64(tmp64 * exec_delta64, sample_interval64);\n-\n-\tthis_load = (unsigned long)tmp64;\n-\n-do_avg:\n\n/* Update our load: */\nfor (i = 0, scale = 1; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n@@ -2034,7 +2072,13 @@ do_avg:\n\nold_load = this_rq->cpu_load[i];\nnew_load = this_load;\n-\n+\t\t/*\n+\t\t * Round up the averaging division if load is increasing. This\n+\t\t * prevents us from getting stuck on 9 if the load is 10, for\n+\t\t * example.\n+\t\t */\n+\t\tif (new_load > old_load)\n+\t\t\tnew_load += scale-1;\nthis_rq->cpu_load[i] = (old_load*(scale-1) + new_load) >> i;\n}\n}\n@@ -2111,7 +2155,7 @@ static void double_lock_balance(struct r\n/*\n* If dest_cpu is allowed for this process, migrate the task to it.\n* This is accomplished by forcing the cpu_allowed mask to only\n- * allow dest_cpu, which will force the cpu onto dest_cpu.  Then\n+ * allow dest_cpu, which will force the cpu onto dest_cpu. Then\n* the cpu_allowed mask is restored.\n*/\nstatic void sched_migrate_task(struct task_struct *p, int dest_cpu)\n@@ -2188,27 +2232,52 @@ int can_migrate_task(struct task_struct\n* 2) cannot be migrated to this CPU due to cpus_allowed, or\n* 3) are cache-hot on their current CPU.\n*/\n-\tif (!cpu_isset(this_cpu, p->cpus_allowed))\n+\tif (!cpu_isset(this_cpu, p->cpus_allowed)) {\n+\t\tschedstat_inc(p, se.nr_failed_migrations_affine);\nreturn 0;\n+\t}\n*all_pinned = 0;\n\n-\tif (task_running(rq, p))\n+\tif (task_running(rq, p)) {\n+\t\tschedstat_inc(p, se.nr_failed_migrations_running);\nreturn 0;\n+\t}\n+\n+\t/*\n+\t * Aggressive migration if:\n+\t * 1) task is cache cold, or\n+\t * 2) too many balance attempts have failed.\n+\t */\n+\n+\tif (!task_hot(p, rq->clock, sd) ||\n+\t\t\tsd->nr_balance_failed > sd->cache_nice_tries) {\n+#ifdef CONFIG_SCHEDSTATS\n+\t\tif (task_hot(p, rq->clock, sd)) {\n+\t\t\tschedstat_inc(sd, lb_hot_gained[idle]);\n+\t\t\tschedstat_inc(p, se.nr_forced_migrations);\n+\t\t}\n+#endif\n+\t\treturn 1;\n+\t}\n\n+\tif (task_hot(p, rq->clock, sd)) {\n+\t\tschedstat_inc(p, se.nr_failed_migrations_hot);\n+\t\treturn 0;\n+\t}\nreturn 1;\n}\n\n-static int balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,\n-\t\t      unsigned long max_nr_move, unsigned long max_load_move,\n-\t\t      struct sched_domain *sd, enum cpu_idle_type idle,\n-\t\t      int *all_pinned, unsigned long *load_moved,\n-\t\t      int *this_best_prio, struct rq_iterator *iterator)\n+static unsigned long\n+balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,\n+\t      unsigned long max_load_move, struct sched_domain *sd,\n+\t      enum cpu_idle_type idle, int *all_pinned,\n+\t      int *this_best_prio, struct rq_iterator *iterator)\n{\n-\tint pulled = 0, pinned = 0, skip_for_load;\n+\tint loops = 0, pulled = 0, pinned = 0, skip_for_load;\nstruct task_struct *p;\nlong rem_load_move = max_load_move;\n\n-\tif (max_nr_move == 0 || max_load_move == 0)\n+\tif (max_load_move == 0)\ngoto out;\n\npinned = 1;\n@@ -2218,10 +2287,10 @@ static int balance_tasks(struct rq *this\n*/\np = iterator->start(iterator->arg);\nnext:\n-\tif (!p)\n+\tif (!p || loops++ > sysctl_sched_nr_migrate)\ngoto out;\n/*\n-\t * To help distribute high priority tasks accross CPUs we don\'t\n+\t * To help distribute high priority tasks across CPUs we don\'t\n* skip a task if it will be the highest priority task (i.e. smallest\n* prio value) on its new queue regardless of its load weight\n*/\n@@ -2238,10 +2307,9 @@ next:\nrem_load_move -= p->se.load.weight;\n\n/*\n-\t * We only want to steal up to the prescribed number of tasks\n-\t * and the prescribed amount of weighted load.\n+\t * We only want to steal up to the prescribed amount of weighted load.\n*/\n-\tif (pulled < max_nr_move && rem_load_move > 0) {\n+\tif (rem_load_move > 0) {\nif (p->prio < *this_best_prio)\n*this_best_prio = p->prio;\np = iterator->next(iterator->arg);\n@@ -2249,7 +2317,7 @@ next:\n}\nout:\n/*\n-\t * Right now, this is the only place pull_task() is called,\n+\t * Right now, this is one of only two places pull_task() is called,\n* so we can safely collect pull_task() stats here rather than\n* inside pull_task().\n*/\n@@ -2257,8 +2325,8 @@ out:\n\nif (all_pinned)\n*all_pinned = pinned;\n-\t*load_moved = max_load_move - rem_load_move;\n-\treturn pulled;\n+\n+\treturn max_load_move - rem_load_move;\n}\n\n/*\n@@ -2273,14 +2341,14 @@ static int move_tasks(struct rq *this_rq\nstruct sched_domain *sd, enum cpu_idle_type idle,\nint *all_pinned)\n{\n-\tstruct sched_class *class = sched_class_highest;\n+\tconst struct sched_class *class = sched_class_highest;\nunsigned long total_load_moved = 0;\nint this_best_prio = this_rq->curr->prio;\n\ndo {\ntotal_load_moved +=\nclass->load_balance(this_rq, this_cpu, busiest,\n-\t\t\t\tULONG_MAX, max_load_move - total_load_moved,\n+\t\t\t\tmax_load_move - total_load_moved,\nsd, idle, all_pinned, &this_best_prio);\nclass = class->next;\n} while (class && max_load_move > total_load_moved);\n@@ -2288,6 +2356,32 @@ static int move_tasks(struct rq *this_rq\nreturn total_load_moved > 0;\n}\n\n+static int\n+iter_move_one_task(struct rq *this_rq, int this_cpu, struct rq *busiest,\n+\t\t   struct sched_domain *sd, enum cpu_idle_type idle,\n+\t\t   struct rq_iterator *iterator)\n+{\n+\tstruct task_struct *p = iterator->start(iterator->arg);\n+\tint pinned = 0;\n+\n+\twhile (p) {\n+\t\tif (can_migrate_task(p, busiest, this_cpu, sd, idle, &pinned)) {\n+\t\t\tpull_task(busiest, p, this_rq, this_cpu);\n+\t\t\t/*\n+\t\t\t * Right now, this is only the second place pull_task()\n+\t\t\t * is called, so we can safely collect pull_task()\n+\t\t\t * stats here rather than inside pull_task().\n+\t\t\t */\n+\t\t\tschedstat_inc(sd, lb_gained[idle]);\n+\n+\t\t\treturn 1;\n+\t\t}\n+\t\tp = iterator->next(iterator->arg);\n+\t}\n+\n+\treturn 0;\n+}\n+\n/*\n* move_one_task tries to move exactly one task from busiest to this_rq, as\n* part of active balancing operations within "domain".\n@@ -2298,13 +2392,10 @@ static int move_tasks(struct rq *this_rq\nstatic int move_one_task(struct rq *this_rq, int this_cpu, struct rq *busiest,\nstruct sched_domain *sd, enum cpu_idle_type idle)\n{\n-\tstruct sched_class *class;\n-\tint this_best_prio = MAX_PRIO;\n+\tconst struct sched_class *class;\n\nfor (class = sched_class_highest; class; class = class->next)\n-\t\tif (class->load_balance(this_rq, this_cpu, busiest,\n-\t\t\t\t\t1, ULONG_MAX, sd, idle, NULL,\n-\t\t\t\t\t&this_best_prio))\n+\t\tif (class->move_one_task(this_rq, this_cpu, busiest, sd, idle))\nreturn 1;\n\nreturn 0;\n@@ -2325,7 +2416,7 @@ find_busiest_group(struct sched_domain *\nunsigned long max_pull;\nunsigned long busiest_load_per_task, busiest_nr_running;\nunsigned long this_load_per_task, this_nr_running;\n-\tint load_idx;\n+\tint load_idx, group_imb = 0;\n#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)\nint power_savings_balance = 1;\nunsigned long leader_nr_running = 0, min_load_per_task = 0;\n@@ -2344,9 +2435,10 @@ find_busiest_group(struct sched_domain *\nload_idx = sd->idle_idx;\n\ndo {\n-\t\tunsigned long load, group_capacity;\n+\t\tunsigned long load, group_capacity, max_cpu_load, min_cpu_load;\nint local_group;\nint i;\n+\t\tint __group_imb = 0;\nunsigned int balance_cpu = -1, first_idle_cpu = 0;\nunsigned long sum_nr_running, sum_weighted_load;\n\n@@ -2357,6 +2449,8 @@ find_busiest_group(struct sched_domain *\n\n/* Tally up the load of all CPUs in the group */\nsum_weighted_load = sum_nr_running = avg_load = 0;\n+\t\tmax_cpu_load = 0;\n+\t\tmin_cpu_load = ~0UL;\n\nfor_each_cpu_mask(i, group->cpumask) {\nstruct rq *rq;\n@@ -2377,8 +2471,13 @@ find_busiest_group(struct sched_domain *\n}\n\nload = target_load(i, load_idx);\n-\t\t\t} else\n+\t\t\t} else {\nload = source_load(i, load_idx);\n+\t\t\t\tif (load > max_cpu_load)\n+\t\t\t\t\tmax_cpu_load = load;\n+\t\t\t\tif (min_cpu_load > load)\n+\t\t\t\t\tmin_cpu_load = load;\n+\t\t\t}\n\navg_load += load;\nsum_nr_running += rq->nr_running;\n@@ -2404,6 +2503,9 @@ find_busiest_group(struct sched_domain *\navg_load = sg_div_cpu_power(group,\navg_load * SCHED_LOAD_SCALE);\n\n+\t\tif ((max_cpu_load - min_cpu_load) > SCHED_LOAD_SCALE)\n+\t\t\t__group_imb = 1;\n+\ngroup_capacity = group->__cpu_power / SCHED_LOAD_SCALE;\n\nif (local_group) {\n@@ -2412,11 +2514,12 @@ find_busiest_group(struct sched_domain *\nthis_nr_running = sum_nr_running;\nthis_load_per_task = sum_weighted_load;\n} else if (avg_load > max_load &&\n-\t\t\t   sum_nr_running > group_capacity) {\n+\t\t\t   (sum_nr_running > group_capacity || __group_imb)) {\nmax_load = avg_load;\nbusiest = group;\nbusiest_nr_running = sum_nr_running;\nbusiest_load_per_task = sum_weighted_load;\n+\t\t\tgroup_imb = __group_imb;\n}\n\n#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)\n@@ -2488,6 +2591,9 @@ group_next:\ngoto out_balanced;\n\nbusiest_load_per_task /= busiest_nr_running;\n+\tif (group_imb)\n+\t\tbusiest_load_per_task = min(busiest_load_per_task, avg_load);\n+\n/*\n* We\'re trying to get all the cpus to the average_load, so we don\'t\n* want to push ourselves above the average load, nor do we wish to\n@@ -2496,7 +2602,7 @@ group_next:\n* tasks around. Thus we look for the minimum possible imbalance.\n* Negative imbalances (*we* are more loaded than anyone else) will\n* be counted as no imbalance for these purposes -- we can\'t fix that\n-\t * by pulling tasks to us.  Be careful of negative numbers as they\'ll\n+\t * by pulling tasks to us. Be careful of negative numbers as they\'ll\n* appear as very large values with unsigned longs.\n*/\nif (max_load <= busiest_load_per_task)\n@@ -2662,7 +2768,7 @@ static int load_balance(int this_cpu, st\n!test_sd_parent(sd, SD_POWERSAVINGS_BALANCE))\nsd_idle = 1;\n\n-\tschedstat_inc(sd, lb_cnt[idle]);\n+\tschedstat_inc(sd, lb_count[idle]);\n\nredo:\ngroup = find_busiest_group(sd, this_cpu, &imbalance, idle, &sd_idle,\n@@ -2815,7 +2921,7 @@ load_balance_newidle(int this_cpu, struc\n!test_sd_parent(sd, SD_POWERSAVINGS_BALANCE))\nsd_idle = 1;\n\n-\tschedstat_inc(sd, lb_cnt[CPU_NEWLY_IDLE]);\n+\tschedstat_inc(sd, lb_count[CPU_NEWLY_IDLE]);\nredo:\ngroup = find_busiest_group(sd, this_cpu, &imbalance, CPU_NEWLY_IDLE,\n&sd_idle, &cpus, NULL);\n@@ -2931,7 +3037,7 @@ static void active_load_balance(struct r\n\n/*\n* This condition is "impossible", if it occurs\n-\t * we need to fix it.  Originally reported by\n+\t * we need to fix it. Originally reported by\n* Bjorn Helgaas on a 128-cpu setup.\n*/\nBUG_ON(busiest_rq == target_rq);\n@@ -2949,7 +3055,7 @@ static void active_load_balance(struct r\n}\n\nif (likely(sd)) {\n-\t\tschedstat_inc(sd, alb_cnt);\n+\t\tschedstat_inc(sd, alb_count);\n\nif (move_one_task(target_rq, target_cpu, busiest_rq,\nsd, CPU_IDLE))\n@@ -2963,7 +3069,7 @@ static void active_load_balance(struct r\n#ifdef CONFIG_NO_HZ\nstatic struct {\natomic_t load_balancer;\n-\tcpumask_t  cpu_mask;\n+\tcpumask_t cpu_mask;\n} nohz ____cacheline_aligned = {\n.load_balancer = ATOMIC_INIT(-1),\n.cpu_mask = CPU_MASK_NONE,\n@@ -3042,7 +3148,7 @@ static DEFINE_SPINLOCK(balancing);\n*\n* Balancing parameters are set up in arch_init_sched_domains.\n*/\n-static inline void rebalance_domains(int cpu, enum cpu_idle_type idle)\n+static void rebalance_domains(int cpu, enum cpu_idle_type idle)\n{\nint balance = 1;\nstruct rq *rq = cpu_rq(cpu);\n@@ -3226,18 +3332,6 @@ static inline void idle_balance(int cpu,\n{\n}\n\n-/* Avoid "used but not defined" warning on UP */\n-static int balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,\n-\t\t      unsigned long max_nr_move, unsigned long max_load_move,\n-\t\t      struct sched_domain *sd, enum cpu_idle_type idle,\n-\t\t      int *all_pinned, unsigned long *load_moved,\n-\t\t      int *this_best_prio, struct rq_iterator *iterator)\n-{\n-\t*load_moved = 0;\n-\n-\treturn 0;\n-}\n-\n#endif\n\nDEFINE_PER_CPU(struct kernel_stat, kstat);\n@@ -3256,7 +3350,7 @@ unsigned long long task_sched_runtime(st\n\nrq = task_rq_lock(p, &flags);\nns = p->se.sum_exec_runtime;\n-\tif (rq->curr == p) {\n+\tif (task_current(rq, p)) {\nupdate_rq_clock(rq);\ndelta_exec = rq->clock - p->se.exec_start;\nif ((s64)delta_exec > 0)\n@@ -3270,7 +3364,6 @@ unsigned long long task_sched_runtime(st\n/*\n* Account user cpu time to a process.\n* @p: the process that the cpu time gets accounted to\n- * @hardirq_offset: the offset to subtract from hardirq_count()\n* @cputime: the cpu time spent in user space since the last update\n*/\nvoid account_user_time(struct task_struct *p, cputime_t cputime)\n@@ -3289,6 +3382,35 @@ void account_user_time(struct task_struc\n}\n\n/*\n+ * Account guest cpu time to a process.\n+ * @p: the process that the cpu time gets accounted to\n+ * @cputime: the cpu time spent in virtual machine since the last update\n+ */\n+static void account_guest_time(struct task_struct *p, cputime_t cputime)\n+{\n+\tcputime64_t tmp;\n+\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n+\n+\ttmp = cputime_to_cputime64(cputime);\n+\n+\tp->utime = cputime_add(p->utime, cputime);\n+\tp->gtime = cputime_add(p->gtime, cputime);\n+\n+\tcpustat->user = cputime64_add(cpustat->user, tmp);\n+\tcpustat->guest = cputime64_add(cpustat->guest, tmp);\n+}\n+\n+/*\n+ * Account scaled user cpu time to a process.\n+ * @p: the process that the cpu time gets accounted to\n+ * @cputime: the cpu time spent in user space since the last update\n+ */\n+void account_user_time_scaled(struct task_struct *p, cputime_t cputime)\n+{\n+\tp->utimescaled = cputime_add(p->utimescaled, cputime);\n+}\n+\n+/*\n* Account system cpu time to a process.\n* @p: the process that the cpu time gets accounted to\n* @hardirq_offset: the offset to subtract from hardirq_count()\n@@ -3301,6 +3423,9 @@ void account_system_time(struct task_str\nstruct rq *rq = this_rq();\ncputime64_t tmp;\n\n+\tif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0))\n+\t\treturn account_guest_time(p, cputime);\n+\np->stime = cputime_add(p->stime, cputime);\n\n/* Add system time to cpustat. */\n@@ -3320,6 +3445,17 @@ void account_system_time(struct task_str\n}\n\n/*\n+ * Account scaled system cpu time to a process.\n+ * @p: the process that the cpu time gets accounted to\n+ * @hardirq_offset: the offset to subtract from hardirq_count()\n+ * @cputime: the cpu time spent in kernel space since the last update\n+ */\n+void account_system_time_scaled(struct task_struct *p, cputime_t cputime)\n+{\n+\tp->stimescaled = cputime_add(p->stimescaled, cputime);\n+}\n+\n+/*\n* Account for involuntary wait time.\n* @p: the process from which the cpu time has been stolen\n* @steal: the cpu time spent in involuntary wait\n@@ -3416,12 +3552,19 @@ EXPORT_SYMBOL(sub_preempt_count);\n*/\nstatic noinline void __schedule_bug(struct task_struct *prev)\n{\n-\tprintk(KERN_ERR "BUG: scheduling while atomic: %s/0x%08x/%d\\n",\n-\t\tprev->comm, preempt_count(), prev->pid);\n+\tstruct pt_regs *regs = get_irq_regs();\n+\n+\tprintk(KERN_ERR "BUG: scheduling while atomic: %s/%d/0x%08x\\n",\n+\t\tprev->comm, prev->pid, preempt_count());\n+\ndebug_show_held_locks(prev);\nif (irqs_disabled())\nprint_irqtrace_events(prev);\n-\tdump_stack();\n+\n+\tif (regs)\n+\t\tshow_regs(regs);\n+\telse\n+\t\tdump_stack();\n}\n\n/*\n@@ -3430,7 +3573,7 @@ static noinline void __schedule_bug(stru\nstatic inline void schedule_debug(struct task_struct *prev)\n{\n/*\n-\t * Test if we are atomic.  Since do_exit() needs to call into\n+\t * Test if we are atomic. Since do_exit() needs to call into\n* schedule() atomically, we ignore that path for now.\n* Otherwise, whine if we are scheduling when we should not be.\n*/\n@@ -3439,7 +3582,13 @@ static inline void schedule_debug(struct\n\nprofile_hit(SCHED_PROFILING, __builtin_return_address(0));\n\n-\tschedstat_inc(this_rq(), sched_cnt);\n+\tschedstat_inc(this_rq(), sched_count);\n+#ifdef CONFIG_SCHEDSTATS\n+\tif (unlikely(prev->lock_depth >= 0)) {\n+\t\tschedstat_inc(this_rq(), bkl_count);\n+\t\tschedstat_inc(prev, sched_info.bkl_count);\n+\t}\n+#endif\n}\n\n/*\n@@ -3448,7 +3597,7 @@ static inline void schedule_debug(struct\nstatic inline struct task_struct *\npick_next_task(struct rq *rq, struct task_struct *prev)\n{\n-\tstruct sched_class *class;\n+\tconst struct sched_class *class;\nstruct task_struct *p;\n\n/*\n@@ -3497,9 +3646,13 @@ need_resched_nonpreemptible:\n\nschedule_debug(prev);\n\n-\tspin_lock_irq(&rq->lock);\n-\tclear_tsk_need_resched(prev);\n+\t/*\n+\t * Do the rq-clock update outside the rq lock:\n+\t */\n+\tlocal_irq_disable();\n__update_rq_clock(rq);\n+\tspin_lock(&rq->lock);\n+\tclear_tsk_need_resched(prev);\n\nif (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {\nif (unlikely((prev->state & TASK_INTERRUPTIBLE) &&\n@@ -3542,7 +3695,7 @@ EXPORT_SYMBOL(schedule);\n#ifdef CONFIG_PREEMPT\n/*\n* this is the entry point to schedule() from in-kernel preemption\n- * off of preempt_enable.  Kernel preemptions off return from interrupt\n+ * off of preempt_enable. Kernel preemptions off return from interrupt\n* occur there and call schedule directly.\n*/\nasmlinkage void __sched preempt_schedule(void)\n@@ -3554,32 +3707,35 @@ asmlinkage void __sched preempt_schedule\n#endif\n/*\n* If there is a non-zero preempt_count or interrupts are disabled,\n-\t * we do not want to preempt the current task.  Just return..\n+\t * we do not want to preempt the current task. Just return..\n*/\nif (likely(ti->preempt_count || irqs_disabled()))\nreturn;\n\n-need_resched:\n-\tadd_preempt_count(PREEMPT_ACTIVE);\n-\t/*\n-\t * We keep the big kernel semaphore locked, but we\n-\t * clear ->lock_depth so that schedule() doesnt\n-\t * auto-release the semaphore:\n-\t */\n+\tdo {\n+\t\tadd_preempt_count(PREEMPT_ACTIVE);\n+\n+\t\t/*\n+\t\t * We keep the big kernel semaphore locked, but we\n+\t\t * clear ->lock_depth so that schedule() doesnt\n+\t\t * auto-release the semaphore:\n+\t\t */\n#ifdef CONFIG_PREEMPT_BKL\n-\tsaved_lock_depth = task->lock_depth;\n-\ttask->lock_depth = -1;\n+\t\tsaved_lock_depth = task->lock_depth;\n+\t\ttask->lock_depth = -1;\n#endif\n-\tschedule();\n+\t\tschedule();\n#ifdef CONFIG_PREEMPT_BKL\n-\ttask->lock_depth = saved_lock_depth;\n+\t\ttask->lock_depth = saved_lock_depth;\n#endif\n-\tsub_preempt_count(PREEMPT_ACTIVE);\n+\t\tsub_preempt_count(PREEMPT_ACTIVE);\n\n-\t/* we could miss a preemption opportunity between schedule and now */\n-\tbarrier();\n-\tif (unlikely(test_thread_flag(TIF_NEED_RESCHED)))\n-\t\tgoto need_resched;\n+\t\t/*\n+\t\t * Check again in case we missed a preemption opportunity\n+\t\t * between schedule and now.\n+\t\t */\n+\t\tbarrier();\n+\t} while (unlikely(test_thread_flag(TIF_NEED_RESCHED)));\n}\nEXPORT_SYMBOL(preempt_schedule);\n\n@@ -3599,29 +3755,32 @@ asmlinkage void __sched preempt_schedule\n/* Catch callers which need to be fixed */\nBUG_ON(ti->preempt_count || !irqs_disabled());\n\n-need_resched:\n-\tadd_preempt_count(PREEMPT_ACTIVE);\n-\t/*\n-\t * We keep the big kernel semaphore locked, but we\n-\t * clear ->lock_depth so that schedule() doesnt\n-\t * auto-release the semaphore:\n-\t */\n+\tdo {\n+\t\tadd_preempt_count(PREEMPT_ACTIVE);\n+\n+\t\t/*\n+\t\t * We keep the big kernel semaphore locked, but we\n+\t\t * clear ->lock_depth so that schedule() doesnt\n+\t\t * auto-release the semaphore:\n+\t\t */\n#ifdef CONFIG_PREEMPT_BKL\n-\tsaved_lock_depth = task->lock_depth;\n-\ttask->lock_depth = -1;\n+\t\tsaved_lock_depth = task->lock_depth;\n+\t\ttask->lock_depth = -1;\n#endif\n-\tlocal_irq_enable();\n-\tschedule();\n-\tlocal_irq_disable();\n+\t\tlocal_irq_enable();\n+\t\tschedule();\n+\t\tlocal_irq_disable();\n#ifdef CONFIG_PREEMPT_BKL\n-\ttask->lock_depth = saved_lock_depth;\n+\t\ttask->lock_depth = saved_lock_depth;\n#endif\n-\tsub_preempt_count(PREEMPT_ACTIVE);\n+\t\tsub_preempt_count(PREEMPT_ACTIVE);\n\n-\t/* we could miss a preemption opportunity between schedule and now */\n-\tbarrier();\n-\tif (unlikely(test_thread_flag(TIF_NEED_RESCHED)))\n-\t\tgoto need_resched;\n+\t\t/*\n+\t\t * Check again in case we missed a preemption opportunity\n+\t\t * between schedule and now.\n+\t\t */\n+\t\tbarrier();\n+\t} while (unlikely(test_thread_flag(TIF_NEED_RESCHED)));\n}\n\n#endif /* CONFIG_PREEMPT */\n@@ -3634,21 +3793,20 @@ int default_wake_function(wait_queue_t *\nEXPORT_SYMBOL(default_wake_function);\n\n/*\n- * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just\n- * wake everything up.  If it\'s an exclusive wakeup (nr_exclusive == small +ve\n+ * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just\n+ * wake everything up. If it\'s an exclusive wakeup (nr_exclusive == small +ve\n* number) then we wake all the non-exclusive tasks and one exclusive task.\n*\n* There are circumstances in which we can try to wake a task which has already\n- * started to run but is not in state TASK_RUNNING.  try_to_wake_up() returns\n+ * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns\n* zero in this (rare) case, and we handle it by continuing to scan the queue.\n*/\nstatic void __wake_up_common(wait_queue_head_t *q, unsigned int mode,\nint nr_exclusive, int sync, void *key)\n{\n-\tstruct list_head *tmp, *next;\n+\twait_queue_t *curr, *next;\n\n-\tlist_for_each_safe(tmp, next, &q->task_list) {\n-\t\twait_queue_t *curr = list_entry(tmp, wait_queue_t, task_list);\n+\tlist_for_each_entry_safe(curr, next, &q->task_list, task_list) {\nunsigned flags = curr->flags;\n\nif (curr->func(curr, mode, sync, key) &&\n@@ -3714,7 +3872,7 @@ __wake_up_sync(wait_queue_head_t *q, uns\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync);\t/* For internal use only */\n\n-void fastcall complete(struct completion *x)\n+void complete(struct completion *x)\n{\nunsigned long flags;\n\n@@ -3726,7 +3884,7 @@ void fastcall complete(struct completion\n}\nEXPORT_SYMBOL(complete);\n\n-void fastcall complete_all(struct completion *x)\n+void complete_all(struct completion *x)\n{\nunsigned long flags;\n\n@@ -3738,206 +3896,119 @@ void fastcall complete_all(struct comple\n}\nEXPORT_SYMBOL(complete_all);\n\n-void fastcall __sched wait_for_completion(struct completion *x)\n-{\n-\tmight_sleep();\n-\n-\tspin_lock_irq(&x->wait.lock);\n-\tif (!x->done) {\n-\t\tDECLARE_WAITQUEUE(wait, current);\n-\n-\t\twait.flags |= WQ_FLAG_EXCLUSIVE;\n-\t\t__add_wait_queue_tail(&x->wait, &wait);\n-\t\tdo {\n-\t\t\t__set_current_state(TASK_UNINTERRUPTIBLE);\n-\t\t\tspin_unlock_irq(&x->wait.lock);\n-\t\t\tschedule();\n-\t\t\tspin_lock_irq(&x->wait.lock);\n-\t\t} while (!x->done);\n-\t\t__remove_wait_queue(&x->wait, &wait);\n-\t}\n-\tx->done--;\n-\tspin_unlock_irq(&x->wait.lock);\n-}\n-EXPORT_SYMBOL(wait_for_completion);\n-\n-unsigned long fastcall __sched\n-wait_for_completion_timeout(struct completion *x, unsigned long timeout)\n+static inline long __sched\n+do_wait_for_common(struct completion *x, long timeout, int state)\n{\n-\tmight_sleep();\n-\n-\tspin_lock_irq(&x->wait.lock);\nif (!x->done) {\nDECLARE_WAITQUEUE(wait, current);\n\nwait.flags |= WQ_FLAG_EXCLUSIVE;\n__add_wait_queue_tail(&x->wait, &wait);\ndo {\n-\t\t\t__set_current_state(TASK_UNINTERRUPTIBLE);\n+\t\t\tif (state == TASK_INTERRUPTIBLE &&\n+\t\t\t    signal_pending(current)) {\n+\t\t\t\t__remove_wait_queue(&x->wait, &wait);\n+\t\t\t\treturn -ERESTARTSYS;\n+\t\t\t}\n+\t\t\t__set_current_state(state);\nspin_unlock_irq(&x->wait.lock);\ntimeout = schedule_timeout(timeout);\nspin_lock_irq(&x->wait.lock);\nif (!timeout) {\n__remove_wait_queue(&x->wait, &wait);\n-\t\t\t\tgoto out;\n+\t\t\t\treturn timeout;\n}\n} while (!x->done);\n__remove_wait_queue(&x->wait, &wait);\n}\nx->done--;\n-out:\n-\tspin_unlock_irq(&x->wait.lock);\nreturn timeout;\n}\n-EXPORT_SYMBOL(wait_for_completion_timeout);\n\n-int fastcall __sched wait_for_completion_interruptible(struct completion *x)\n+static long __sched\n+wait_for_common(struct completion *x, long timeout, int state)\n{\n-\tint ret = 0;\n-\nmight_sleep();\n\nspin_lock_irq(&x->wait.lock);\n-\tif (!x->done) {\n-\t\tDECLARE_WAITQUEUE(wait, current);\n-\n-\t\twait.flags |= WQ_FLAG_EXCLUSIVE;\n-\t\t__add_wait_queue_tail(&x->wait, &wait);\n-\t\tdo {\n-\t\t\tif (signal_pending(current)) {\n-\t\t\t\tret = -ERESTARTSYS;\n-\t\t\t\t__remove_wait_queue(&x->wait, &wait);\n-\t\t\t\tgoto out;\n-\t\t\t}\n-\t\t\t__set_current_state(TASK_INTERRUPTIBLE);\n-\t\t\tspin_unlock_irq(&x->wait.lock);\n-\t\t\tschedule();\n-\t\t\tspin_lock_irq(&x->wait.lock);\n-\t\t} while (!x->done);\n-\t\t__remove_wait_queue(&x->wait, &wait);\n-\t}\n-\tx->done--;\n-out:\n+\ttimeout = do_wait_for_common(x, timeout, state);\nspin_unlock_irq(&x->wait.lock);\n-\n-\treturn ret;\n+\treturn timeout;\n}\n-EXPORT_SYMBOL(wait_for_completion_interruptible);\n\n-unsigned long fastcall __sched\n-wait_for_completion_interruptible_timeout(struct completion *x,\n-\t\t\t\t\t  unsigned long timeout)\n+void __sched wait_for_completion(struct completion *x)\n{\n-\tmight_sleep();\n-\n-\tspin_lock_irq(&x->wait.lock);\n-\tif (!x->done) {\n-\t\tDECLARE_WAITQUEUE(wait, current);\n+\twait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);\n+}\n+EXPORT_SYMBOL(wait_for_completion);\n\n-\t\twait.flags |= WQ_FLAG_EXCLUSIVE;\n-\t\t__add_wait_queue_tail(&x->wait, &wait);\n-\t\tdo {\n-\t\t\tif (signal_pending(current)) {\n-\t\t\t\ttimeout = -ERESTARTSYS;\n-\t\t\t\t__remove_wait_queue(&x->wait, &wait);\n-\t\t\t\tgoto out;\n-\t\t\t}\n-\t\t\t__set_current_state(TASK_INTERRUPTIBLE);\n-\t\t\tspin_unlock_irq(&x->wait.lock);\n-\t\t\ttimeout = schedule_timeout(timeout);\n-\t\t\tspin_lock_irq(&x->wait.lock);\n-\t\t\tif (!timeout) {\n-\t\t\t\t__remove_wait_queue(&x->wait, &wait);\n-\t\t\t\tgoto out;\n-\t\t\t}\n-\t\t} while (!x->done);\n-\t\t__remove_wait_queue(&x->wait, &wait);\n-\t}\n-\tx->done--;\n-out:\n-\tspin_unlock_irq(&x->wait.lock);\n-\treturn timeout;\n+unsigned long __sched\n+wait_for_completion_timeout(struct completion *x, unsigned long timeout)\n+{\n+\treturn wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);\n}\n-EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);\n+EXPORT_SYMBOL(wait_for_completion_timeout);\n\n-static inline void\n-sleep_on_head(wait_queue_head_t *q, wait_queue_t *wait, unsigned long *flags)\n+int __sched wait_for_completion_interruptible(struct completion *x)\n{\n-\tspin_lock_irqsave(&q->lock, *flags);\n-\t__add_wait_queue(q, wait);\n-\tspin_unlock(&q->lock);\n+\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);\n+\tif (t == -ERESTARTSYS)\n+\t\treturn t;\n+\treturn 0;\n}\n+EXPORT_SYMBOL(wait_for_completion_interruptible);\n\n-static inline void\n-sleep_on_tail(wait_queue_head_t *q, wait_queue_t *wait, unsigned long *flags)\n+unsigned long __sched\n+wait_for_completion_interruptible_timeout(struct completion *x,\n+\t\t\t\t\t  unsigned long timeout)\n{\n-\tspin_lock_irq(&q->lock);\n-\t__remove_wait_queue(q, wait);\n-\tspin_unlock_irqrestore(&q->lock, *flags);\n+\treturn wait_for_common(x, timeout, TASK_INTERRUPTIBLE);\n}\n+EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);\n\n-void __sched interruptible_sleep_on(wait_queue_head_t *q)\n+static long __sched\n+sleep_on_common(wait_queue_head_t *q, int state, long timeout)\n{\nunsigned long flags;\nwait_queue_t wait;\n\ninit_waitqueue_entry(&wait, current);\n\n-\tcurrent->state = TASK_INTERRUPTIBLE;\n+\t__set_current_state(state);\n\n-\tsleep_on_head(q, &wait, &flags);\n-\tschedule();\n-\tsleep_on_tail(q, &wait, &flags);\n+\tspin_lock_irqsave(&q->lock, flags);\n+\t__add_wait_queue(q, &wait);\n+\tspin_unlock(&q->lock);\n+\ttimeout = schedule_timeout(timeout);\n+\tspin_lock_irq(&q->lock);\n+\t__remove_wait_queue(q, &wait);\n+\tspin_unlock_irqrestore(&q->lock, flags);\n+\n+\treturn timeout;\n+}\n+\n+void __sched interruptible_sleep_on(wait_queue_head_t *q)\n+{\n+\tsleep_on_common(q, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(interruptible_sleep_on);\n\nlong __sched\ninterruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n-\tunsigned long flags;\n-\twait_queue_t wait;\n-\n-\tinit_waitqueue_entry(&wait, current);\n-\n-\tcurrent->state = TASK_INTERRUPTIBLE;\n-\n-\tsleep_on_head(q, &wait, &flags);\n-\ttimeout = schedule_timeout(timeout);\n-\tsleep_on_tail(q, &wait, &flags);\n-\n-\treturn timeout;\n+\treturn sleep_on_common(q, TASK_INTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(interruptible_sleep_on_timeout);\n\nvoid __sched sleep_on(wait_queue_head_t *q)\n{\n-\tunsigned long flags;\n-\twait_queue_t wait;\n-\n-\tinit_waitqueue_entry(&wait, current);\n-\n-\tcurrent->state = TASK_UNINTERRUPTIBLE;\n-\n-\tsleep_on_head(q, &wait, &flags);\n-\tschedule();\n-\tsleep_on_tail(q, &wait, &flags);\n+\tsleep_on_common(q, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(sleep_on);\n\nlong __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n-\tunsigned long flags;\n-\twait_queue_t wait;\n-\n-\tinit_waitqueue_entry(&wait, current);\n-\n-\tcurrent->state = TASK_UNINTERRUPTIBLE;\n-\n-\tsleep_on_head(q, &wait, &flags);\n-\ttimeout = schedule_timeout(timeout);\n-\tsleep_on_tail(q, &wait, &flags);\n-\n-\treturn timeout;\n+\treturn sleep_on_common(q, TASK_UNINTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(sleep_on_timeout);\n\n@@ -3956,7 +4027,7 @@ EXPORT_SYMBOL(sleep_on_timeout);\nvoid rt_mutex_setprio(struct task_struct *p, int prio)\n{\nunsigned long flags;\n-\tint oldprio, on_rq;\n+\tint oldprio, on_rq, running;\nstruct rq *rq;\n\nBUG_ON(prio < 0 || prio > MAX_PRIO);\n@@ -3966,8 +4037,12 @@ void rt_mutex_setprio(struct task_struct\n\noldprio = p->prio;\non_rq = p->se.on_rq;\n-\tif (on_rq)\n+\trunning = task_current(rq, p);\n+\tif (on_rq) {\ndequeue_task(rq, p, 0);\n+\t\tif (running)\n+\t\t\tp->sched_class->put_prev_task(rq, p);\n+\t}\n\nif (rt_prio(prio))\np->sched_class = &rt_sched_class;\n@@ -3977,13 +4052,15 @@ void rt_mutex_setprio(struct task_struct\np->prio = prio;\n\nif (on_rq) {\n+\t\tif (running)\n+\t\t\tp->sched_class->set_curr_task(rq);\nenqueue_task(rq, p, 0);\n/*\n* Reschedule if we are currently running on this runqueue and\n* our priority decreased, or if we are not currently running on\n* this runqueue and our priority is higher than the current\'s\n*/\n-\t\tif (task_running(rq, p)) {\n+\t\tif (running) {\nif (p->prio > oldprio)\nresched_task(rq->curr);\n} else {\n@@ -4147,9 +4224,9 @@ struct task_struct *idle_task(int cpu)\n* find_process_by_pid - find a process with a matching PID value.\n* @pid: the pid in question.\n*/\n-static inline struct task_struct *find_process_by_pid(pid_t pid)\n+static struct task_struct *find_process_by_pid(pid_t pid)\n{\n-\treturn pid ? find_task_by_pid(pid) : current;\n+\treturn pid ? find_task_by_vpid(pid) : current;\n}\n\n/* Actually do priority change: must hold rq lock. */\n@@ -4189,7 +4266,7 @@ __setscheduler(struct rq *rq, struct tas\nint sched_setscheduler(struct task_struct *p, int policy,\nstruct sched_param *param)\n{\n-\tint retval, oldprio, oldpolicy = -1, on_rq;\n+\tint retval, oldprio, oldpolicy = -1, on_rq, running;\nunsigned long flags;\nstruct rq *rq;\n\n@@ -4271,18 +4348,26 @@ recheck:\n}\nupdate_rq_clock(rq);\non_rq = p->se.on_rq;\n-\tif (on_rq)\n+\trunning = task_current(rq, p);\n+\tif (on_rq) {\ndeactivate_task(rq, p, 0);\n+\t\tif (running)\n+\t\t\tp->sched_class->put_prev_task(rq, p);\n+\t}\n+\noldprio = p->prio;\n__setscheduler(rq, p, policy, param->sched_priority);\n+\nif (on_rq) {\n+\t\tif (running)\n+\t\t\tp->sched_class->set_curr_task(rq);\nactivate_task(rq, p, 0);\n/*\n* Reschedule if we are currently running on this runqueue and\n* our priority decreased, or if we are not currently running on\n* this runqueue and our priority is higher than the current\'s\n*/\n-\t\tif (task_running(rq, p)) {\n+\t\tif (running) {\nif (p->prio > oldprio)\nresched_task(rq->curr);\n} else {\n@@ -4326,8 +4411,8 @@ do_sched_setscheduler(pid_t pid, int pol\n* @policy: new policy.\n* @param: structure containing the new RT priority.\n*/\n-asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,\n-\t\t\t\t       struct sched_param __user *param)\n+asmlinkage long\n+sys_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)\n{\n/* negative values for policy are not valid */\nif (policy < 0)\n@@ -4353,10 +4438,10 @@ asmlinkage long sys_sched_setparam(pid_t\nasmlinkage long sys_sched_getscheduler(pid_t pid)\n{\nstruct task_struct *p;\n-\tint retval = -EINVAL;\n+\tint retval;\n\nif (pid < 0)\n-\t\tgoto out_nounlock;\n+\t\treturn -EINVAL;\n\nretval = -ESRCH;\nread_lock(&tasklist_lock);\n@@ -4367,8 +4452,6 @@ asmlinkage long sys_sched_getscheduler(p\nretval = p->policy;\n}\nread_unlock(&tasklist_lock);\n-\n-out_nounlock:\nreturn retval;\n}\n\n@@ -4381,10 +4464,10 @@ asmlinkage long sys_sched_getparam(pid_t\n{\nstruct sched_param lp;\nstruct task_struct *p;\n-\tint retval = -EINVAL;\n+\tint retval;\n\nif (!param || pid < 0)\n-\t\tgoto out_nounlock;\n+\t\treturn -EINVAL;\n\nread_lock(&tasklist_lock);\np = find_process_by_pid(pid);\n@@ -4404,7 +4487,6 @@ asmlinkage long sys_sched_getparam(pid_t\n*/\nretval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;\n\n-out_nounlock:\nreturn retval;\n\nout_unlock:\n@@ -4430,7 +4512,7 @@ long sched_setaffinity(pid_t pid, cpumas\n\n/*\n* It is not safe to call set_cpus_allowed with the\n-\t * tasklist_lock held.  We will bump the task_struct\'s\n+\t * tasklist_lock held. We will bump the task_struct\'s\n* usage count and then drop tasklist_lock.\n*/\nget_task_struct(p);\n@@ -4447,8 +4529,21 @@ long sched_setaffinity(pid_t pid, cpumas\n\ncpus_allowed = cpuset_cpus_allowed(p);\ncpus_and(new_mask, new_mask, cpus_allowed);\n+ again:\nretval = set_cpus_allowed(p, new_mask);\n\n+\tif (!retval) {\n+\t\tcpus_allowed = cpuset_cpus_allowed(p);\n+\t\tif (!cpus_subset(new_mask, cpus_allowed)) {\n+\t\t\t/*\n+\t\t\t * We must have raced with a concurrent cpuset\n+\t\t\t * update. Just reset the cpus_allowed to the\n+\t\t\t * cpuset\'s cpus_allowed\n+\t\t\t */\n+\t\t\tnew_mask = cpus_allowed;\n+\t\t\tgoto again;\n+\t\t}\n+\t}\nout_unlock:\nput_task_struct(p);\nmutex_unlock(&sched_hotcpu_mutex);\n@@ -4564,8 +4659,8 @@ asmlinkage long sys_sched_yield(void)\n{\nstruct rq *rq = this_rq_lock();\n\n-\tschedstat_inc(rq, yld_cnt);\n-\tcurrent->sched_class->yield_task(rq, current);\n+\tschedstat_inc(rq, yld_count);\n+\tcurrent->sched_class->yield_task(rq);\n\n/*\n* Since we are going to call schedule() anyway, there\'s\n@@ -4613,7 +4708,7 @@ EXPORT_SYMBOL(cond_resched);\n* cond_resched_lock() - if a reschedule is pending, drop the given lock,\n* call schedule, and on return reacquire the lock.\n*\n- * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level\n+ * This works OK both with and without CONFIG_PREEMPT. We do strange low-level\n* operations here to prevent schedule() from being called twice (once via\n* spin_unlock(), once by hand).\n*/\n@@ -4667,7 +4762,7 @@ void __sched yield(void)\nEXPORT_SYMBOL(yield);\n\n/*\n- * This task is about to go to sleep on IO.  Increment rq->nr_iowait so\n+ * This task is about to go to sleep on IO. Increment rq->nr_iowait so\n* that process accounting knows that this is a task in IO wait state.\n*\n* But don\'t do that if it is a deliberate, throttling IO wait (this task\n@@ -4759,11 +4854,12 @@ asmlinkage\nlong sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)\n{\nstruct task_struct *p;\n-\tint retval = -EINVAL;\n+\tunsigned int time_slice;\n+\tint retval;\nstruct timespec t;\n\nif (pid < 0)\n-\t\tgoto out_nounlock;\n+\t\treturn -EINVAL;\n\nretval = -ESRCH;\nread_lock(&tasklist_lock);\n@@ -4775,12 +4871,28 @@ long sys_sched_rr_get_interval(pid_t pid\nif (retval)\ngoto out_unlock;\n\n-\tjiffies_to_timespec(p->policy == SCHED_FIFO ?\n-\t\t\t\t0 : static_prio_timeslice(p->static_prio), &t);\n+\t/*\n+\t * Time slice is 0 for SCHED_FIFO tasks and for SCHED_OTHER\n+\t * tasks that are on an otherwise idle runqueue:\n+\t */\n+\ttime_slice = 0;\n+\tif (p->policy == SCHED_RR) {\n+\t\ttime_slice = DEF_TIMESLICE;\n+\t} else {\n+\t\tstruct sched_entity *se = &p->se;\n+\t\tunsigned long flags;\n+\t\tstruct rq *rq;\n+\n+\t\trq = task_rq_lock(p, &flags);\n+\t\tif (rq->cfs.load.weight)\n+\t\t\ttime_slice = NS_TO_JIFFIES(sched_slice(&rq->cfs, se));\n+\t\ttask_rq_unlock(rq, &flags);\n+\t}\nread_unlock(&tasklist_lock);\n+\tjiffies_to_timespec(time_slice, &t);\nretval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;\n-out_nounlock:\nreturn retval;\n+\nout_unlock:\nread_unlock(&tasklist_lock);\nreturn retval;\n@@ -4794,18 +4906,18 @@ static void show_task(struct task_struct\nunsigned state;\n\nstate = p->state ? __ffs(p->state) + 1 : 0;\n-\tprintk("%-13.13s %c", p->comm,\n+\tprintk(KERN_INFO "%-13.13s %c", p->comm,\nstate < sizeof(stat_nam) - 1 ? stat_nam[state] : \'?\');\n#if BITS_PER_LONG == 32\nif (state == TASK_RUNNING)\n-\t\tprintk(" running  ");\n+\t\tprintk(KERN_CONT " running  ");\nelse\n-\t\tprintk(" %08lx ", thread_saved_pc(p));\n+\t\tprintk(KERN_CONT " %08lx ", thread_saved_pc(p));\n#else\nif (state == TASK_RUNNING)\n-\t\tprintk("  running task    ");\n+\t\tprintk(KERN_CONT "  running task    ");\nelse\n-\t\tprintk(" %016lx ", thread_saved_pc(p));\n+\t\tprintk(KERN_CONT " %016lx ", thread_saved_pc(p));\n#endif\n#ifdef CONFIG_DEBUG_STACK_USAGE\n{\n@@ -4815,7 +4927,8 @@ static void show_task(struct task_struct\nfree = (unsigned long)n - (unsigned long)end_of_stack(p);\n}\n#endif\n-\tprintk("%5lu %5d %6d\\n", free, p->pid, p->parent->pid);\n+\tprintk(KERN_CONT "%5lu %5d %6d\\n", free,\n+\t\ttask_pid_nr(p), task_pid_nr(p->parent));\n\nif (state != TASK_RUNNING)\nshow_stack(p, NULL);\n@@ -4921,7 +5034,7 @@ cpumask_t nohz_cpu_mask = CPU_MASK_NONE;\nstatic inline void sched_init_granularity(void)\n{\nunsigned int factor = 1 + ilog2(num_online_cpus());\n-\tconst unsigned long limit = 100000000;\n+\tconst unsigned long limit = 200000000;\n\nsysctl_sched_min_granularity *= factor;\nif (sysctl_sched_min_granularity > limit)\n@@ -4931,8 +5044,8 @@ static inline void sched_init_granularit\nif (sysctl_sched_latency > limit)\nsysctl_sched_latency = limit;\n\n-\tsysctl_sched_runtime_limit = sysctl_sched_latency;\n-\tsysctl_sched_wakeup_granularity = sysctl_sched_min_granularity / 2;\n+\tsysctl_sched_wakeup_granularity *= factor;\n+\tsysctl_sched_batch_wakeup_granularity *= factor;\n}\n\n#ifdef CONFIG_SMP\n@@ -4958,7 +5071,7 @@ static inline void sched_init_granularit\n* is removed from the allowed bitmask.\n*\n* NOTE: the caller must have a valid reference to the task, the\n- * task must not exit() & deallocate itself prematurely.  The\n+ * task must not exit() & deallocate itself prematurely. The\n* call is not atomic; no spinlocks may be held.\n*/\nint set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)\n@@ -4995,7 +5108,7 @@ out:\nEXPORT_SYMBOL_GPL(set_cpus_allowed);\n\n/*\n- * Move (not current) task off this cpu, onto dest cpu.  We\'re doing\n+ * Move (not current) task off this cpu, onto dest cpu. We\'re doing\n* this because either it can\'t run here any more (set_cpus_allowed()\n* away from this CPU, or CPU going down), or because we\'re\n* attempting to rebalance this task on exec (sched_exec).\n@@ -5057,6 +5170,8 @@ static int migration_thread(void *data)\nstruct migration_req *req;\nstruct list_head *head;\n\n+\t\ttry_to_freeze();\n+\nspin_lock_irq(&rq->lock);\n\nif (cpu_is_offline(cpu)) {\n@@ -5101,8 +5216,19 @@ wait_to_die:\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n+\n+static int __migrate_task_irq(struct task_struct *p, int src_cpu, int dest_cpu)\n+{\n+\tint ret;\n+\n+\tlocal_irq_disable();\n+\tret = __migrate_task(p, src_cpu, dest_cpu);\n+\tlocal_irq_enable();\n+\treturn ret;\n+}\n+\n/*\n- * Figure out where task on dead CPU should go, use force if neccessary.\n+ * Figure out where task on dead CPU should go, use force if necessary.\n* NOTE: interrupts should be disabled by the caller\n*/\nstatic void move_task_off_dead_cpu(int dead_cpu, struct task_struct *p)\n@@ -5112,35 +5238,43 @@ static void move_task_off_dead_cpu(int d\nstruct rq *rq;\nint dest_cpu;\n\n-restart:\n-\t/* On same node? */\n-\tmask = node_to_cpumask(cpu_to_node(dead_cpu));\n-\tcpus_and(mask, mask, p->cpus_allowed);\n-\tdest_cpu = any_online_cpu(mask);\n-\n-\t/* On any allowed CPU? */\n-\tif (dest_cpu == NR_CPUS)\n-\t\tdest_cpu = any_online_cpu(p->cpus_allowed);\n+\tdo {\n+\t\t/* On same node? */\n+\t\tmask = node_to_cpumask(cpu_to_node(dead_cpu));\n+\t\tcpus_and(mask, mask, p->cpus_allowed);\n+\t\tdest_cpu = any_online_cpu(mask);\n+\n+\t\t/* On any allowed CPU? */\n+\t\tif (dest_cpu == NR_CPUS)\n+\t\t\tdest_cpu = any_online_cpu(p->cpus_allowed);\n+\n+\t\t/* No more Mr. Nice Guy. */\n+\t\tif (dest_cpu == NR_CPUS) {\n+\t\t\tcpumask_t cpus_allowed = cpuset_cpus_allowed_locked(p);\n+\t\t\t/*\n+\t\t\t * Try to stay on the same cpuset, where the\n+\t\t\t * current cpuset may be a subset of all cpus.\n+\t\t\t * The cpuset_cpus_allowed_locked() variant of\n+\t\t\t * cpuset_cpus_allowed() will not block. It must be\n+\t\t\t * called within calls to cpuset_lock/cpuset_unlock.\n+\t\t\t */\n+\t\t\trq = task_rq_lock(p, &flags);\n+\t\t\tp->cpus_allowed = cpus_allowed;\n+\t\t\tdest_cpu = any_online_cpu(p->cpus_allowed);\n+\t\t\ttask_rq_unlock(rq, &flags);\n\n-\t/* No more Mr. Nice Guy. */\n-\tif (dest_cpu == NR_CPUS) {\n-\t\trq = task_rq_lock(p, &flags);\n-\t\tcpus_setall(p->cpus_allowed);\n-\t\tdest_cpu = any_online_cpu(p->cpus_allowed);\n-\t\ttask_rq_unlock(rq, &flags);\n-\n-\t\t/*\n-\t\t * Don\'t tell them about moving exiting tasks or\n-\t\t * kernel threads (both mm NULL), since they never\n-\t\t * leave kernel.\n-\t\t */\n-\t\tif (p->mm && printk_ratelimit())\n-\t\t\tprintk(KERN_INFO "process %d (%s) no "\n-\t\t\t       "longer affine to cpu%d\\n",\n-\t\t\t       p->pid, p->comm, dead_cpu);\n-\t}\n-\tif (!__migrate_task(p, dead_cpu, dest_cpu))\n-\t\tgoto restart;\n+\t\t\t/*\n+\t\t\t * Don\'t tell them about moving exiting tasks or\n+\t\t\t * kernel threads (both mm NULL), since they never\n+\t\t\t * leave kernel.\n+\t\t\t */\n+\t\t\tif (p->mm && printk_ratelimit()) {\n+\t\t\t\tprintk(KERN_INFO "process %d (%s) no "\n+\t\t\t\t       "longer affine to cpu%d\\n",\n+\t\t\t\t\ttask_pid_nr(p), p->comm, dead_cpu);\n+\t\t\t}\n+\t\t}\n+\t} while (!__migrate_task_irq(p, dead_cpu, dest_cpu));\n}\n\n/*\n@@ -5168,7 +5302,7 @@ static void migrate_live_tasks(int src_c\n{\nstruct task_struct *p, *t;\n\n-\twrite_lock_irq(&tasklist_lock);\n+\tread_lock(&tasklist_lock);\n\ndo_each_thread(t, p) {\nif (p == current)\n@@ -5178,13 +5312,13 @@ static void migrate_live_tasks(int src_c\nmove_task_off_dead_cpu(src_cpu, p);\n} while_each_thread(t, p);\n\n-\twrite_unlock_irq(&tasklist_lock);\n+\tread_unlock(&tasklist_lock);\n}\n\n/*\n* Schedules idle task to be the next runnable task on current CPU.\n- * It does so by boosting its priority to highest possible and adding it to\n- * the _front_ of the runqueue. Used by CPU offline code.\n+ * It does so by boosting its priority to highest possible.\n+ * Used by CPU offline code.\n*/\nvoid sched_idle_next(void)\n{\n@@ -5204,8 +5338,8 @@ void sched_idle_next(void)\n\n__setscheduler(rq, p, SCHED_FIFO, MAX_RT_PRIO-1);\n\n-\t/* Add idle task to the _front_ of its priority queue: */\n-\tactivate_idle_task(p, rq);\n+\tupdate_rq_clock(rq);\n+\tactivate_task(rq, p, 0);\n\nspin_unlock_irqrestore(&rq->lock, flags);\n}\n@@ -5231,7 +5365,7 @@ static void migrate_dead(unsigned int de\nstruct rq *rq = cpu_rq(dead_cpu);\n\n/* Must be exiting, otherwise would be on tasklist. */\n-\tBUG_ON(p->exit_state != EXIT_ZOMBIE && p->exit_state != EXIT_DEAD);\n+\tBUG_ON(!p->exit_state);\n\n/* Cannot have done final schedule yet: would have vanished. */\nBUG_ON(p->state == TASK_DEAD);\n@@ -5240,13 +5374,12 @@ static void migrate_dead(unsigned int de\n\n/*\n* Drop lock around migration; if someone else moves it,\n-\t * that\'s OK.  No task can be added to this CPU, so iteration is\n+\t * that\'s OK. No task can be added to this CPU, so iteration is\n* fine.\n-\t * NOTE: interrupts should be left disabled  --dev@\n*/\n-\tspin_unlock(&rq->lock);\n+\tspin_unlock_irq(&rq->lock);\nmove_task_off_dead_cpu(dead_cpu, p);\n-\tspin_lock(&rq->lock);\n+\tspin_lock_irq(&rq->lock);\n\nput_task_struct(p);\n}\n@@ -5277,7 +5410,7 @@ static struct ctl_table sd_ctl_dir[] = {\n.procname\t= "sched_domain",\n.mode\t\t= 0555,\n},\n-\t{0,},\n+\t{0, },\n};\n\nstatic struct ctl_table sd_ctl_root[] = {\n@@ -5287,20 +5420,38 @@ static struct ctl_table sd_ctl_root[] =\n.mode\t\t= 0555,\n.child\t\t= sd_ctl_dir,\n},\n-\t{0,},\n+\t{0, },\n};\n\nstatic struct ctl_table *sd_alloc_ctl_entry(int n)\n{\nstruct ctl_table *entry =\n-\t\tkmalloc(n * sizeof(struct ctl_table), GFP_KERNEL);\n-\n-\tBUG_ON(!entry);\n-\tmemset(entry, 0, n * sizeof(struct ctl_table));\n+\t\tkcalloc(n, sizeof(struct ctl_table), GFP_KERNEL);\n\nreturn entry;\n}\n\n+static void sd_free_ctl_entry(struct ctl_table **tablep)\n+{\n+\tstruct ctl_table *entry;\n+\n+\t/*\n+\t * In the intermediate directories, both the child directory and\n+\t * procname are dynamically allocated and could fail but the mode\n+\t * will always be set. In the lowest directory the names are\n+\t * static strings and all have proc handlers.\n+\t */\n+\tfor (entry = *tablep; entry->mode; entry++) {\n+\t\tif (entry->child)\n+\t\t\tsd_free_ctl_entry(&entry->child);\n+\t\tif (entry->proc_handler == NULL)\n+\t\t\tkfree(entry->procname);\n+\t}\n+\n+\tkfree(*tablep);\n+\t*tablep = NULL;\n+}\n+\nstatic void\nset_table_entry(struct ctl_table *entry,\nconst char *procname, void *data, int maxlen,\n@@ -5318,6 +5469,9 @@ sd_alloc_ctl_domain_table(struct sched_d\n{\nstruct ctl_table *table = sd_alloc_ctl_entry(12);\n\n+\tif (table == NULL)\n+\t\treturn NULL;\n+\nset_table_entry(&table[0], "min_interval", &sd->min_interval,\nsizeof(long), 0644, proc_doulongvec_minmax);\nset_table_entry(&table[1], "max_interval", &sd->max_interval,\n@@ -5341,6 +5495,7 @@ sd_alloc_ctl_domain_table(struct sched_d\nsizeof(int), 0644, proc_dointvec_minmax);\nset_table_entry(&table[10], "flags", &sd->flags,\nsizeof(int), 0644, proc_dointvec_minmax);\n+\t/* &table[11] is terminator */\n\nreturn table;\n}\n@@ -5355,6 +5510,8 @@ static ctl_table *sd_alloc_ctl_cpu_table\nfor_each_domain(cpu, sd)\ndomain_num++;\nentry = table = sd_alloc_ctl_entry(domain_num + 1);\n+\tif (table == NULL)\n+\t\treturn NULL;\n\ni = 0;\nfor_each_domain(cpu, sd) {\n@@ -5369,24 +5526,44 @@ static ctl_table *sd_alloc_ctl_cpu_table\n}\n\nstatic struct ctl_table_header *sd_sysctl_header;\n-static void init_sched_domain_sysctl(void)\n+static void register_sched_domain_sysctl(void)\n{\nint i, cpu_num = num_online_cpus();\nstruct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);\nchar buf[32];\n\n+\tWARN_ON(sd_ctl_dir[0].child);\nsd_ctl_dir[0].child = entry;\n\n-\tfor (i = 0; i < cpu_num; i++, entry++) {\n+\tif (entry == NULL)\n+\t\treturn;\n+\n+\tfor_each_online_cpu(i) {\nsnprintf(buf, 32, "cpu%d", i);\nentry->procname = kstrdup(buf, GFP_KERNEL);\nentry->mode = 0555;\nentry->child = sd_alloc_ctl_cpu_table(i);\n+\t\tentry++;\n}\n+\n+\tWARN_ON(sd_sysctl_header);\nsd_sysctl_header = register_sysctl_table(sd_ctl_root);\n}\n+\n+/* may be called multiple times per register */\n+static void unregister_sched_domain_sysctl(void)\n+{\n+\tif (sd_sysctl_header)\n+\t\tunregister_sysctl_table(sd_sysctl_header);\n+\tsd_sysctl_header = NULL;\n+\tif (sd_ctl_dir[0].child)\n+\t\tsd_free_ctl_entry(&sd_ctl_dir[0].child);\n+}\n#else\n-static void init_sched_domain_sysctl(void)\n+static void register_sched_domain_sysctl(void)\n+{\n+}\n+static void unregister_sched_domain_sysctl(void)\n{\n}\n#endif\n@@ -5413,6 +5590,7 @@ migration_call(struct notifier_block *nf\np = kthread_create(migration_thread, hcpu, "migration/%d", cpu);\nif (IS_ERR(p))\nreturn NOTIFY_BAD;\n+\t\tp->flags |= PF_NOFREEZE;\nkthread_bind(p, cpu);\n/* Must be high prio: stop_machine expects to yield to it. */\nrq = task_rq_lock(p, &flags);\n@@ -5423,7 +5601,7 @@ migration_call(struct notifier_block *nf\n\ncase CPU_ONLINE:\ncase CPU_ONLINE_FROZEN:\n-\t\t/* Strictly unneccessary, as first user will wake it. */\n+\t\t/* Strictly unnecessary, as first user will wake it. */\nwake_up_process(cpu_rq(cpu)->migration_thread);\nbreak;\n\n@@ -5432,7 +5610,7 @@ migration_call(struct notifier_block *nf\ncase CPU_UP_CANCELED_FROZEN:\nif (!cpu_rq(cpu)->migration_thread)\nbreak;\n-\t\t/* Unbind it from offline cpu so it can run.  Fall thru. */\n+\t\t/* Unbind it from offline cpu so it can run. Fall thru. */\nkthread_bind(cpu_rq(cpu)->migration_thread,\nany_online_cpu(cpu_online_map));\nkthread_stop(cpu_rq(cpu)->migration_thread);\n@@ -5441,25 +5619,29 @@ migration_call(struct notifier_block *nf\n\ncase CPU_DEAD:\ncase CPU_DEAD_FROZEN:\n+\t\tcpuset_lock(); /* around calls to cpuset_cpus_allowed_lock() */\nmigrate_live_tasks(cpu);\nrq = cpu_rq(cpu);\nkthread_stop(rq->migration_thread);\nrq->migration_thread = NULL;\n/* Idle task back to normal (off runqueue, low prio) */\n-\t\trq = task_rq_lock(rq->idle, &flags);\n+\t\tspin_lock_irq(&rq->lock);\nupdate_rq_clock(rq);\ndeactivate_task(rq, rq->idle, 0);\nrq->idle->static_prio = MAX_PRIO;\n__setscheduler(rq, rq->idle, SCHED_NORMAL, 0);\nrq->idle->sched_class = &idle_sched_class;\nmigrate_dead_tasks(cpu);\n-\t\ttask_rq_unlock(rq, &flags);\n+\t\tspin_unlock_irq(&rq->lock);\n+\t\tcpuset_unlock();\nmigrate_nr_uninterruptible(rq);\nBUG_ON(rq->nr_running != 0);\n\n-\t\t/* No need to migrate the tasks: it was best-effort if\n-\t\t * they didn\'t take sched_hotcpu_mutex.  Just wake up\n-\t\t * the requestors. */\n+\t\t/*\n+\t\t * No need to migrate the tasks: it was best-effort if\n+\t\t * they didn\'t take sched_hotcpu_mutex. Just wake up\n+\t\t * the requestors.\n+\t\t */\nspin_lock_irq(&rq->lock);\nwhile (!list_empty(&rq->migration_queue)) {\nstruct migration_req *req;\n@@ -5487,7 +5669,7 @@ static struct notifier_block __cpuinitda\n.priority = 10\n};\n\n-int __init migration_init(void)\n+void __init migration_init(void)\n{\nvoid *cpu = (void *)(long)smp_processor_id();\nint err;\n@@ -5497,8 +5679,6 @@ int __init migration_init(void)\nBUG_ON(err == NOTIFY_BAD);\nmigration_call(&migration_notifier, CPU_ONLINE, cpu);\nregister_cpu_notifier(&migration_notifier);\n-\n-\treturn 0;\n}\n#endif\n\n@@ -5508,100 +5688,102 @@ int __init migration_init(void)\nint nr_cpu_ids __read_mostly = NR_CPUS;\nEXPORT_SYMBOL(nr_cpu_ids);\n\n-#undef SCHED_DOMAIN_DEBUG\n-#ifdef SCHED_DOMAIN_DEBUG\n-static void sched_domain_debug(struct sched_domain *sd, int cpu)\n-{\n-\tint level = 0;\n+#ifdef CONFIG_SCHED_DEBUG\n\n-\tif (!sd) {\n-\t\tprintk(KERN_DEBUG "CPU%d attaching NULL sched-domain.\\n", cpu);\n-\t\treturn;\n+static int sched_domain_debug_one(struct sched_domain *sd, int cpu, int level)\n+{\n+\tstruct sched_group *group = sd->groups;\n+\tcpumask_t groupmask;\n+\tchar str[NR_CPUS];\n+\n+\tcpumask_scnprintf(str, NR_CPUS, sd->span);\n+\tcpus_clear(groupmask);\n+\n+\tprintk(KERN_DEBUG "%*s domain %d: ", level, "", level);\n+\n+\tif (!(sd->flags & SD_LOAD_BALANCE)) {\n+\t\tprintk("does not load-balance\\n");\n+\t\tif (sd->parent)\n+\t\t\tprintk(KERN_ERR "ERROR: !SD_LOAD_BALANCE domain"\n+\t\t\t\t\t" has parent");\n+\t\treturn -1;\n}\n\n-\tprintk(KERN_DEBUG "CPU%d attaching sched-domain:\\n", cpu);\n+\tprintk(KERN_CONT "span %s\\n", str);\n\n+\tif (!cpu_isset(cpu, sd->span)) {\n+\t\tprintk(KERN_ERR "ERROR: domain->span does not contain "\n+\t\t\t\t"CPU%d\\n", cpu);\n+\t}\n+\tif (!cpu_isset(cpu, group->cpumask)) {\n+\t\tprintk(KERN_ERR "ERROR: domain->groups does not contain"\n+\t\t\t\t" CPU%d\\n", cpu);\n+\t}\n+\n+\tprintk(KERN_DEBUG "%*s groups:", level + 1, "");\ndo {\n-\t\tint i;\n-\t\tchar str[NR_CPUS];\n-\t\tstruct sched_group *group = sd->groups;\n-\t\tcpumask_t groupmask;\n-\n-\t\tcpumask_scnprintf(str, NR_CPUS, sd->span);\n-\t\tcpus_clear(groupmask);\n-\n-\t\tprintk(KERN_DEBUG);\n-\t\tfor (i = 0; i < level + 1; i++)\n-\t\t\tprintk(" ");\n-\t\tprintk("domain %d: ", level);\n-\n-\t\tif (!(sd->flags & SD_LOAD_BALANCE)) {\n-\t\t\tprintk("does not load-balance\\n");\n-\t\t\tif (sd->parent)\n-\t\t\t\tprintk(KERN_ERR "ERROR: !SD_LOAD_BALANCE domain"\n-\t\t\t\t\t\t" has parent");\n+\t\tif (!group) {\n+\t\t\tprintk("\\n");\n+\t\t\tprintk(KERN_ERR "ERROR: group is NULL\\n");\nbreak;\n}\n\n-\t\tprintk("span %s\\n", str);\n+\t\tif (!group->__cpu_power) {\n+\t\t\tprintk(KERN_CONT "\\n");\n+\t\t\tprintk(KERN_ERR "ERROR: domain->cpu_power not "\n+\t\t\t\t\t"set\\n");\n+\t\t\tbreak;\n+\t\t}\n\n-\t\tif (!cpu_isset(cpu, sd->span))\n-\t\t\tprintk(KERN_ERR "ERROR: domain->span does not contain "\n-\t\t\t\t\t"CPU%d\\n", cpu);\n-\t\tif (!cpu_isset(cpu, group->cpumask))\n-\t\t\tprintk(KERN_ERR "ERROR: domain->groups does not contain"\n-\t\t\t\t\t" CPU%d\\n", cpu);\n-\n-\t\tprintk(KERN_DEBUG);\n-\t\tfor (i = 0; i < level + 2; i++)\n-\t\t\tprintk(" ");\n-\t\tprintk("groups:");\n-\t\tdo {\n-\t\t\tif (!group) {\n-\t\t\t\tprintk("\\n");\n-\t\t\t\tprintk(KERN_ERR "ERROR: group is NULL\\n");\n-\t\t\t\tbreak;\n-\t\t\t}\n+\t\tif (!cpus_weight(group->cpumask)) {\n+\t\t\tprintk(KERN_CONT "\\n");\n+\t\t\tprintk(KERN_ERR "ERROR: empty group\\n");\n+\t\t\tbreak;\n+\t\t}\n\n-\t\t\tif (!group->__cpu_power) {\n-\t\t\t\tprintk("\\n");\n-\t\t\t\tprintk(KERN_ERR "ERROR: domain->cpu_power not "\n-\t\t\t\t\t\t"set\\n");\n-\t\t\t}\n+\t\tif (cpus_intersects(groupmask, group->cpumask)) {\n+\t\t\tprintk(KERN_CONT "\\n");\n+\t\t\tprintk(KERN_ERR "ERROR: repeated CPUs\\n");\n+\t\t\tbreak;\n+\t\t}\n\n-\t\t\tif (!cpus_weight(group->cpumask)) {\n-\t\t\t\tprintk("\\n");\n-\t\t\t\tprintk(KERN_ERR "ERROR: empty group\\n");\n-\t\t\t}\n+\t\tcpus_or(groupmask, groupmask, group->cpumask);\n\n-\t\t\tif (cpus_intersects(groupmask, group->cpumask)) {\n-\t\t\t\tprintk("\\n");\n-\t\t\t\tprintk(KERN_ERR "ERROR: repeated CPUs\\n");\n-\t\t\t}\n+\t\tcpumask_scnprintf(str, NR_CPUS, group->cpumask);\n+\t\tprintk(KERN_CONT " %s", str);\n+\n+\t\tgroup = group->next;\n+\t} while (group != sd->groups);\n+\tprintk(KERN_CONT "\\n");\n+\n+\tif (!cpus_equal(sd->span, groupmask))\n+\t\tprintk(KERN_ERR "ERROR: groups don\'t span domain->span\\n");\n+\n+\tif (sd->parent && !cpus_subset(groupmask, sd->parent->span))\n+\t\tprintk(KERN_ERR "ERROR: parent span is not a superset "\n+\t\t\t"of domain->span\\n");\n+\treturn 0;\n+}\n\n-\t\t\tcpus_or(groupmask, groupmask, group->cpumask);\n+static void sched_domain_debug(struct sched_domain *sd, int cpu)\n+{\n+\tint level = 0;\n\n-\t\t\tcpumask_scnprintf(str, NR_CPUS, group->cpumask);\n-\t\t\tprintk(" %s", str);\n+\tif (!sd) {\n+\t\tprintk(KERN_DEBUG "CPU%d attaching NULL sched-domain.\\n", cpu);\n+\t\treturn;\n+\t}\n\n-\t\t\tgroup = group->next;\n-\t\t} while (group != sd->groups);\n-\t\tprintk("\\n");\n-\n-\t\tif (!cpus_equal(sd->span, groupmask))\n-\t\t\tprintk(KERN_ERR "ERROR: groups don\'t span "\n-\t\t\t\t\t"domain->span\\n");\n+\tprintk(KERN_DEBUG "CPU%d attaching sched-domain:\\n", cpu);\n\n+\tfor (;;) {\n+\t\tif (sched_domain_debug_one(sd, cpu, level))\n+\t\t\tbreak;\nlevel++;\nsd = sd->parent;\nif (!sd)\n-\t\t\tcontinue;\n-\n-\t\tif (!cpus_subset(groupmask, sd->span))\n-\t\t\tprintk(KERN_ERR "ERROR: parent span is not a superset "\n-\t\t\t\t"of domain->span\\n");\n-\n-\t} while (sd);\n+\t\t\tbreak;\n+\t}\n}\n#else\n# define sched_domain_debug(sd, cpu) do { } while (0)\n@@ -5710,7 +5892,7 @@ static int __init isolated_cpu_setup(cha\nreturn 1;\n}\n\n-__setup ("isolcpus=", isolated_cpu_setup);\n+__setup("isolcpus=", isolated_cpu_setup);\n\n/*\n* init_sched_build_groups takes the cpumask we wish to span, and a pointer\n@@ -5767,7 +5949,7 @@ init_sched_build_groups(cpumask_t span,\n* @node: node whose sched_domain we\'re building\n* @used_nodes: nodes already in the sched_domain\n*\n- * Find the next node to include in a given scheduling domain.  Simply\n+ * Find the next node to include in a given scheduling domain. Simply\n* finds the closest node not already in the @used_nodes map.\n*\n* Should use nodemask_t.\n@@ -5807,7 +5989,7 @@ static int find_next_best_node(int node,\n* @node: node whose cpumask we\'re constructing\n* @size: number of nodes to include in this span\n*\n- * Given a node, construct a good cpumask for its sched_domain to span.  It\n+ * Given a node, construct a good cpumask for its sched_domain to span. It\n* should be one that prevents unnecessary balancing, but also spreads tasks\n* out optimally.\n*/\n@@ -5844,8 +6026,8 @@ int sched_smt_power_savings = 0, sched_m\nstatic DEFINE_PER_CPU(struct sched_domain, cpu_domains);\nstatic DEFINE_PER_CPU(struct sched_group, sched_group_cpus);\n\n-static int cpu_to_cpu_group(int cpu, const cpumask_t *cpu_map,\n-\t\t\t    struct sched_group **sg)\n+static int\n+cpu_to_cpu_group(int cpu, const cpumask_t *cpu_map, struct sched_group **sg)\n{\nif (sg)\n*sg = &per_cpu(sched_group_cpus, cpu);\n@@ -5862,11 +6044,11 @@ static DEFINE_PER_CPU(struct sched_group\n#endif\n\n#if defined(CONFIG_SCHED_MC) && defined(CONFIG_SCHED_SMT)\n-static int cpu_to_core_group(int cpu, const cpumask_t *cpu_map,\n-\t\t\t     struct sched_group **sg)\n+static int\n+cpu_to_core_group(int cpu, const cpumask_t *cpu_map, struct sched_group **sg)\n{\nint group;\n-\tcpumask_t mask = cpu_sibling_map[cpu];\n+\tcpumask_t mask = cpu_sibling_map(cpu);\ncpus_and(mask, mask, *cpu_map);\ngroup = first_cpu(mask);\nif (sg)\n@@ -5874,8 +6056,8 @@ static int cpu_to_core_group(int cpu, co\nreturn group;\n}\n#elif defined(CONFIG_SCHED_MC)\n-static int cpu_to_core_group(int cpu, const cpumask_t *cpu_map,\n-\t\t\t     struct sched_group **sg)\n+static int\n+cpu_to_core_group(int cpu, const cpumask_t *cpu_map, struct sched_group **sg)\n{\nif (sg)\n*sg = &per_cpu(sched_group_core, cpu);\n@@ -5886,8 +6068,8 @@ static int cpu_to_core_group(int cpu, co\nstatic DEFINE_PER_CPU(struct sched_domain, phys_domains);\nstatic DEFINE_PER_CPU(struct sched_group, sched_group_phys);\n\n-static int cpu_to_phys_group(int cpu, const cpumask_t *cpu_map,\n-\t\t\t     struct sched_group **sg)\n+static int\n+cpu_to_phys_group(int cpu, const cpumask_t *cpu_map, struct sched_group **sg)\n{\nint group;\n#ifdef CONFIG_SCHED_MC\n@@ -5895,7 +6077,7 @@ static int cpu_to_phys_group(int cpu, co\ncpus_and(mask, mask, *cpu_map);\ngroup = first_cpu(mask);\n#elif defined(CONFIG_SCHED_SMT)\n-\tcpumask_t mask = cpu_sibling_map[cpu];\n+\tcpumask_t mask = cpu_sibling_map(cpu);\ncpus_and(mask, mask, *cpu_map);\ngroup = first_cpu(mask);\n#else\n@@ -5939,24 +6121,23 @@ static void init_numa_sched_groups_power\n\nif (!sg)\nreturn;\n-next_sg:\n-\tfor_each_cpu_mask(j, sg->cpumask) {\n-\t\tstruct sched_domain *sd;\n+\tdo {\n+\t\tfor_each_cpu_mask(j, sg->cpumask) {\n+\t\t\tstruct sched_domain *sd;\n\n-\t\tsd = &per_cpu(phys_domains, j);\n-\t\tif (j != first_cpu(sd->groups->cpumask)) {\n-\t\t\t/*\n-\t\t\t * Only add "power" once for each\n-\t\t\t * physical package.\n-\t\t\t */\n-\t\t\tcontinue;\n-\t\t}\n+\t\t\tsd = &per_cpu(phys_domains, j);\n+\t\t\tif (j != first_cpu(sd->groups->cpumask)) {\n+\t\t\t\t/*\n+\t\t\t\t * Only add "power" once for each\n+\t\t\t\t * physical package.\n+\t\t\t\t */\n+\t\t\t\tcontinue;\n+\t\t\t}\n\n-\t\tsg_inc_cpu_power(sg, sd->groups->__cpu_power);\n-\t}\n-\tsg = sg->next;\n-\tif (sg != group_head)\n-\t\tgoto next_sg;\n+\t\t\tsg_inc_cpu_power(sg, sd->groups->__cpu_power);\n+\t\t}\n+\t\tsg = sg->next;\n+\t} while (sg != group_head);\n}\n#endif\n\n@@ -6067,8 +6248,8 @@ static int build_sched_domains(const cpu\n/*\n* Allocate the per-node list of sched groups\n*/\n-\tsched_group_nodes = kzalloc(sizeof(struct sched_group *)*MAX_NUMNODES,\n-\t\t\t\t\t   GFP_KERNEL);\n+\tsched_group_nodes = kcalloc(MAX_NUMNODES, sizeof(struct sched_group *),\n+\t\t\t\t    GFP_KERNEL);\nif (!sched_group_nodes) {\nprintk(KERN_WARNING "Can not alloc sched group node list\\n");\nreturn -ENOMEM;\n@@ -6130,7 +6311,7 @@ static int build_sched_domains(const cpu\np = sd;\nsd = &per_cpu(cpu_domains, i);\n*sd = SD_SIBLING_INIT;\n-\t\tsd->span = cpu_sibling_map[i];\n+\t\tsd->span = cpu_sibling_map(i);\ncpus_and(sd->span, sd->span, *cpu_map);\nsd->parent = p;\np->child = sd;\n@@ -6141,7 +6322,7 @@ static int build_sched_domains(const cpu\n#ifdef CONFIG_SCHED_SMT\n/* Set up CPU (sibling) groups */\nfor_each_cpu_mask(i, *cpu_map) {\n-\t\tcpumask_t this_sibling_map = cpu_sibling_map[i];\n+\t\tcpumask_t this_sibling_map = cpu_sibling_map(i);\ncpus_and(this_sibling_map, this_sibling_map, *cpu_map);\nif (i != first_cpu(this_sibling_map))\ncontinue;\n@@ -6303,22 +6484,33 @@ error:\nreturn -ENOMEM;\n#endif\n}\n+\n+static cpumask_t *doms_cur;\t/* current sched domains */\n+static int ndoms_cur;\t\t/* number of sched domains in \'doms_cur\' */\n+\n+/*\n+ * Special case: If a kmalloc of a doms_cur partition (array of\n+ * cpumask_t) fails, then fallback to a single sched domain,\n+ * as determined by the single cpumask_t fallback_doms.\n+ */\n+static cpumask_t fallback_doms;\n+\n/*\n- * Set up scheduler domains and groups.  Callers must hold the hotplug lock.\n+ * Set up scheduler domains and groups. Callers must hold the hotplug lock.\n+ * For now this just excludes isolated cpus, but could be used to\n+ * exclude other special cases in the future.\n*/\nstatic int arch_init_sched_domains(const cpumask_t *cpu_map)\n{\n-\tcpumask_t cpu_default_map;\nint err;\n\n-\t/*\n-\t * Setup mask for cpus without special case scheduling requirements.\n-\t * For now this just excludes isolated cpus, but could be used to\n-\t * exclude other special cases in the future.\n-\t */\n-\tcpus_andnot(cpu_default_map, *cpu_map, cpu_isolated_map);\n-\n-\terr = build_sched_domains(&cpu_default_map);\n+\tndoms_cur = 1;\n+\tdoms_cur = kmalloc(sizeof(cpumask_t), GFP_KERNEL);\n+\tif (!doms_cur)\n+\t\tdoms_cur = &fallback_doms;\n+\tcpus_andnot(*doms_cur, *cpu_map, cpu_isolated_map);\n+\terr = build_sched_domains(doms_cur);\n+\tregister_sched_domain_sysctl();\n\nreturn err;\n}\n@@ -6336,6 +6528,8 @@ static void detach_destroy_domains(const\n{\nint i;\n\n+\tunregister_sched_domain_sysctl();\n+\nfor_each_cpu_mask(i, *cpu_map)\ncpu_attach_domain(NULL, i);\nsynchronize_sched();\n@@ -6343,30 +6537,70 @@ static void detach_destroy_domains(const\n}\n\n/*\n- * Partition sched domains as specified by the cpumasks below.\n- * This attaches all cpus from the cpumasks to the NULL domain,\n- * waits for a RCU quiescent period, recalculates sched\n- * domain information and then attaches them back to the\n- * correct sched domains\n+ * Partition sched domains as specified by the \'ndoms_new\'\n+ * cpumasks in the array doms_new[] of cpumasks. This compares\n+ * doms_new[] to the current sched domain partitioning, doms_cur[].\n+ * It destroys each deleted domain and builds each new domain.\n+ *\n+ * \'doms_new\' is an array of cpumask_t\'s of length \'ndoms_new\'.\n+ * The masks don\'t intersect (don\'t overlap.) We should setup one\n+ * sched domain for each mask. CPUs not in any of the cpumasks will\n+ * not be load balanced. If the same cpumask appears both in the\n+ * current \'doms_cur\' domains and in the new \'doms_new\', we can leave\n+ * it as it is.\n+ *\n+ * The passed in \'doms_new\' should be kmalloc\'d. This routine takes\n+ * ownership of it and will kfree it when done with it. If the caller\n+ * failed the kmalloc call, then it can pass in doms_new == NULL,\n+ * and partition_sched_domains() will fallback to the single partition\n+ * \'fallback_doms\'.\n+ *\n* Call with hotplug lock held\n*/\n-int partition_sched_domains(cpumask_t *partition1, cpumask_t *partition2)\n+void partition_sched_domains(int ndoms_new, cpumask_t *doms_new)\n{\n-\tcpumask_t change_map;\n-\tint err = 0;\n+\tint i, j;\n\n-\tcpus_and(*partition1, *partition1, cpu_online_map);\n-\tcpus_and(*partition2, *partition2, cpu_online_map);\n-\tcpus_or(change_map, *partition1, *partition2);\n-\n-\t/* Detach sched domains from all of the affected cpus */\n-\tdetach_destroy_domains(&change_map);\n-\tif (!cpus_empty(*partition1))\n-\t\terr = build_sched_domains(partition1);\n-\tif (!err && !cpus_empty(*partition2))\n-\t\terr = build_sched_domains(partition2);\n+\t/* always unregister in case we don\'t destroy any domains */\n+\tunregister_sched_domain_sysctl();\n\n-\treturn err;\n+\tif (doms_new == NULL) {\n+\t\tndoms_new = 1;\n+\t\tdoms_new = &fallback_doms;\n+\t\tcpus_andnot(doms_new[0], cpu_online_map, cpu_isolated_map);\n+\t}\n+\n+\t/* Destroy deleted domains */\n+\tfor (i = 0; i < ndoms_cur; i++) {\n+\t\tfor (j = 0; j < ndoms_new; j++) {\n+\t\t\tif (cpus_equal(doms_cur[i], doms_new[j]))\n+\t\t\t\tgoto match1;\n+\t\t}\n+\t\t/* no match - a current sched domain not in new doms_new[] */\n+\t\tdetach_destroy_domains(doms_cur + i);\n+match1:\n+\t\t;\n+\t}\n+\n+\t/* Build new domains */\n+\tfor (i = 0; i < ndoms_new; i++) {\n+\t\tfor (j = 0; j < ndoms_cur; j++) {\n+\t\t\tif (cpus_equal(doms_new[i], doms_cur[j]))\n+\t\t\t\tgoto match2;\n+\t\t}\n+\t\t/* no match - add a new doms_new */\n+\t\tbuild_sched_domains(doms_new + i);\n+match2:\n+\t\t;\n+\t}\n+\n+\t/* Remember the new sched domains */\n+\tif (doms_cur != &fallback_doms)\n+\t\tkfree(doms_cur);\n+\tdoms_cur = doms_new;\n+\tndoms_cur = ndoms_new;\n+\n+\tregister_sched_domain_sysctl();\n}\n\n#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)\n@@ -6446,7 +6680,7 @@ int sched_create_sysfs_power_savings_ent\n#endif\n\n/*\n- * Force a reinitialization of the sched domains hierarchy.  The domains\n+ * Force a reinitialization of the sched domains hierarchy. The domains\n* and groups cannot be updated in place without racing with the balancing\n* code, so we temporarily attach all running cpus to the NULL domain\n* which will prevent rebalancing while the sched domains are recalculated.\n@@ -6497,8 +6731,6 @@ void __init sched_init_smp(void)\n/* XXX: Theoretical race here - CPU may be hotplugged now */\nhotcpu_notifier(update_sched_domains, 0);\n\n-\tinit_sched_domain_sysctl();\n-\n/* Move init over to a non-isolated CPU */\nif (set_cpus_allowed(current, non_isolated_cpus) < 0)\nBUG();\n@@ -6513,36 +6745,25 @@ void __init sched_init_smp(void)\n\nint in_sched_functions(unsigned long addr)\n{\n-\t/* Linker adds these: start and end of __sched functions */\n-\textern char __sched_text_start[], __sched_text_end[];\n-\nreturn in_lock_functions(addr) ||\n(addr >= (unsigned long)__sched_text_start\n&& addr < (unsigned long)__sched_text_end);\n}\n\n-static inline void init_cfs_rq(struct cfs_rq *cfs_rq, struct rq *rq)\n+static void init_cfs_rq(struct cfs_rq *cfs_rq, struct rq *rq)\n{\ncfs_rq->tasks_timeline = RB_ROOT;\n-\tcfs_rq->fair_clock = 1;\n#ifdef CONFIG_FAIR_GROUP_SCHED\ncfs_rq->rq = rq;\n#endif\n+\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n}\n\nvoid __init sched_init(void)\n{\n-\tu64 now = sched_clock();\nint highest_cpu = 0;\nint i, j;\n\n-\t/*\n-\t * Link up the scheduling class hierarchy:\n-\t */\n-\trt_sched_class.next = &fair_sched_class;\n-\tfair_sched_class.next = &idle_sched_class;\n-\tidle_sched_class.next = NULL;\n-\nfor_each_possible_cpu(i) {\nstruct rt_prio_array *array;\nstruct rq *rq;\n@@ -6555,10 +6776,28 @@ void __init sched_init(void)\ninit_cfs_rq(&rq->cfs, rq);\n#ifdef CONFIG_FAIR_GROUP_SCHED\nINIT_LIST_HEAD(&rq->leaf_cfs_rq_list);\n-\t\tlist_add(&rq->cfs.leaf_cfs_rq_list, &rq->leaf_cfs_rq_list);\n+\t\t{\n+\t\t\tstruct cfs_rq *cfs_rq = &per_cpu(init_cfs_rq, i);\n+\t\t\tstruct sched_entity *se =\n+\t\t\t\t\t &per_cpu(init_sched_entity, i);\n+\n+\t\t\tinit_cfs_rq_p[i] = cfs_rq;\n+\t\t\tinit_cfs_rq(cfs_rq, rq);\n+\t\t\tcfs_rq->tg = &init_task_group;\n+\t\t\tlist_add(&cfs_rq->leaf_cfs_rq_list,\n+\t\t\t\t\t\t\t &rq->leaf_cfs_rq_list);\n+\n+\t\t\tinit_sched_entity_p[i] = se;\n+\t\t\tse->cfs_rq = &rq->cfs;\n+\t\t\tse->my_q = cfs_rq;\n+\t\t\tse->load.weight = init_task_group_load;\n+\t\t\tse->load.inv_weight =\n+\t\t\t\t div64_64(1ULL<<32, init_task_group_load);\n+\t\t\tse->parent = NULL;\n+\t\t}\n+\t\tinit_task_group.shares = init_task_group_load;\n+\t\tspin_lock_init(&init_task_group.lock);\n#endif\n-\t\trq->ls.load_update_last = now;\n-\t\trq->ls.load_update_start = now;\n\nfor (j = 0; j < CPU_LOAD_IDX_MAX; j++)\nrq->cpu_load[j] = 0;\n@@ -6646,26 +6885,40 @@ EXPORT_SYMBOL(__might_sleep);\n#endif\n\n#ifdef CONFIG_MAGIC_SYSRQ\n+static void normalize_task(struct rq *rq, struct task_struct *p)\n+{\n+\tint on_rq;\n+\tupdate_rq_clock(rq);\n+\ton_rq = p->se.on_rq;\n+\tif (on_rq)\n+\t\tdeactivate_task(rq, p, 0);\n+\t__setscheduler(rq, p, SCHED_NORMAL, 0);\n+\tif (on_rq) {\n+\t\tactivate_task(rq, p, 0);\n+\t\tresched_task(rq->curr);\n+\t}\n+}\n+\nvoid normalize_rt_tasks(void)\n{\nstruct task_struct *g, *p;\nunsigned long flags;\nstruct rq *rq;\n-\tint on_rq;\n\nread_lock_irq(&tasklist_lock);\ndo_each_thread(g, p) {\n-\t\tp->se.fair_key\t\t\t= 0;\n-\t\tp->se.wait_runtime\t\t= 0;\n+\t\t/*\n+\t\t * Only normalize user tasks:\n+\t\t */\n+\t\tif (!p->mm)\n+\t\t\tcontinue;\n+\np->se.exec_start\t\t= 0;\n-\t\tp->se.wait_start_fair\t\t= 0;\n-\t\tp->se.sleep_start_fair\t\t= 0;\n#ifdef CONFIG_SCHEDSTATS\np->se.wait_start\t\t= 0;\np->se.sleep_start\t\t= 0;\np->se.block_start\t\t= 0;\n#endif\n-\t\ttask_rq(p)->cfs.fair_clock\t= 0;\ntask_rq(p)->clock\t\t= 0;\n\nif (!rt_task(p)) {\n@@ -6680,26 +6933,9 @@ void normalize_rt_tasks(void)\n\nspin_lock_irqsave(&p->pi_lock, flags);\nrq = __task_rq_lock(p);\n-#ifdef CONFIG_SMP\n-\t\t/*\n-\t\t * Do not touch the migration thread:\n-\t\t */\n-\t\tif (p == rq->migration_thread)\n-\t\t\tgoto out_unlock;\n-#endif\n\n-\t\tupdate_rq_clock(rq);\n-\t\ton_rq = p->se.on_rq;\n-\t\tif (on_rq)\n-\t\t\tdeactivate_task(rq, p, 0);\n-\t\t__setscheduler(rq, p, SCHED_NORMAL, 0);\n-\t\tif (on_rq) {\n-\t\t\tactivate_task(rq, p, 0);\n-\t\t\tresched_task(rq->curr);\n-\t\t}\n-#ifdef CONFIG_SMP\n- out_unlock:\n-#endif\n+\t\tnormalize_task(rq, p);\n+\n__task_rq_unlock(rq);\nspin_unlock_irqrestore(&p->pi_lock, flags);\n} while_each_thread(g, p);\n@@ -6737,8 +6973,8 @@ struct task_struct *curr_task(int cpu)\n* @p: the task pointer to set.\n*\n* Description: This function must only be used when non-maskable interrupts\n- * are serviced on a separate stack.  It allows the architecture to switch the\n- * notion of the current task on a cpu in a non-blocking manner.  This function\n+ * are serviced on a separate stack. It allows the architecture to switch the\n+ * notion of the current task on a cpu in a non-blocking manner. This function\n* must be called with all CPU\'s synchronized, and interrupts disabled, the\n* and caller must save the original value of the current task (see\n* curr_task() above) and restore that value before reenabling interrupts and\n@@ -6794,3 +7030,425 @@ void set_kernel_trace_flag_all_tasks(voi\nread_unlock(&tasklist_lock);\n}\nEXPORT_SYMBOL_GPL(set_kernel_trace_flag_all_tasks);\n+\n+#ifdef CONFIG_FAIR_GROUP_SCHED\n+\n+/* allocate runqueue etc for a new task group */\n+struct task_group *sched_create_group(void)\n+{\n+\tstruct task_group *tg;\n+\tstruct cfs_rq *cfs_rq;\n+\tstruct sched_entity *se;\n+\tstruct rq *rq;\n+\tint i;\n+\n+\ttg = kzalloc(sizeof(*tg), GFP_KERNEL);\n+\tif (!tg)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\ttg->cfs_rq = kzalloc(sizeof(cfs_rq) * NR_CPUS, GFP_KERNEL);\n+\tif (!tg->cfs_rq)\n+\t\tgoto err;\n+\ttg->se = kzalloc(sizeof(se) * NR_CPUS, GFP_KERNEL);\n+\tif (!tg->se)\n+\t\tgoto err;\n+\n+\tfor_each_possible_cpu(i) {\n+\t\trq = cpu_rq(i);\n+\n+\t\tcfs_rq = kmalloc_node(sizeof(struct cfs_rq), GFP_KERNEL,\n+\t\t\t\t\t\t\t cpu_to_node(i));\n+\t\tif (!cfs_rq)\n+\t\t\tgoto err;\n+\n+\t\tse = kmalloc_node(sizeof(struct sched_entity), GFP_KERNEL,\n+\t\t\t\t\t\t\tcpu_to_node(i));\n+\t\tif (!se)\n+\t\t\tgoto err;\n+\n+\t\tmemset(cfs_rq, 0, sizeof(struct cfs_rq));\n+\t\tmemset(se, 0, sizeof(struct sched_entity));\n+\n+\t\ttg->cfs_rq[i] = cfs_rq;\n+\t\tinit_cfs_rq(cfs_rq, rq);\n+\t\tcfs_rq->tg = tg;\n+\n+\t\ttg->se[i] = se;\n+\t\tse->cfs_rq = &rq->cfs;\n+\t\tse->my_q = cfs_rq;\n+\t\tse->load.weight = NICE_0_LOAD;\n+\t\tse->load.inv_weight = div64_64(1ULL<<32, NICE_0_LOAD);\n+\t\tse->parent = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(i) {\n+\t\trq = cpu_rq(i);\n+\t\tcfs_rq = tg->cfs_rq[i];\n+\t\tlist_add_rcu(&cfs_rq->leaf_cfs_rq_list, &rq->leaf_cfs_rq_list);\n+\t}\n+\n+\ttg->shares = NICE_0_LOAD;\n+\tspin_lock_init(&tg->lock);\n+\n+\treturn tg;\n+\n+err:\n+\tfor_each_possible_cpu(i) {\n+\t\tif (tg->cfs_rq)\n+\t\t\tkfree(tg->cfs_rq[i]);\n+\t\tif (tg->se)\n+\t\t\tkfree(tg->se[i]);\n+\t}\n+\tkfree(tg->cfs_rq);\n+\tkfree(tg->se);\n+\tkfree(tg);\n+\n+\treturn ERR_PTR(-ENOMEM);\n+}\n+\n+/* rcu callback to free various structures associated with a task group */\n+static void free_sched_group(struct rcu_head *rhp)\n+{\n+\tstruct task_group *tg = container_of(rhp, struct task_group, rcu);\n+\tstruct cfs_rq *cfs_rq;\n+\tstruct sched_entity *se;\n+\tint i;\n+\n+\t/* now it should be safe to free those cfs_rqs */\n+\tfor_each_possible_cpu(i) {\n+\t\tcfs_rq = tg->cfs_rq[i];\n+\t\tkfree(cfs_rq);\n+\n+\t\tse = tg->se[i];\n+\t\tkfree(se);\n+\t}\n+\n+\tkfree(tg->cfs_rq);\n+\tkfree(tg->se);\n+\tkfree(tg);\n+}\n+\n+/* Destroy runqueue etc associated with a task group */\n+void sched_destroy_group(struct task_group *tg)\n+{\n+\tstruct cfs_rq *cfs_rq = NULL;\n+\tint i;\n+\n+\tfor_each_possible_cpu(i) {\n+\t\tcfs_rq = tg->cfs_rq[i];\n+\t\tlist_del_rcu(&cfs_rq->leaf_cfs_rq_list);\n+\t}\n+\n+\tBUG_ON(!cfs_rq);\n+\n+\t/* wait for possible concurrent references to cfs_rqs complete */\n+\tcall_rcu(&tg->rcu, free_sched_group);\n+}\n+\n+/* change task\'s runqueue when it moves between groups.\n+ *\tThe caller of this function should have put the task in its new group\n+ *\tby now. This function just updates tsk->se.cfs_rq and tsk->se.parent to\n+ *\treflect its new group.\n+ */\n+void sched_move_task(struct task_struct *tsk)\n+{\n+\tint on_rq, running;\n+\tunsigned long flags;\n+\tstruct rq *rq;\n+\n+\trq = task_rq_lock(tsk, &flags);\n+\n+\tif (tsk->sched_class != &fair_sched_class) {\n+\t\tset_task_cfs_rq(tsk, task_cpu(tsk));\n+\t\tgoto done;\n+\t}\n+\n+\tupdate_rq_clock(rq);\n+\n+\trunning = task_current(rq, tsk);\n+\ton_rq = tsk->se.on_rq;\n+\n+\tif (on_rq) {\n+\t\tdequeue_task(rq, tsk, 0);\n+\t\tif (unlikely(running))\n+\t\t\ttsk->sched_class->put_prev_task(rq, tsk);\n+\t}\n+\n+\tset_task_cfs_rq(tsk, task_cpu(tsk));\n+\n+\tif (on_rq) {\n+\t\tif (unlikely(running))\n+\t\t\ttsk->sched_class->set_curr_task(rq);\n+\t\tenqueue_task(rq, tsk, 0);\n+\t}\n+\n+done:\n+\ttask_rq_unlock(rq, &flags);\n+}\n+\n+static void set_se_shares(struct sched_entity *se, unsigned long shares)\n+{\n+\tstruct cfs_rq *cfs_rq = se->cfs_rq;\n+\tstruct rq *rq = cfs_rq->rq;\n+\tint on_rq;\n+\n+\tspin_lock_irq(&rq->lock);\n+\n+\ton_rq = se->on_rq;\n+\tif (on_rq)\n+\t\tdequeue_entity(cfs_rq, se, 0);\n+\n+\tse->load.weight = shares;\n+\tse->load.inv_weight = div64_64((1ULL<<32), shares);\n+\n+\tif (on_rq)\n+\t\tenqueue_entity(cfs_rq, se, 0);\n+\n+\tspin_unlock_irq(&rq->lock);\n+}\n+\n+int sched_group_set_shares(struct task_group *tg, unsigned long shares)\n+{\n+\tint i;\n+\n+\tspin_lock(&tg->lock);\n+\tif (tg->shares == shares)\n+\t\tgoto done;\n+\n+\ttg->shares = shares;\n+\tfor_each_possible_cpu(i)\n+\t\tset_se_shares(tg->se[i], shares);\n+\n+done:\n+\tspin_unlock(&tg->lock);\n+\treturn 0;\n+}\n+\n+unsigned long sched_group_shares(struct task_group *tg)\n+{\n+\treturn tg->shares;\n+}\n+\n+#endif\t/* CONFIG_FAIR_GROUP_SCHED */\n+\n+#ifdef CONFIG_FAIR_CGROUP_SCHED\n+\n+/* return corresponding task_group object of a cgroup */\n+static inline struct task_group *cgroup_tg(struct cgroup *cgrp)\n+{\n+\treturn container_of(cgroup_subsys_state(cgrp, cpu_cgroup_subsys_id),\n+\t\t\t    struct task_group, css);\n+}\n+\n+static struct cgroup_subsys_state *\n+cpu_cgroup_create(struct cgroup_subsys *ss, struct cgroup *cgrp)\n+{\n+\tstruct task_group *tg;\n+\n+\tif (!cgrp->parent) {\n+\t\t/* This is early initialization for the top cgroup */\n+\t\tinit_task_group.css.cgroup = cgrp;\n+\t\treturn &init_task_group.css;\n+\t}\n+\n+\t/* we support only 1-level deep hierarchical scheduler atm */\n+\tif (cgrp->parent->parent)\n+\t\treturn ERR_PTR(-EINVAL);\n+\n+\ttg = sched_create_group();\n+\tif (IS_ERR(tg))\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\t/* Bind the cgroup to task_group object we just created */\n+\ttg->css.cgroup = cgrp;\n+\n+\treturn &tg->css;\n+}\n+\n+static void\n+cpu_cgroup_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n+{\n+\tstruct task_group *tg = cgroup_tg(cgrp);\n+\n+\tsched_destroy_group(tg);\n+}\n+\n+static int\n+cpu_cgroup_can_attach(struct cgroup_subsys *ss, struct cgroup *cgrp,\n+\t\t      struct task_struct *tsk)\n+{\n+\t/* We don\'t support RT-tasks being in separate groups */\n+\tif (tsk->sched_class != &fair_sched_class)\n+\t\treturn -EINVAL;\n+\n+\treturn 0;\n+}\n+\n+static void\n+cpu_cgroup_attach(struct cgroup_subsys *ss, struct cgroup *cgrp,\n+\t\t\tstruct cgroup *old_cont, struct task_struct *tsk)\n+{\n+\tsched_move_task(tsk);\n+}\n+\n+static int cpu_shares_write_uint(struct cgroup *cgrp, struct cftype *cftype,\n+\t\t\t\tu64 shareval)\n+{\n+\treturn sched_group_set_shares(cgroup_tg(cgrp), shareval);\n+}\n+\n+static u64 cpu_shares_read_uint(struct cgroup *cgrp, struct cftype *cft)\n+{\n+\tstruct task_group *tg = cgroup_tg(cgrp);\n+\n+\treturn (u64) tg->shares;\n+}\n+\n+static struct cftype cpu_files[] = {\n+\t{\n+\t\t.name = "shares",\n+\t\t.read_uint = cpu_shares_read_uint,\n+\t\t.write_uint = cpu_shares_write_uint,\n+\t},\n+};\n+\n+static int cpu_cgroup_populate(struct cgroup_subsys *ss, struct cgroup *cont)\n+{\n+\treturn cgroup_add_files(cont, ss, cpu_files, ARRAY_SIZE(cpu_files));\n+}\n+\n+struct cgroup_subsys cpu_cgroup_subsys = {\n+\t.name\t\t= "cpu",\n+\t.create\t\t= cpu_cgroup_create,\n+\t.destroy\t= cpu_cgroup_destroy,\n+\t.can_attach\t= cpu_cgroup_can_attach,\n+\t.attach\t\t= cpu_cgroup_attach,\n+\t.populate\t= cpu_cgroup_populate,\n+\t.subsys_id\t= cpu_cgroup_subsys_id,\n+\t.early_init\t= 1,\n+};\n+\n+#endif\t/* CONFIG_FAIR_CGROUP_SCHED */\n+\n+#ifdef CONFIG_CGROUP_CPUACCT\n+\n+/*\n+ * CPU accounting code for task groups.\n+ *\n+ * Based on the work by Paul Menage (menage@google.com) and Balbir Singh\n+ * (balbir@in.ibm.com).\n+ */\n+\n+/* track cpu usage of a group of tasks */\n+struct cpuacct {\n+\tstruct cgroup_subsys_state css;\n+\t/* cpuusage holds pointer to a u64-type object on every cpu */\n+\tu64 *cpuusage;\n+};\n+\n+struct cgroup_subsys cpuacct_subsys;\n+\n+/* return cpu accounting group corresponding to this container */\n+static inline struct cpuacct *cgroup_ca(struct cgroup *cont)\n+{\n+\treturn container_of(cgroup_subsys_state(cont, cpuacct_subsys_id),\n+\t\t\t    struct cpuacct, css);\n+}\n+\n+/* return cpu accounting group to which this task belongs */\n+static inline struct cpuacct *task_ca(struct task_struct *tsk)\n+{\n+\treturn container_of(task_subsys_state(tsk, cpuacct_subsys_id),\n+\t\t\t    struct cpuacct, css);\n+}\n+\n+/* create a new cpu accounting group */\n+static struct cgroup_subsys_state *cpuacct_create(\n+\tstruct cgroup_subsys *ss, struct cgroup *cont)\n+{\n+\tstruct cpuacct *ca = kzalloc(sizeof(*ca), GFP_KERNEL);\n+\n+\tif (!ca)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tca->cpuusage = alloc_percpu(u64);\n+\tif (!ca->cpuusage) {\n+\t\tkfree(ca);\n+\t\treturn ERR_PTR(-ENOMEM);\n+\t}\n+\n+\treturn &ca->css;\n+}\n+\n+/* destroy an existing cpu accounting group */\n+static void\n+cpuacct_destroy(struct cgroup_subsys *ss, struct cgroup *cont)\n+{\n+\tstruct cpuacct *ca = cgroup_ca(cont);\n+\n+\tfree_percpu(ca->cpuusage);\n+\tkfree(ca);\n+}\n+\n+/* return total cpu usage (in nanoseconds) of a group */\n+static u64 cpuusage_read(struct cgroup *cont, struct cftype *cft)\n+{\n+\tstruct cpuacct *ca = cgroup_ca(cont);\n+\tu64 totalcpuusage = 0;\n+\tint i;\n+\n+\tfor_each_possible_cpu(i) {\n+\t\tu64 *cpuusage = percpu_ptr(ca->cpuusage, i);\n+\n+\t\t/*\n+\t\t * Take rq->lock to make 64-bit addition safe on 32-bit\n+\t\t * platforms.\n+\t\t */\n+\t\tspin_lock_irq(&cpu_rq(i)->lock);\n+\t\ttotalcpuusage += *cpuusage;\n+\t\tspin_unlock_irq(&cpu_rq(i)->lock);\n+\t}\n+\n+\treturn totalcpuusage;\n+}\n+\n+static struct cftype files[] = {\n+\t{\n+\t\t.name = "usage",\n+\t\t.read_uint = cpuusage_read,\n+\t},\n+};\n+\n+static int cpuacct_populate(struct cgroup_subsys *ss, struct cgroup *cont)\n+{\n+\treturn cgroup_add_files(cont, ss, files, ARRAY_SIZE(files));\n+}\n+\n+/*\n+ * charge this task\'s execution time to its accounting group.\n+ *\n+ * called with rq->lock held.\n+ */\n+static void cpuacct_charge(struct task_struct *tsk, u64 cputime)\n+{\n+\tstruct cpuacct *ca;\n+\n+\tif (!cpuacct_subsys.active)\n+\t\treturn;\n+\n+\tca = task_ca(tsk);\n+\tif (ca) {\n+\t\tu64 *cpuusage = percpu_ptr(ca->cpuusage, task_cpu(tsk));\n+\n+\t\t*cpuusage += cputime;\n+\t}\n+}\n+\n+struct cgroup_subsys cpuacct_subsys = {\n+\t.name = "cpuacct",\n+\t.create = cpuacct_create,\n+\t.destroy = cpuacct_destroy,\n+\t.populate = cpuacct_populate,\n+\t.subsys_id = cpuacct_subsys_id,\n+};\n+#endif\t/* CONFIG_CGROUP_CPUACCT */'),
 ('linux-2.6.23.orig/kernel/sched_debug.c',
  'linux-2.6.23/kernel/sched_debug.c',
  'Index: linux-2.6.23/kernel/sched_debug.c\n===================================================================\n--- linux-2.6.23.orig/kernel/sched_debug.c\n+++ linux-2.6.23/kernel/sched_debug.c\n@@ -28,6 +28,31 @@\nprintk(x);\t\t\t\\\n} while (0)\n\n+/*\n+ * Ease the printing of nsec fields:\n+ */\n+static long long nsec_high(long long nsec)\n+{\n+\tif (nsec < 0) {\n+\t\tnsec = -nsec;\n+\t\tdo_div(nsec, 1000000);\n+\t\treturn -nsec;\n+\t}\n+\tdo_div(nsec, 1000000);\n+\n+\treturn nsec;\n+}\n+\n+static unsigned long nsec_low(long long nsec)\n+{\n+\tif (nsec < 0)\n+\t\tnsec = -nsec;\n+\n+\treturn do_div(nsec, 1000000);\n+}\n+\n+#define SPLIT_NS(x) nsec_high(x), nsec_low(x)\n+\nstatic void\nprint_task(struct seq_file *m, struct rq *rq, struct task_struct *p)\n{\n@@ -36,42 +61,35 @@ print_task(struct seq_file *m, struct rq\nelse\nSEQ_printf(m, " ");\n\n-\tSEQ_printf(m, "%15s %5d %15Ld %13Ld %13Ld %9Ld %5d ",\n+\tSEQ_printf(m, "%15s %5d %9Ld.%06ld %9Ld %5d ",\np->comm, p->pid,\n-\t\t(long long)p->se.fair_key,\n-\t\t(long long)(p->se.fair_key - rq->cfs.fair_clock),\n-\t\t(long long)p->se.wait_runtime,\n+\t\tSPLIT_NS(p->se.vruntime),\n(long long)(p->nvcsw + p->nivcsw),\np->prio);\n#ifdef CONFIG_SCHEDSTATS\n-\tSEQ_printf(m, "%15Ld %15Ld %15Ld %15Ld %15Ld\\n",\n-\t\t(long long)p->se.sum_exec_runtime,\n-\t\t(long long)p->se.sum_wait_runtime,\n-\t\t(long long)p->se.sum_sleep_runtime,\n-\t\t(long long)p->se.wait_runtime_overruns,\n-\t\t(long long)p->se.wait_runtime_underruns);\n+\tSEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld\\n",\n+\t\tSPLIT_NS(p->se.vruntime),\n+\t\tSPLIT_NS(p->se.sum_exec_runtime),\n+\t\tSPLIT_NS(p->se.sum_sleep_runtime));\n#else\n-\tSEQ_printf(m, "%15Ld %15Ld %15Ld %15Ld %15Ld\\n",\n-\t\t0LL, 0LL, 0LL, 0LL, 0LL);\n+\tSEQ_printf(m, "%15Ld %15Ld %15Ld.%06ld %15Ld.%06ld %15Ld.%06ld\\n",\n+\t\t0LL, 0LL, 0LL, 0L, 0LL, 0L, 0LL, 0L);\n#endif\n}\n\nstatic void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)\n{\nstruct task_struct *g, *p;\n+\tunsigned long flags;\n\nSEQ_printf(m,\n"\\nrunnable tasks:\\n"\n-\t"            task   PID        tree-key         delta       waiting"\n-\t"  switches  prio"\n-\t"        sum-exec        sum-wait       sum-sleep"\n-\t"    wait-overrun   wait-underrun\\n"\n-\t"------------------------------------------------------------------"\n-\t"----------------"\n-\t"------------------------------------------------"\n-\t"--------------------------------\\n");\n+\t"            task   PID         tree-key  switches  prio"\n+\t"     exec-runtime         sum-exec        sum-sleep\\n"\n+\t"------------------------------------------------------"\n+\t"----------------------------------------------------\\n");\n\n-\tread_lock_irq(&tasklist_lock);\n+\tread_lock_irqsave(&tasklist_lock, flags);\n\ndo_each_thread(g, p) {\nif (!p->se.on_rq || task_cpu(p) != rq_cpu)\n@@ -80,48 +98,51 @@ static void print_rq(struct seq_file *m,\nprint_task(m, rq, p);\n} while_each_thread(g, p);\n\n-\tread_unlock_irq(&tasklist_lock);\n+\tread_unlock_irqrestore(&tasklist_lock, flags);\n}\n\n-static void\n-print_cfs_rq_runtime_sum(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)\n+void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)\n{\n-\ts64 wait_runtime_rq_sum = 0;\n-\tstruct task_struct *p;\n-\tstruct rb_node *curr;\n-\tunsigned long flags;\n+\ts64 MIN_vruntime = -1, min_vruntime, max_vruntime = -1,\n+\t\tspread, rq0_min_vruntime, spread0;\nstruct rq *rq = &per_cpu(runqueues, cpu);\n+\tstruct sched_entity *last;\n+\tunsigned long flags;\n\n-\tspin_lock_irqsave(&rq->lock, flags);\n-\tcurr = first_fair(cfs_rq);\n-\twhile (curr) {\n-\t\tp = rb_entry(curr, struct task_struct, se.run_node);\n-\t\twait_runtime_rq_sum += p->se.wait_runtime;\n-\n-\t\tcurr = rb_next(curr);\n-\t}\n-\tspin_unlock_irqrestore(&rq->lock, flags);\n-\n-\tSEQ_printf(m, "  .%-30s: %Ld\\n", "wait_runtime_rq_sum",\n-\t\t(long long)wait_runtime_rq_sum);\n-}\n-\n-void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)\n-{\nSEQ_printf(m, "\\ncfs_rq\\n");\n\n-#define P(x) \\\n-\tSEQ_printf(m, "  .%-30s: %Ld\\n", #x, (long long)(cfs_rq->x))\n-\n-\tP(fair_clock);\n-\tP(exec_clock);\n-\tP(wait_runtime);\n-\tP(wait_runtime_overruns);\n-\tP(wait_runtime_underruns);\n-\tP(sleeper_bonus);\n-#undef P\n+\tSEQ_printf(m, "  .%-30s: %Ld.%06ld\\n", "exec_clock",\n+\t\t\tSPLIT_NS(cfs_rq->exec_clock));\n\n-\tprint_cfs_rq_runtime_sum(m, cpu, cfs_rq);\n+\tspin_lock_irqsave(&rq->lock, flags);\n+\tif (cfs_rq->rb_leftmost)\n+\t\tMIN_vruntime = (__pick_next_entity(cfs_rq))->vruntime;\n+\tlast = __pick_last_entity(cfs_rq);\n+\tif (last)\n+\t\tmax_vruntime = last->vruntime;\n+\tmin_vruntime = rq->cfs.min_vruntime;\n+\trq0_min_vruntime = per_cpu(runqueues, 0).cfs.min_vruntime;\n+\tspin_unlock_irqrestore(&rq->lock, flags);\n+\tSEQ_printf(m, "  .%-30s: %Ld.%06ld\\n", "MIN_vruntime",\n+\t\t\tSPLIT_NS(MIN_vruntime));\n+\tSEQ_printf(m, "  .%-30s: %Ld.%06ld\\n", "min_vruntime",\n+\t\t\tSPLIT_NS(min_vruntime));\n+\tSEQ_printf(m, "  .%-30s: %Ld.%06ld\\n", "max_vruntime",\n+\t\t\tSPLIT_NS(max_vruntime));\n+\tspread = max_vruntime - MIN_vruntime;\n+\tSEQ_printf(m, "  .%-30s: %Ld.%06ld\\n", "spread",\n+\t\t\tSPLIT_NS(spread));\n+\tspread0 = min_vruntime - rq0_min_vruntime;\n+\tSEQ_printf(m, "  .%-30s: %Ld.%06ld\\n", "spread0",\n+\t\t\tSPLIT_NS(spread0));\n+\tSEQ_printf(m, "  .%-30s: %ld\\n", "nr_running", cfs_rq->nr_running);\n+\tSEQ_printf(m, "  .%-30s: %ld\\n", "load", cfs_rq->load.weight);\n+#ifdef CONFIG_SCHEDSTATS\n+\tSEQ_printf(m, "  .%-30s: %d\\n", "bkl_count",\n+\t\t\trq->bkl_count);\n+#endif\n+\tSEQ_printf(m, "  .%-30s: %ld\\n", "nr_spread_over",\n+\t\t\tcfs_rq->nr_spread_over);\n}\n\nstatic void print_cpu(struct seq_file *m, int cpu)\n@@ -141,31 +162,32 @@ static void print_cpu(struct seq_file *m\n\n#define P(x) \\\nSEQ_printf(m, "  .%-30s: %Ld\\n", #x, (long long)(rq->x))\n+#define PN(x) \\\n+\tSEQ_printf(m, "  .%-30s: %Ld.%06ld\\n", #x, SPLIT_NS(rq->x))\n\nP(nr_running);\nSEQ_printf(m, "  .%-30s: %lu\\n", "load",\n-\t\t   rq->ls.load.weight);\n-\tP(ls.delta_fair);\n-\tP(ls.delta_exec);\n+\t\t   rq->load.weight);\nP(nr_switches);\nP(nr_load_updates);\nP(nr_uninterruptible);\nSEQ_printf(m, "  .%-30s: %lu\\n", "jiffies", jiffies);\n-\tP(next_balance);\n+\tPN(next_balance);\nP(curr->pid);\n-\tP(clock);\n-\tP(idle_clock);\n-\tP(prev_clock_raw);\n+\tPN(clock);\n+\tPN(idle_clock);\n+\tPN(prev_clock_raw);\nP(clock_warps);\nP(clock_overflows);\nP(clock_deep_idle_events);\n-\tP(clock_max_delta);\n+\tPN(clock_max_delta);\nP(cpu_load[0]);\nP(cpu_load[1]);\nP(cpu_load[2]);\nP(cpu_load[3]);\nP(cpu_load[4]);\n#undef P\n+#undef PN\n\nprint_cfs_stats(m, cpu);\n\n@@ -177,12 +199,25 @@ static int sched_debug_show(struct seq_f\nu64 now = ktime_to_ns(ktime_get());\nint cpu;\n\n-\tSEQ_printf(m, "Sched Debug Version: v0.05-v20, %s %.*s\\n",\n+\tSEQ_printf(m, "Sched Debug Version: v0.07, %s %.*s\\n",\ninit_utsname()->release,\n(int)strcspn(init_utsname()->version, " "),\ninit_utsname()->version);\n\n-\tSEQ_printf(m, "now at %Lu nsecs\\n", (unsigned long long)now);\n+\tSEQ_printf(m, "now at %Lu.%06ld msecs\\n", SPLIT_NS(now));\n+\n+#define P(x) \\\n+\tSEQ_printf(m, "  .%-40s: %Ld\\n", #x, (long long)(x))\n+#define PN(x) \\\n+\tSEQ_printf(m, "  .%-40s: %Ld.%06ld\\n", #x, SPLIT_NS(x))\n+\tPN(sysctl_sched_latency);\n+\tPN(sysctl_sched_min_granularity);\n+\tPN(sysctl_sched_wakeup_granularity);\n+\tPN(sysctl_sched_batch_wakeup_granularity);\n+\tPN(sysctl_sched_child_runs_first);\n+\tP(sysctl_sched_features);\n+#undef PN\n+#undef P\n\nfor_each_online_cpu(cpu)\nprint_cpu(m, cpu);\n@@ -202,7 +237,7 @@ static int sched_debug_open(struct inode\nreturn single_open(filp, sched_debug_show, NULL);\n}\n\n-static struct file_operations sched_debug_fops = {\n+static const struct file_operations sched_debug_fops = {\n.open\t\t= sched_debug_open,\n.read\t\t= seq_read,\n.llseek\t\t= seq_lseek,\n@@ -226,6 +261,7 @@ __initcall(init_sched_debug_procfs);\n\nvoid proc_sched_show_task(struct task_struct *p, struct seq_file *m)\n{\n+\tunsigned long nr_switches;\nunsigned long flags;\nint num_threads = 1;\n\n@@ -237,41 +273,91 @@ void proc_sched_show_task(struct task_st\nrcu_read_unlock();\n\nSEQ_printf(m, "%s (%d, #threads: %d)\\n", p->comm, p->pid, num_threads);\n-\tSEQ_printf(m, "----------------------------------------------\\n");\n+\tSEQ_printf(m,\n+\t\t"---------------------------------------------------------\\n");\n+#define __P(F) \\\n+\tSEQ_printf(m, "%-35s:%21Ld\\n", #F, (long long)F)\n#define P(F) \\\n-\tSEQ_printf(m, "%-25s:%20Ld\\n", #F, (long long)p->F)\n+\tSEQ_printf(m, "%-35s:%21Ld\\n", #F, (long long)p->F)\n+#define __PN(F) \\\n+\tSEQ_printf(m, "%-35s:%14Ld.%06ld\\n", #F, SPLIT_NS((long long)F))\n+#define PN(F) \\\n+\tSEQ_printf(m, "%-35s:%14Ld.%06ld\\n", #F, SPLIT_NS((long long)p->F))\n+\n+\tPN(se.exec_start);\n+\tPN(se.vruntime);\n+\tPN(se.sum_exec_runtime);\n\n-\tP(se.wait_runtime);\n-\tP(se.wait_start_fair);\n-\tP(se.exec_start);\n-\tP(se.sleep_start_fair);\n-\tP(se.sum_exec_runtime);\n+\tnr_switches = p->nvcsw + p->nivcsw;\n\n#ifdef CONFIG_SCHEDSTATS\n-\tP(se.wait_start);\n-\tP(se.sleep_start);\n-\tP(se.block_start);\n-\tP(se.sleep_max);\n-\tP(se.block_max);\n-\tP(se.exec_max);\n-\tP(se.wait_max);\n-\tP(se.wait_runtime_overruns);\n-\tP(se.wait_runtime_underruns);\n-\tP(se.sum_wait_runtime);\n+\tPN(se.wait_start);\n+\tPN(se.sleep_start);\n+\tPN(se.block_start);\n+\tPN(se.sleep_max);\n+\tPN(se.block_max);\n+\tPN(se.exec_max);\n+\tPN(se.slice_max);\n+\tPN(se.wait_max);\n+\tP(sched_info.bkl_count);\n+\tP(se.nr_migrations);\n+\tP(se.nr_migrations_cold);\n+\tP(se.nr_failed_migrations_affine);\n+\tP(se.nr_failed_migrations_running);\n+\tP(se.nr_failed_migrations_hot);\n+\tP(se.nr_forced_migrations);\n+\tP(se.nr_forced2_migrations);\n+\tP(se.nr_wakeups);\n+\tP(se.nr_wakeups_sync);\n+\tP(se.nr_wakeups_migrate);\n+\tP(se.nr_wakeups_local);\n+\tP(se.nr_wakeups_remote);\n+\tP(se.nr_wakeups_affine);\n+\tP(se.nr_wakeups_affine_attempts);\n+\tP(se.nr_wakeups_passive);\n+\tP(se.nr_wakeups_idle);\n+\n+\t{\n+\t\tu64 avg_atom, avg_per_cpu;\n+\n+\t\tavg_atom = p->se.sum_exec_runtime;\n+\t\tif (nr_switches)\n+\t\t\tdo_div(avg_atom, nr_switches);\n+\t\telse\n+\t\t\tavg_atom = -1LL;\n+\n+\t\tavg_per_cpu = p->se.sum_exec_runtime;\n+\t\tif (p->se.nr_migrations) {\n+\t\t\tavg_per_cpu = div64_64(avg_per_cpu,\n+\t\t\t\t\t       p->se.nr_migrations);\n+\t\t} else {\n+\t\t\tavg_per_cpu = -1LL;\n+\t\t}\n+\n+\t\t__PN(avg_atom);\n+\t\t__PN(avg_per_cpu);\n+\t}\n#endif\n-\tSEQ_printf(m, "%-25s:%20Ld\\n",\n-\t\t   "nr_switches", (long long)(p->nvcsw + p->nivcsw));\n+\t__P(nr_switches);\n+\tSEQ_printf(m, "%-35s:%21Ld\\n",\n+\t\t   "nr_voluntary_switches", (long long)p->nvcsw);\n+\tSEQ_printf(m, "%-35s:%21Ld\\n",\n+\t\t   "nr_involuntary_switches", (long long)p->nivcsw);\n+\nP(se.load.weight);\nP(policy);\nP(prio);\n+#undef PN\n+#undef __PN\n#undef P\n+#undef __P\n\n{\nu64 t0, t1;\n\nt0 = sched_clock();\nt1 = sched_clock();\n-\t\tSEQ_printf(m, "%-25s:%20Ld\\n",\n+\t\tSEQ_printf(m, "%-35s:%21Ld\\n",\n"clock-delta", (long long)(t1-t0));\n}\n}\n@@ -279,9 +365,32 @@ void proc_sched_show_task(struct task_st\nvoid proc_sched_set_task(struct task_struct *p)\n{\n#ifdef CONFIG_SCHEDSTATS\n-\tp->se.sleep_max = p->se.block_max = p->se.exec_max = p->se.wait_max = 0;\n-\tp->se.wait_runtime_overruns = p->se.wait_runtime_underruns = 0;\n+\tp->se.wait_max\t\t\t\t= 0;\n+\tp->se.sleep_max\t\t\t\t= 0;\n+\tp->se.sum_sleep_runtime\t\t\t= 0;\n+\tp->se.block_max\t\t\t\t= 0;\n+\tp->se.exec_max\t\t\t\t= 0;\n+\tp->se.slice_max\t\t\t\t= 0;\n+\tp->se.nr_migrations\t\t\t= 0;\n+\tp->se.nr_migrations_cold\t\t= 0;\n+\tp->se.nr_failed_migrations_affine\t= 0;\n+\tp->se.nr_failed_migrations_running\t= 0;\n+\tp->se.nr_failed_migrations_hot\t\t= 0;\n+\tp->se.nr_forced_migrations\t\t= 0;\n+\tp->se.nr_forced2_migrations\t\t= 0;\n+\tp->se.nr_wakeups\t\t\t= 0;\n+\tp->se.nr_wakeups_sync\t\t\t= 0;\n+\tp->se.nr_wakeups_migrate\t\t= 0;\n+\tp->se.nr_wakeups_local\t\t\t= 0;\n+\tp->se.nr_wakeups_remote\t\t\t= 0;\n+\tp->se.nr_wakeups_affine\t\t\t= 0;\n+\tp->se.nr_wakeups_affine_attempts\t= 0;\n+\tp->se.nr_wakeups_passive\t\t= 0;\n+\tp->se.nr_wakeups_idle\t\t\t= 0;\n+\tp->sched_info.bkl_count\t\t\t= 0;\n#endif\n-\tp->se.sum_exec_runtime = 0;\n-\tp->se.prev_sum_exec_runtime\t= 0;\n+\tp->se.sum_exec_runtime\t\t\t= 0;\n+\tp->se.prev_sum_exec_runtime\t\t= 0;\n+\tp->nvcsw\t\t\t\t= 0;\n+\tp->nivcsw\t\t\t\t= 0;\n}'),
 ('linux-2.6.23.orig/kernel/sched_fair.c',
  'linux-2.6.23/kernel/sched_fair.c',
  'Index: linux-2.6.23/kernel/sched_fair.c\n===================================================================\n--- linux-2.6.23.orig/kernel/sched_fair.c\n+++ linux-2.6.23/kernel/sched_fair.c\n@@ -22,25 +22,34 @@\n\n/*\n* Targeted preemption latency for CPU-bound tasks:\n- * (default: 20ms, units: nanoseconds)\n+ * (default: 20ms * (1 + ilog(ncpus)), units: nanoseconds)\n*\n* NOTE: this latency value is not the same as the concept of\n- * \'timeslice length\' - timeslices in CFS are of variable length.\n- * (to see the precise effective timeslice length of your workload,\n- *  run vmstat and monitor the context-switches field)\n+ * \'timeslice length\' - timeslices in CFS are of variable length\n+ * and have no persistent notion like in traditional, time-slice\n+ * based scheduling concepts.\n*\n- * On SMP systems the value of this is multiplied by the log2 of the\n- * number of CPUs. (i.e. factor 2x on 2-way systems, 3x on 4-way\n- * systems, 4x on 8-way systems, 5x on 16-way systems, etc.)\n- * Targeted preemption latency for CPU-bound tasks:\n+ * (to see the precise effective timeslice length of your workload,\n+ *  run vmstat and monitor the context-switches (cs) field)\n*/\n-unsigned int sysctl_sched_latency __read_mostly = 20000000ULL;\n+unsigned int sysctl_sched_latency = 20000000ULL;\n\n/*\n* Minimal preemption granularity for CPU-bound tasks:\n- * (default: 2 msec, units: nanoseconds)\n+ * (default: 4 msec * (1 + ilog(ncpus)), units: nanoseconds)\n+ */\n+unsigned int sysctl_sched_min_granularity = 4000000ULL;\n+\n+/*\n+ * is kept at sysctl_sched_latency / sysctl_sched_min_granularity\n*/\n-unsigned int sysctl_sched_min_granularity __read_mostly = 2000000ULL;\n+static unsigned int sched_nr_latency = 5;\n+\n+/*\n+ * After fork, child runs first. (default) If set to 0 then\n+ * parent will (try to) run first.\n+ */\n+const_debug unsigned int sysctl_sched_child_runs_first = 1;\n\n/*\n* sys_sched_yield() compat mode\n@@ -52,52 +61,25 @@ unsigned int __read_mostly sysctl_sched_\n\n/*\n* SCHED_BATCH wake-up granularity.\n- * (default: 25 msec, units: nanoseconds)\n+ * (default: 10 msec * (1 + ilog(ncpus)), units: nanoseconds)\n*\n* This option delays the preemption effects of decoupled workloads\n* and reduces their over-scheduling. Synchronous workloads will still\n* have immediate wakeup/sleep latencies.\n*/\n-unsigned int sysctl_sched_batch_wakeup_granularity __read_mostly = 25000000UL;\n+unsigned int sysctl_sched_batch_wakeup_granularity = 10000000UL;\n\n/*\n* SCHED_OTHER wake-up granularity.\n- * (default: 1 msec, units: nanoseconds)\n+ * (default: 10 msec * (1 + ilog(ncpus)), units: nanoseconds)\n*\n* This option delays the preemption effects of decoupled workloads\n* and reduces their over-scheduling. Synchronous workloads will still\n* have immediate wakeup/sleep latencies.\n*/\n-unsigned int sysctl_sched_wakeup_granularity __read_mostly = 1000000UL;\n-\n-unsigned int sysctl_sched_stat_granularity __read_mostly;\n-\n-/*\n- * Initialized in sched_init_granularity() [to 5 times the base granularity]:\n- */\n-unsigned int sysctl_sched_runtime_limit __read_mostly;\n-\n-/*\n- * Debugging: various feature bits\n- */\n-enum {\n-\tSCHED_FEAT_FAIR_SLEEPERS\t= 1,\n-\tSCHED_FEAT_SLEEPER_AVG\t\t= 2,\n-\tSCHED_FEAT_SLEEPER_LOAD_AVG\t= 4,\n-\tSCHED_FEAT_PRECISE_CPU_LOAD\t= 8,\n-\tSCHED_FEAT_START_DEBIT\t\t= 16,\n-\tSCHED_FEAT_SKIP_INITIAL\t\t= 32,\n-};\n-\n-unsigned int sysctl_sched_features __read_mostly =\n-\t\tSCHED_FEAT_FAIR_SLEEPERS\t*1 |\n-\t\tSCHED_FEAT_SLEEPER_AVG\t\t*0 |\n-\t\tSCHED_FEAT_SLEEPER_LOAD_AVG\t*1 |\n-\t\tSCHED_FEAT_PRECISE_CPU_LOAD\t*0 |\n-\t\tSCHED_FEAT_START_DEBIT\t\t*1 |\n-\t\tSCHED_FEAT_SKIP_INITIAL\t\t*0;\n+unsigned int sysctl_sched_wakeup_granularity = 10000000UL;\n\n-extern struct sched_class fair_sched_class;\n+const_debug unsigned int sysctl_sched_migration_cost = 500000UL;\n\n/**************************************************************\n* CFS operations on generic schedulable entities:\n@@ -111,21 +93,9 @@ static inline struct rq *rq_of(struct cf\nreturn cfs_rq->rq;\n}\n\n-/* currently running entity (if any) on this cfs_rq */\n-static inline struct sched_entity *cfs_rq_curr(struct cfs_rq *cfs_rq)\n-{\n-\treturn cfs_rq->curr;\n-}\n-\n/* An entity is a task if it doesn\'t "own" a runqueue */\n#define entity_is_task(se)\t(!se->my_q)\n\n-static inline void\n-set_cfs_rq_curr(struct cfs_rq *cfs_rq, struct sched_entity *se)\n-{\n-\tcfs_rq->curr = se;\n-}\n-\n#else\t/* CONFIG_FAIR_GROUP_SCHED */\n\nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n@@ -133,21 +103,8 @@ static inline struct rq *rq_of(struct cf\nreturn container_of(cfs_rq, struct rq, cfs);\n}\n\n-static inline struct sched_entity *cfs_rq_curr(struct cfs_rq *cfs_rq)\n-{\n-\tstruct rq *rq = rq_of(cfs_rq);\n-\n-\tif (unlikely(rq->curr->sched_class != &fair_sched_class))\n-\t\treturn NULL;\n-\n-\treturn &rq->curr->se;\n-}\n-\n#define entity_is_task(se)\t1\n\n-static inline void\n-set_cfs_rq_curr(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\n-\n#endif\t/* CONFIG_FAIR_GROUP_SCHED */\n\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n@@ -160,16 +117,38 @@ static inline struct task_struct *task_o\n* Scheduling class tree data structure manipulation methods:\n*/\n\n+static inline u64 max_vruntime(u64 min_vruntime, u64 vruntime)\n+{\n+\ts64 delta = (s64)(vruntime - min_vruntime);\n+\tif (delta > 0)\n+\t\tmin_vruntime = vruntime;\n+\n+\treturn min_vruntime;\n+}\n+\n+static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n+{\n+\ts64 delta = (s64)(vruntime - min_vruntime);\n+\tif (delta < 0)\n+\t\tmin_vruntime = vruntime;\n+\n+\treturn min_vruntime;\n+}\n+\n+static inline s64 entity_key(struct cfs_rq *cfs_rq, struct sched_entity *se)\n+{\n+\treturn se->vruntime - cfs_rq->min_vruntime;\n+}\n+\n/*\n* Enqueue an entity into the rb-tree:\n*/\n-static inline void\n-__enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n+static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\nstruct rb_node **link = &cfs_rq->tasks_timeline.rb_node;\nstruct rb_node *parent = NULL;\nstruct sched_entity *entry;\n-\ts64 key = se->fair_key;\n+\ts64 key = entity_key(cfs_rq, se);\nint leftmost = 1;\n\n/*\n@@ -182,7 +161,7 @@ __enqueue_entity(struct cfs_rq *cfs_rq,\n* We dont care about collisions. Nodes with\n* the same key stay together.\n*/\n-\t\tif (key - entry->fair_key < 0) {\n+\t\tif (key < entity_key(cfs_rq, entry)) {\nlink = &parent->rb_left;\n} else {\nlink = &parent->rb_right;\n@@ -199,24 +178,14 @@ __enqueue_entity(struct cfs_rq *cfs_rq,\n\nrb_link_node(&se->run_node, parent, link);\nrb_insert_color(&se->run_node, &cfs_rq->tasks_timeline);\n-\tupdate_load_add(&cfs_rq->load, se->load.weight);\n-\tcfs_rq->nr_running++;\n-\tse->on_rq = 1;\n-\n-\tschedstat_add(cfs_rq, wait_runtime, se->wait_runtime);\n}\n\n-static inline void\n-__dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n+static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\nif (cfs_rq->rb_leftmost == &se->run_node)\ncfs_rq->rb_leftmost = rb_next(&se->run_node);\n-\trb_erase(&se->run_node, &cfs_rq->tasks_timeline);\n-\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n-\tcfs_rq->nr_running--;\n-\tse->on_rq = 0;\n\n-\tschedstat_add(cfs_rq, wait_runtime, -se->wait_runtime);\n+\trb_erase(&se->run_node, &cfs_rq->tasks_timeline);\n}\n\nstatic inline struct rb_node *first_fair(struct cfs_rq *cfs_rq)\n@@ -229,118 +198,103 @@ static struct sched_entity *__pick_next_\nreturn rb_entry(first_fair(cfs_rq), struct sched_entity, run_node);\n}\n\n+static inline struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)\n+{\n+\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_node;\n+\tstruct sched_entity *se = NULL;\n+\tstruct rb_node *parent;\n+\n+\twhile (*link) {\n+\t\tparent = *link;\n+\t\tse = rb_entry(parent, struct sched_entity, run_node);\n+\t\tlink = &parent->rb_right;\n+\t}\n+\n+\treturn se;\n+}\n+\n/**************************************************************\n* Scheduling class statistics methods:\n*/\n\n+#ifdef CONFIG_SCHED_DEBUG\n+int sched_nr_latency_handler(struct ctl_table *table, int write,\n+\t\tstruct file *filp, void __user *buffer, size_t *lenp,\n+\t\tloff_t *ppos)\n+{\n+\tint ret = proc_dointvec_minmax(table, write, filp, buffer, lenp, ppos);\n+\n+\tif (ret || !write)\n+\t\treturn ret;\n+\n+\tsched_nr_latency = DIV_ROUND_UP(sysctl_sched_latency,\n+\t\t\t\t\tsysctl_sched_min_granularity);\n+\n+\treturn 0;\n+}\n+#endif\n+\n/*\n- * Calculate the preemption granularity needed to schedule every\n- * runnable task once per sysctl_sched_latency amount of time.\n- * (down to a sensible low limit on granularity)\n- *\n- * For example, if there are 2 tasks running and latency is 10 msecs,\n- * we switch tasks every 5 msecs. If we have 3 tasks running, we have\n- * to switch tasks every 3.33 msecs to get a 10 msecs observed latency\n- * for each task. We do finer and finer scheduling up to until we\n- * reach the minimum granularity value.\n- *\n- * To achieve this we use the following dynamic-granularity rule:\n- *\n- *    gran = lat/nr - lat/nr/nr\n+ * The idea is to set a period in which each task runs once.\n*\n- * This comes out of the following equations:\n+ * When there are too many tasks (sysctl_sched_nr_latency) we have to stretch\n+ * this period because otherwise the slices get too small.\n*\n- *    kA1 + gran = kB1\n- *    kB2 + gran = kA2\n- *    kA2 = kA1\n- *    kB2 = kB1 - d + d/nr\n- *    lat = d * nr\n- *\n- * Where \'k\' is key, \'A\' is task A (waiting), \'B\' is task B (running),\n- * \'1\' is start of time, \'2\' is end of time, \'d\' is delay between\n- * 1 and 2 (during which task B was running), \'nr\' is number of tasks\n- * running, \'lat\' is the the period of each task. (\'lat\' is the\n- * sched_latency that we aim for.)\n+ * p = (nr <= nl) ? l : l*nr/nl\n*/\n-static long\n-sched_granularity(struct cfs_rq *cfs_rq)\n+static u64 __sched_period(unsigned long nr_running)\n{\n-\tunsigned int gran = sysctl_sched_latency;\n-\tunsigned int nr = cfs_rq->nr_running;\n+\tu64 period = sysctl_sched_latency;\n+\tunsigned long nr_latency = sched_nr_latency;\n\n-\tif (nr > 1) {\n-\t\tgran = gran/nr - gran/nr/nr;\n-\t\tgran = max(gran, sysctl_sched_min_granularity);\n+\tif (unlikely(nr_running > nr_latency)) {\n+\t\tperiod *= nr_running;\n+\t\tdo_div(period, nr_latency);\n}\n\n-\treturn gran;\n+\treturn period;\n}\n\n/*\n- * We rescale the rescheduling granularity of tasks according to their\n- * nice level, but only linearly, not exponentially:\n+ * We calculate the wall-time slice from the period by taking a part\n+ * proportional to the weight.\n+ *\n+ * s = p*w/rw\n*/\n-static long\n-niced_granularity(struct sched_entity *curr, unsigned long granularity)\n+static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\tu64 tmp;\n+\tu64 slice = __sched_period(cfs_rq->nr_running);\n\n-\tif (likely(curr->load.weight == NICE_0_LOAD))\n-\t\treturn granularity;\n-\t/*\n-\t * Positive nice levels get the same granularity as nice-0:\n-\t */\n-\tif (likely(curr->load.weight < NICE_0_LOAD)) {\n-\t\ttmp = curr->load.weight * (u64)granularity;\n-\t\treturn (long) (tmp >> NICE_0_SHIFT);\n-\t}\n-\t/*\n-\t * Negative nice level tasks get linearly finer\n-\t * granularity:\n-\t */\n-\ttmp = curr->load.inv_weight * (u64)granularity;\n+\tslice *= se->load.weight;\n+\tdo_div(slice, cfs_rq->load.weight);\n\n-\t/*\n-\t * It will always fit into \'long\':\n-\t */\n-\treturn (long) (tmp >> (WMULT_SHIFT-NICE_0_SHIFT));\n+\treturn slice;\n}\n\n-static inline void\n-limit_wait_runtime(struct cfs_rq *cfs_rq, struct sched_entity *se)\n+/*\n+ * We calculate the vruntime slice.\n+ *\n+ * vs = s/w = p/rw\n+ */\n+static u64 __sched_vslice(unsigned long rq_weight, unsigned long nr_running)\n{\n-\tlong limit = sysctl_sched_runtime_limit;\n+\tu64 vslice = __sched_period(nr_running);\n\n-\t/*\n-\t * Niced tasks have the same history dynamic range as\n-\t * non-niced tasks:\n-\t */\n-\tif (unlikely(se->wait_runtime > limit)) {\n-\t\tse->wait_runtime = limit;\n-\t\tschedstat_inc(se, wait_runtime_overruns);\n-\t\tschedstat_inc(cfs_rq, wait_runtime_overruns);\n-\t}\n-\tif (unlikely(se->wait_runtime < -limit)) {\n-\t\tse->wait_runtime = -limit;\n-\t\tschedstat_inc(se, wait_runtime_underruns);\n-\t\tschedstat_inc(cfs_rq, wait_runtime_underruns);\n-\t}\n+\tvslice *= NICE_0_LOAD;\n+\tdo_div(vslice, rq_weight);\n+\n+\treturn vslice;\n}\n\n-static inline void\n-__add_wait_runtime(struct cfs_rq *cfs_rq, struct sched_entity *se, long delta)\n+static u64 sched_vslice(struct cfs_rq *cfs_rq)\n{\n-\tse->wait_runtime += delta;\n-\tschedstat_add(se, sum_wait_runtime, delta);\n-\tlimit_wait_runtime(cfs_rq, se);\n+\treturn __sched_vslice(cfs_rq->load.weight, cfs_rq->nr_running);\n}\n\n-static void\n-add_wait_runtime(struct cfs_rq *cfs_rq, struct sched_entity *se, long delta)\n+static u64 sched_vslice_add(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\tschedstat_add(cfs_rq, wait_runtime, -se->wait_runtime);\n-\t__add_wait_runtime(cfs_rq, se, delta);\n-\tschedstat_add(cfs_rq, wait_runtime, se->wait_runtime);\n+\treturn __sched_vslice(cfs_rq->load.weight + se->load.weight,\n+\t\t\tcfs_rq->nr_running + 1);\n}\n\n/*\n@@ -348,46 +302,41 @@ add_wait_runtime(struct cfs_rq *cfs_rq,\n* are not in our scheduling class.\n*/\nstatic inline void\n-__update_curr(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n+__update_curr(struct cfs_rq *cfs_rq, struct sched_entity *curr,\n+\t      unsigned long delta_exec)\n{\n-\tunsigned long delta, delta_exec, delta_fair, delta_mine;\n-\tstruct load_weight *lw = &cfs_rq->load;\n-\tunsigned long load = lw->weight;\n+\tunsigned long delta_exec_weighted;\n+\tu64 vruntime;\n\n-\tdelta_exec = curr->delta_exec;\nschedstat_set(curr->exec_max, max((u64)delta_exec, curr->exec_max));\n\ncurr->sum_exec_runtime += delta_exec;\n-\tcfs_rq->exec_clock += delta_exec;\n-\n-\tif (unlikely(!load))\n-\t\treturn;\n-\n-\tdelta_fair = calc_delta_fair(delta_exec, lw);\n-\tdelta_mine = calc_delta_mine(delta_exec, curr->load.weight, lw);\n-\n-\tif (cfs_rq->sleeper_bonus > sysctl_sched_min_granularity) {\n-\t\tdelta = min((u64)delta_mine, cfs_rq->sleeper_bonus);\n-\t\tdelta = min(delta, (unsigned long)(\n-\t\t\t(long)sysctl_sched_runtime_limit - curr->wait_runtime));\n-\t\tcfs_rq->sleeper_bonus -= delta;\n-\t\tdelta_mine -= delta;\n+\tschedstat_add(cfs_rq, exec_clock, delta_exec);\n+\tdelta_exec_weighted = delta_exec;\n+\tif (unlikely(curr->load.weight != NICE_0_LOAD)) {\n+\t\tdelta_exec_weighted = calc_delta_fair(delta_exec_weighted,\n+\t\t\t\t\t\t\t&curr->load);\n}\n+\tcurr->vruntime += delta_exec_weighted;\n\n-\tcfs_rq->fair_clock += delta_fair;\n/*\n-\t * We executed delta_exec amount of time on the CPU,\n-\t * but we were only entitled to delta_mine amount of\n-\t * time during that period (if nr_running == 1 then\n-\t * the two values are equal)\n-\t * [Note: delta_mine - delta_exec is negative]:\n+\t * maintain cfs_rq->min_vruntime to be a monotonic increasing\n+\t * value tracking the leftmost vruntime in the tree.\n*/\n-\tadd_wait_runtime(cfs_rq, curr, delta_mine - delta_exec);\n+\tif (first_fair(cfs_rq)) {\n+\t\tvruntime = min_vruntime(curr->vruntime,\n+\t\t\t\t__pick_next_entity(cfs_rq)->vruntime);\n+\t} else\n+\t\tvruntime = curr->vruntime;\n+\n+\tcfs_rq->min_vruntime =\n+\t\tmax_vruntime(cfs_rq->min_vruntime, vruntime);\n}\n\nstatic void update_curr(struct cfs_rq *cfs_rq)\n{\n-\tstruct sched_entity *curr = cfs_rq_curr(cfs_rq);\n+\tstruct sched_entity *curr = cfs_rq->curr;\n+\tu64 now = rq_of(cfs_rq)->clock;\nunsigned long delta_exec;\n\nif (unlikely(!curr))\n@@ -398,135 +347,53 @@ static void update_curr(struct cfs_rq *c\n* since the last time we changed load (this cannot\n* overflow on 32 bits):\n*/\n-\tdelta_exec = (unsigned long)(rq_of(cfs_rq)->clock - curr->exec_start);\n+\tdelta_exec = (unsigned long)(now - curr->exec_start);\n+\n+\t__update_curr(cfs_rq, curr, delta_exec);\n+\tcurr->exec_start = now;\n\n-\tcurr->delta_exec += delta_exec;\n+\tif (entity_is_task(curr)) {\n+\t\tstruct task_struct *curtask = task_of(curr);\n\n-\tif (unlikely(curr->delta_exec > sysctl_sched_stat_granularity)) {\n-\t\t__update_curr(cfs_rq, curr);\n-\t\tcurr->delta_exec = 0;\n+\t\tcpuacct_charge(curtask, delta_exec);\n}\n-\tcurr->exec_start = rq_of(cfs_rq)->clock;\n}\n\nstatic inline void\nupdate_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\tse->wait_start_fair = cfs_rq->fair_clock;\nschedstat_set(se->wait_start, rq_of(cfs_rq)->clock);\n}\n\n/*\n- * We calculate fair deltas here, so protect against the random effects\n- * of a multiplication overflow by capping it to the runtime limit:\n- */\n-#if BITS_PER_LONG == 32\n-static inline unsigned long\n-calc_weighted(unsigned long delta, unsigned long weight, int shift)\n-{\n-\tu64 tmp = (u64)delta * weight >> shift;\n-\n-\tif (unlikely(tmp > sysctl_sched_runtime_limit*2))\n-\t\treturn sysctl_sched_runtime_limit*2;\n-\treturn tmp;\n-}\n-#else\n-static inline unsigned long\n-calc_weighted(unsigned long delta, unsigned long weight, int shift)\n-{\n-\treturn delta * weight >> shift;\n-}\n-#endif\n-\n-/*\n* Task is being enqueued - update stats:\n*/\nstatic void update_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\ts64 key;\n-\n/*\n* Are we enqueueing a waiting task? (for current tasks\n* a dequeue/enqueue event is a NOP)\n*/\n-\tif (se != cfs_rq_curr(cfs_rq))\n+\tif (se != cfs_rq->curr)\nupdate_stats_wait_start(cfs_rq, se);\n-\t/*\n-\t * Update the key:\n-\t */\n-\tkey = cfs_rq->fair_clock;\n-\n-\t/*\n-\t * Optimize the common nice 0 case:\n-\t */\n-\tif (likely(se->load.weight == NICE_0_LOAD)) {\n-\t\tkey -= se->wait_runtime;\n-\t} else {\n-\t\tu64 tmp;\n-\n-\t\tif (se->wait_runtime < 0) {\n-\t\t\ttmp = -se->wait_runtime;\n-\t\t\tkey += (tmp * se->load.inv_weight) >>\n-\t\t\t\t\t(WMULT_SHIFT - NICE_0_SHIFT);\n-\t\t} else {\n-\t\t\ttmp = se->wait_runtime;\n-\t\t\tkey -= (tmp * se->load.inv_weight) >>\n-\t\t\t\t\t(WMULT_SHIFT - NICE_0_SHIFT);\n-\t\t}\n-\t}\n-\n-\tse->fair_key = key;\n-}\n-\n-/*\n- * Note: must be called with a freshly updated rq->fair_clock.\n- */\n-static inline void\n-__update_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n-{\n-\tunsigned long delta_fair = se->delta_fair_run;\n-\n-\tschedstat_set(se->wait_max, max(se->wait_max,\n-\t\t\trq_of(cfs_rq)->clock - se->wait_start));\n-\n-\tif (unlikely(se->load.weight != NICE_0_LOAD))\n-\t\tdelta_fair = calc_weighted(delta_fair, se->load.weight,\n-\t\t\t\t\t\t\tNICE_0_SHIFT);\n-\n-\tadd_wait_runtime(cfs_rq, se, delta_fair);\n}\n\nstatic void\nupdate_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\tunsigned long delta_fair;\n-\n-\tif (unlikely(!se->wait_start_fair))\n-\t\treturn;\n-\n-\tdelta_fair = (unsigned long)min((u64)(2*sysctl_sched_runtime_limit),\n-\t\t\t(u64)(cfs_rq->fair_clock - se->wait_start_fair));\n-\n-\tse->delta_fair_run += delta_fair;\n-\tif (unlikely(abs(se->delta_fair_run) >=\n-\t\t\t\tsysctl_sched_stat_granularity)) {\n-\t\t__update_stats_wait_end(cfs_rq, se);\n-\t\tse->delta_fair_run = 0;\n-\t}\n-\n-\tse->wait_start_fair = 0;\n+\tschedstat_set(se->wait_max, max(se->wait_max,\n+\t\t\trq_of(cfs_rq)->clock - se->wait_start));\nschedstat_set(se->wait_start, 0);\n}\n\nstatic inline void\nupdate_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\tupdate_curr(cfs_rq);\n/*\n* Mark the end of the wait period if dequeueing a\n* waiting task:\n*/\n-\tif (se != cfs_rq_curr(cfs_rq))\n+\tif (se != cfs_rq->curr)\nupdate_stats_wait_end(cfs_rq, se);\n}\n\n@@ -542,79 +409,28 @@ update_stats_curr_start(struct cfs_rq *c\nse->exec_start = rq_of(cfs_rq)->clock;\n}\n\n-/*\n- * We are descheduling a task - update its stats:\n- */\n-static inline void\n-update_stats_curr_end(struct cfs_rq *cfs_rq, struct sched_entity *se)\n-{\n-\tse->exec_start = 0;\n-}\n-\n/**************************************************\n* Scheduling class queueing methods:\n*/\n\n-static void __enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)\n+static void\n+account_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\tunsigned long load = cfs_rq->load.weight, delta_fair;\n-\tlong prev_runtime;\n-\n-\t/*\n-\t * Do not boost sleepers if there\'s too much bonus \'in flight\'\n-\t * already:\n-\t */\n-\tif (unlikely(cfs_rq->sleeper_bonus > sysctl_sched_runtime_limit))\n-\t\treturn;\n-\n-\tif (sysctl_sched_features & SCHED_FEAT_SLEEPER_LOAD_AVG)\n-\t\tload = rq_of(cfs_rq)->cpu_load[2];\n-\n-\tdelta_fair = se->delta_fair_sleep;\n-\n-\t/*\n-\t * Fix up delta_fair with the effect of us running\n-\t * during the whole sleep period:\n-\t */\n-\tif (sysctl_sched_features & SCHED_FEAT_SLEEPER_AVG)\n-\t\tdelta_fair = div64_likely32((u64)delta_fair * load,\n-\t\t\t\t\t\tload + se->load.weight);\n-\n-\tif (unlikely(se->load.weight != NICE_0_LOAD))\n-\t\tdelta_fair = calc_weighted(delta_fair, se->load.weight,\n-\t\t\t\t\t\t\tNICE_0_SHIFT);\n-\n-\tprev_runtime = se->wait_runtime;\n-\t__add_wait_runtime(cfs_rq, se, delta_fair);\n-\tdelta_fair = se->wait_runtime - prev_runtime;\n+\tupdate_load_add(&cfs_rq->load, se->load.weight);\n+\tcfs_rq->nr_running++;\n+\tse->on_rq = 1;\n+}\n\n-\t/*\n-\t * Track the amount of bonus we\'ve given to sleepers:\n-\t */\n-\tcfs_rq->sleeper_bonus += delta_fair;\n+static void\n+account_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n+{\n+\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n+\tcfs_rq->nr_running--;\n+\tse->on_rq = 0;\n}\n\nstatic void enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\tstruct task_struct *tsk = task_of(se);\n-\tunsigned long delta_fair;\n-\n-\tif ((entity_is_task(se) && tsk->policy == SCHED_BATCH) ||\n-\t\t\t !(sysctl_sched_features & SCHED_FEAT_FAIR_SLEEPERS))\n-\t\treturn;\n-\n-\tdelta_fair = (unsigned long)min((u64)(2*sysctl_sched_runtime_limit),\n-\t\t(u64)(cfs_rq->fair_clock - se->sleep_start_fair));\n-\n-\tse->delta_fair_sleep += delta_fair;\n-\tif (unlikely(abs(se->delta_fair_sleep) >=\n-\t\t\t\tsysctl_sched_stat_granularity)) {\n-\t\t__enqueue_sleeper(cfs_rq, se);\n-\t\tse->delta_fair_sleep = 0;\n-\t}\n-\n-\tse->sleep_start_fair = 0;\n-\n#ifdef CONFIG_SCHEDSTATS\nif (se->sleep_start) {\nu64 delta = rq_of(cfs_rq)->clock - se->sleep_start;\n@@ -646,6 +462,8 @@ static void enqueue_sleeper(struct cfs_r\n* time that the task spent sleeping:\n*/\nif (unlikely(immediate_read(prof_on) == SLEEP_PROFILING)) {\n+\t\t\tstruct task_struct *tsk = task_of(se);\n+\nprofile_hits(SLEEP_PROFILING, (void *)get_wchan(tsk),\ndelta >> 20);\n}\n@@ -653,27 +471,86 @@ static void enqueue_sleeper(struct cfs_r\n#endif\n}\n\n+static void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)\n+{\n+#ifdef CONFIG_SCHED_DEBUG\n+\ts64 d = se->vruntime - cfs_rq->min_vruntime;\n+\n+\tif (d < 0)\n+\t\td = -d;\n+\n+\tif (d > 3*sysctl_sched_latency)\n+\t\tschedstat_inc(cfs_rq, nr_spread_over);\n+#endif\n+}\n+\n+static void\n+place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)\n+{\n+\tu64 vruntime;\n+\n+\tvruntime = cfs_rq->min_vruntime;\n+\n+\tif (sched_feat(TREE_AVG)) {\n+\t\tstruct sched_entity *last = __pick_last_entity(cfs_rq);\n+\t\tif (last) {\n+\t\t\tvruntime += last->vruntime;\n+\t\t\tvruntime >>= 1;\n+\t\t}\n+\t} else if (sched_feat(APPROX_AVG) && cfs_rq->nr_running)\n+\t\tvruntime += sched_vslice(cfs_rq)/2;\n+\n+\t/*\n+\t * The \'current\' period is already promised to the current tasks,\n+\t * however the extra weight of the new task will slow them down a\n+\t * little, place the new task so that it fits in the slot that\n+\t * stays open at the end.\n+\t */\n+\tif (initial && sched_feat(START_DEBIT))\n+\t\tvruntime += sched_vslice_add(cfs_rq, se);\n+\n+\tif (!initial) {\n+\t\t/* sleeps upto a single latency don\'t count. */\n+\t\tif (sched_feat(NEW_FAIR_SLEEPERS) && entity_is_task(se))\n+\t\t\tvruntime -= sysctl_sched_latency;\n+\n+\t\t/* ensure we never gain time by being placed backwards. */\n+\t\tvruntime = max_vruntime(se->vruntime, vruntime);\n+\t}\n+\n+\tse->vruntime = vruntime;\n+}\n+\nstatic void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int wakeup)\n{\n/*\n-\t * Update the fair clock.\n+\t * Update run-time statistics of the \'current\'.\n*/\nupdate_curr(cfs_rq);\n\n-\tif (wakeup)\n+\tif (wakeup) {\n+\t\tplace_entity(cfs_rq, se, 0);\nenqueue_sleeper(cfs_rq, se);\n+\t}\n\nupdate_stats_enqueue(cfs_rq, se);\n-\t__enqueue_entity(cfs_rq, se);\n+\tcheck_spread(cfs_rq, se);\n+\tif (se != cfs_rq->curr)\n+\t\t__enqueue_entity(cfs_rq, se);\n+\taccount_entity_enqueue(cfs_rq, se);\n}\n\nstatic void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int sleep)\n{\n+\t/*\n+\t * Update run-time statistics of the \'current\'.\n+\t */\n+\tupdate_curr(cfs_rq);\n+\nupdate_stats_dequeue(cfs_rq, se);\nif (sleep) {\n-\t\tse->sleep_start_fair = cfs_rq->fair_clock;\n#ifdef CONFIG_SCHEDSTATS\nif (entity_is_task(se)) {\nstruct task_struct *tsk = task_of(se);\n@@ -685,68 +562,64 @@ dequeue_entity(struct cfs_rq *cfs_rq, st\n}\n#endif\n}\n-\t__dequeue_entity(cfs_rq, se);\n+\n+\tif (se != cfs_rq->curr)\n+\t\t__dequeue_entity(cfs_rq, se);\n+\taccount_entity_dequeue(cfs_rq, se);\n}\n\n/*\n* Preempt the current task with a newly woken task if needed:\n*/\nstatic void\n-__check_preempt_curr_fair(struct cfs_rq *cfs_rq, struct sched_entity *se,\n-\t\t\t  struct sched_entity *curr, unsigned long granularity)\n+check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n-\ts64 __delta = curr->fair_key - se->fair_key;\nunsigned long ideal_runtime, delta_exec;\n\n-\t/*\n-\t * ideal_runtime is compared against sum_exec_runtime, which is\n-\t * walltime, hence do not scale.\n-\t */\n-\tideal_runtime = max(sysctl_sched_latency / cfs_rq->nr_running,\n-\t\t\t(unsigned long)sysctl_sched_min_granularity);\n-\n-\t/*\n-\t * If we executed more than what the latency constraint suggests,\n-\t * reduce the rescheduling granularity. This way the total latency\n-\t * of how much a task is not scheduled converges to\n-\t * sysctl_sched_latency:\n-\t */\n+\tideal_runtime = sched_slice(cfs_rq, curr);\ndelta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;\nif (delta_exec > ideal_runtime)\n-\t\tgranularity = 0;\n-\n-\t/*\n-\t * Take scheduling granularity into account - do not\n-\t * preempt the current task unless the best task has\n-\t * a larger than sched_granularity fairness advantage:\n-\t *\n-\t * scale granularity as key space is in fair_clock.\n-\t */\n-\tif (__delta > niced_granularity(curr, granularity))\nresched_task(rq_of(cfs_rq)->curr);\n}\n\n-static inline void\n+static void\nset_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n-\t/*\n-\t * Any task has to be enqueued before it get to execute on\n-\t * a CPU. So account for the time it spent waiting on the\n-\t * runqueue. (note, here we rely on pick_next_task() having\n-\t * done a put_prev_task_fair() shortly before this, which\n-\t * updated rq->fair_clock - used by update_stats_wait_end())\n-\t */\n-\tupdate_stats_wait_end(cfs_rq, se);\n+\t/* \'current\' is not kept within the tree. */\n+\tif (se->on_rq) {\n+\t\t/*\n+\t\t * Any task has to be enqueued before it get to execute on\n+\t\t * a CPU. So account for the time it spent waiting on the\n+\t\t * runqueue.\n+\t\t */\n+\t\tupdate_stats_wait_end(cfs_rq, se);\n+\t\t__dequeue_entity(cfs_rq, se);\n+\t}\n+\nupdate_stats_curr_start(cfs_rq, se);\n-\tset_cfs_rq_curr(cfs_rq, se);\n+\tcfs_rq->curr = se;\n+#ifdef CONFIG_SCHEDSTATS\n+\t/*\n+\t * Track our maximum slice length, if the CPU\'s load is at\n+\t * least twice that of our own weight (i.e. dont track it\n+\t * when there are only lesser-weight tasks around):\n+\t */\n+\tif (rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {\n+\t\tse->slice_max = max(se->slice_max,\n+\t\t\tse->sum_exec_runtime - se->prev_sum_exec_runtime);\n+\t}\n+#endif\nse->prev_sum_exec_runtime = se->sum_exec_runtime;\n}\n\nstatic struct sched_entity *pick_next_entity(struct cfs_rq *cfs_rq)\n{\n-\tstruct sched_entity *se = __pick_next_entity(cfs_rq);\n+\tstruct sched_entity *se = NULL;\n\n-\tset_next_entity(cfs_rq, se);\n+\tif (first_fair(cfs_rq)) {\n+\t\tse = __pick_next_entity(cfs_rq);\n+\t\tset_next_entity(cfs_rq, se);\n+\t}\n\nreturn se;\n}\n@@ -760,33 +633,24 @@ static void put_prev_entity(struct cfs_r\nif (prev->on_rq)\nupdate_curr(cfs_rq);\n\n-\tupdate_stats_curr_end(cfs_rq, prev);\n-\n-\tif (prev->on_rq)\n+\tcheck_spread(cfs_rq, prev);\n+\tif (prev->on_rq) {\nupdate_stats_wait_start(cfs_rq, prev);\n-\tset_cfs_rq_curr(cfs_rq, NULL);\n+\t\t/* Put \'current\' back into the tree. */\n+\t\t__enqueue_entity(cfs_rq, prev);\n+\t}\n+\tcfs_rq->curr = NULL;\n}\n\nstatic void entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n-\tstruct sched_entity *next;\n-\n-\t/*\n-\t * Dequeue and enqueue the task to update its\n-\t * position within the tree:\n-\t */\n-\tdequeue_entity(cfs_rq, curr, 0);\n-\tenqueue_entity(cfs_rq, curr, 0);\n-\n/*\n-\t * Reschedule if another task tops the current one.\n+\t * Update run-time statistics of the \'current\'.\n*/\n-\tnext = __pick_next_entity(cfs_rq);\n-\tif (next == curr)\n-\t\treturn;\n+\tupdate_curr(cfs_rq);\n\n-\t__check_preempt_curr_fair(cfs_rq, next, curr,\n-\t\t\tsched_granularity(cfs_rq));\n+\tif (cfs_rq->nr_running > 1 || !sched_feat(WAKEUP_PREEMPT))\n+\t\tcheck_preempt_tick(cfs_rq, curr);\n}\n\n/**************************************************\n@@ -821,23 +685,28 @@ static inline struct cfs_rq *group_cfs_r\n*/\nstatic inline struct cfs_rq *cpu_cfs_rq(struct cfs_rq *cfs_rq, int this_cpu)\n{\n-\t/* A later patch will take group into account */\n-\treturn &cpu_rq(this_cpu)->cfs;\n+\treturn cfs_rq->tg->cfs_rq[this_cpu];\n}\n\n/* Iterate thr\' all leaf cfs_rq\'s on a runqueue */\n#define for_each_leaf_cfs_rq(rq, cfs_rq) \\\nlist_for_each_entry(cfs_rq, &rq->leaf_cfs_rq_list, leaf_cfs_rq_list)\n\n-/* Do the two (enqueued) tasks belong to the same group ? */\n-static inline int is_same_group(struct task_struct *curr, struct task_struct *p)\n+/* Do the two (enqueued) entities belong to the same group ? */\n+static inline int\n+is_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n-\tif (curr->se.cfs_rq == p->se.cfs_rq)\n+\tif (se->cfs_rq == pse->cfs_rq)\nreturn 1;\n\nreturn 0;\n}\n\n+static inline struct sched_entity *parent_entity(struct sched_entity *se)\n+{\n+\treturn se->parent;\n+}\n+\n#else\t/* CONFIG_FAIR_GROUP_SCHED */\n\n#define for_each_sched_entity(se) \\\n@@ -870,11 +739,17 @@ static inline struct cfs_rq *cpu_cfs_rq(\n#define for_each_leaf_cfs_rq(rq, cfs_rq) \\\nfor (cfs_rq = &rq->cfs; cfs_rq; cfs_rq = NULL)\n\n-static inline int is_same_group(struct task_struct *curr, struct task_struct *p)\n+static inline int\n+is_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\nreturn 1;\n}\n\n+static inline struct sched_entity *parent_entity(struct sched_entity *se)\n+{\n+\treturn NULL;\n+}\n+\n#endif\t/* CONFIG_FAIR_GROUP_SCHED */\n\n/*\n@@ -892,6 +767,7 @@ static void enqueue_task_fair(struct rq\nbreak;\ncfs_rq = cfs_rq_of(se);\nenqueue_entity(cfs_rq, se, wakeup);\n+\t\twakeup = 1;\n}\n}\n\n@@ -911,6 +787,7 @@ static void dequeue_task_fair(struct rq\n/* Don\'t dequeue parent if it has other entities besides us */\nif (cfs_rq->load.weight)\nbreak;\n+\t\tsleep = 1;\n}\n}\n\n@@ -919,12 +796,11 @@ static void dequeue_task_fair(struct rq\n*\n* If compat_yield is turned on then we requeue to the end of the tree.\n*/\n-static void yield_task_fair(struct rq *rq, struct task_struct *p)\n+static void yield_task_fair(struct rq *rq)\n{\n-\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n-\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_node;\n-\tstruct sched_entity *rightmost, *se = &p->se;\n-\tstruct rb_node *parent;\n+\tstruct task_struct *curr = rq->curr;\n+\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n+\tstruct sched_entity *rightmost, *se = &curr->se;\n\n/*\n* Are we the only task in the tree?\n@@ -932,54 +808,41 @@ static void yield_task_fair(struct rq *r\nif (unlikely(cfs_rq->nr_running == 1))\nreturn;\n\n-\tif (likely(!sysctl_sched_compat_yield)) {\n+\tif (likely(!sysctl_sched_compat_yield) && curr->policy != SCHED_BATCH) {\n__update_rq_clock(rq);\n/*\n-\t\t * Dequeue and enqueue the task to update its\n-\t\t * position within the tree:\n+\t\t * Update run-time statistics of the \'current\'.\n*/\n-\t\tdequeue_entity(cfs_rq, &p->se, 0);\n-\t\tenqueue_entity(cfs_rq, &p->se, 0);\n+\t\tupdate_curr(cfs_rq);\n\nreturn;\n}\n/*\n* Find the rightmost entry in the rbtree:\n*/\n-\tdo {\n-\t\tparent = *link;\n-\t\tlink = &parent->rb_right;\n-\t} while (*link);\n-\n-\trightmost = rb_entry(parent, struct sched_entity, run_node);\n+\trightmost = __pick_last_entity(cfs_rq);\n/*\n* Already in the rightmost position?\n*/\n-\tif (unlikely(rightmost == se))\n+\tif (unlikely(rightmost->vruntime < se->vruntime))\nreturn;\n\n/*\n* Minimally necessary key value to be last in the tree:\n+\t * Upon rescheduling, sched_class::put_prev_task() will place\n+\t * \'current\' within the tree based on its new key value.\n*/\n-\tse->fair_key = rightmost->fair_key + 1;\n-\n-\tif (cfs_rq->rb_leftmost == &se->run_node)\n-\t\tcfs_rq->rb_leftmost = rb_next(&se->run_node);\n-\t/*\n-\t * Relink the task to the rightmost position:\n-\t */\n-\trb_erase(&se->run_node, &cfs_rq->tasks_timeline);\n-\trb_link_node(&se->run_node, parent, link);\n-\trb_insert_color(&se->run_node, &cfs_rq->tasks_timeline);\n+\tse->vruntime = rightmost->vruntime + 1;\n}\n\n/*\n* Preempt the current task with a newly woken task if needed:\n*/\n-static void check_preempt_curr_fair(struct rq *rq, struct task_struct *p)\n+static void check_preempt_wakeup(struct rq *rq, struct task_struct *p)\n{\nstruct task_struct *curr = rq->curr;\nstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n+\tstruct sched_entity *se = &curr->se, *pse = &p->se;\nunsigned long gran;\n\nif (unlikely(rt_prio(p->prio))) {\n@@ -988,16 +851,27 @@ static void check_preempt_curr_fair(stru\nresched_task(curr);\nreturn;\n}\n-\n-\tgran = sysctl_sched_wakeup_granularity;\n/*\n-\t * Batch tasks prefer throughput over latency:\n+\t * Batch tasks do not preempt (their preemption is driven by\n+\t * the tick):\n*/\nif (unlikely(p->policy == SCHED_BATCH))\n-\t\tgran = sysctl_sched_batch_wakeup_granularity;\n+\t\treturn;\n+\n+\tif (!sched_feat(WAKEUP_PREEMPT))\n+\t\treturn;\n+\n+\twhile (!is_same_group(se, pse)) {\n+\t\tse = parent_entity(se);\n+\t\tpse = parent_entity(pse);\n+\t}\n\n-\tif (is_same_group(curr, p))\n-\t\t__check_preempt_curr_fair(cfs_rq, &p->se, &curr->se, gran);\n+\tgran = sysctl_sched_wakeup_granularity;\n+\tif (unlikely(se->load.weight != NICE_0_LOAD))\n+\t\tgran = calc_delta_fair(gran, &se->load);\n+\n+\tif (pse->vruntime + gran < se->vruntime)\n+\t\tresched_task(curr);\n}\n\nstatic struct task_struct *pick_next_task_fair(struct rq *rq)\n@@ -1030,6 +904,7 @@ static void put_prev_task_fair(struct rq\n}\n}\n\n+#ifdef CONFIG_SMP\n/**************************************************\n* Fair scheduling class load-balancing methods:\n*/\n@@ -1041,7 +916,7 @@ static void put_prev_task_fair(struct rq\n* achieve that by always pre-iterating before returning\n* the current task:\n*/\n-static inline struct task_struct *\n+static struct task_struct *\n__load_balance_iterator(struct cfs_rq *cfs_rq, struct rb_node *curr)\n{\nstruct task_struct *p;\n@@ -1078,7 +953,10 @@ static int cfs_rq_best_prio(struct cfs_r\nif (!cfs_rq->nr_running)\nreturn MAX_PRIO;\n\n-\tcurr = __pick_next_entity(cfs_rq);\n+\tcurr = cfs_rq->curr;\n+\tif (!curr)\n+\t\tcurr = __pick_next_entity(cfs_rq);\n+\np = task_of(curr);\n\nreturn p->prio;\n@@ -1087,12 +965,11 @@ static int cfs_rq_best_prio(struct cfs_r\n\nstatic unsigned long\nload_balance_fair(struct rq *this_rq, int this_cpu, struct rq *busiest,\n-\t\t  unsigned long max_nr_move, unsigned long max_load_move,\n+\t\t  unsigned long max_load_move,\nstruct sched_domain *sd, enum cpu_idle_type idle,\nint *all_pinned, int *this_best_prio)\n{\nstruct cfs_rq *busy_cfs_rq;\n-\tunsigned long load_moved, total_nr_moved = 0, nr_moved;\nlong rem_load_move = max_load_move;\nstruct rq_iterator cfs_rq_iterator;\n\n@@ -1120,25 +997,48 @@ load_balance_fair(struct rq *this_rq, in\n#else\n# define maxload rem_load_move\n#endif\n-\t\t/* pass busy_cfs_rq argument into\n+\t\t/*\n+\t\t * pass busy_cfs_rq argument into\n* load_balance_[start|next]_fair iterators\n*/\ncfs_rq_iterator.arg = busy_cfs_rq;\n-\t\tnr_moved = balance_tasks(this_rq, this_cpu, busiest,\n-\t\t\t\tmax_nr_move, maxload, sd, idle, all_pinned,\n-\t\t\t\t&load_moved, this_best_prio, &cfs_rq_iterator);\n-\n-\t\ttotal_nr_moved += nr_moved;\n-\t\tmax_nr_move -= nr_moved;\n-\t\trem_load_move -= load_moved;\n+\t\trem_load_move -= balance_tasks(this_rq, this_cpu, busiest,\n+\t\t\t\t\t       maxload, sd, idle, all_pinned,\n+\t\t\t\t\t       this_best_prio,\n+\t\t\t\t\t       &cfs_rq_iterator);\n\n-\t\tif (max_nr_move <= 0 || rem_load_move <= 0)\n+\t\tif (rem_load_move <= 0)\nbreak;\n}\n\nreturn max_load_move - rem_load_move;\n}\n\n+static int\n+move_one_task_fair(struct rq *this_rq, int this_cpu, struct rq *busiest,\n+\t\t   struct sched_domain *sd, enum cpu_idle_type idle)\n+{\n+\tstruct cfs_rq *busy_cfs_rq;\n+\tstruct rq_iterator cfs_rq_iterator;\n+\n+\tcfs_rq_iterator.start = load_balance_start_fair;\n+\tcfs_rq_iterator.next = load_balance_next_fair;\n+\n+\tfor_each_leaf_cfs_rq(busiest, busy_cfs_rq) {\n+\t\t/*\n+\t\t * pass busy_cfs_rq argument into\n+\t\t * load_balance_[start|next]_fair iterators\n+\t\t */\n+\t\tcfs_rq_iterator.arg = busy_cfs_rq;\n+\t\tif (iter_move_one_task(this_rq, this_cpu, busiest, sd, idle,\n+\t\t\t\t       &cfs_rq_iterator))\n+\t\t    return 1;\n+\t}\n+\n+\treturn 0;\n+}\n+#endif\n+\n/*\n* scheduler tick hitting a task of our scheduling class:\n*/\n@@ -1153,6 +1053,8 @@ static void task_tick_fair(struct rq *rq\n}\n}\n\n+#define swap(a, b) do { typeof(a) tmp = (a); (a) = (b); (b) = tmp; } while (0)\n+\n/*\n* Share the fairness runtime between parent and child, thus the\n* total amount of pressure for CPU stays equal - new tasks\n@@ -1163,37 +1065,28 @@ static void task_tick_fair(struct rq *rq\nstatic void task_new_fair(struct rq *rq, struct task_struct *p)\n{\nstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n-\tstruct sched_entity *se = &p->se, *curr = cfs_rq_curr(cfs_rq);\n+\tstruct sched_entity *se = &p->se, *curr = cfs_rq->curr;\n+\tint this_cpu = smp_processor_id();\n\nsched_info_queued(p);\n\nupdate_curr(cfs_rq);\n-\tupdate_stats_enqueue(cfs_rq, se);\n-\t/*\n-\t * Child runs first: we let it run before the parent\n-\t * until it reschedules once. We set up the key so that\n-\t * it will preempt the parent:\n-\t */\n-\tse->fair_key = curr->fair_key -\n-\t\tniced_granularity(curr, sched_granularity(cfs_rq)) - 1;\n-\t/*\n-\t * The first wait is dominated by the child-runs-first logic,\n-\t * so do not credit it with that waiting time yet:\n-\t */\n-\tif (sysctl_sched_features & SCHED_FEAT_SKIP_INITIAL)\n-\t\tse->wait_start_fair = 0;\n+\tplace_entity(cfs_rq, se, 1);\n\n-\t/*\n-\t * The statistical average of wait_runtime is about\n-\t * -granularity/2, so initialize the task with that:\n-\t */\n-\tif (sysctl_sched_features & SCHED_FEAT_START_DEBIT)\n-\t\tse->wait_runtime = -(sched_granularity(cfs_rq) / 2);\n+\t/* \'curr\' will be NULL if the child belongs to a different group */\n+\tif (sysctl_sched_child_runs_first && this_cpu == task_cpu(p) &&\n+\t\t\tcurr && curr->vruntime < se->vruntime) {\n+\t\t/*\n+\t\t * Upon rescheduling, sched_class::put_prev_task() will place\n+\t\t * \'current\' within the tree based on its new key value.\n+\t\t */\n+\t\tswap(curr->vruntime, se->vruntime);\n+\t}\n\n-\t__enqueue_entity(cfs_rq, se);\n+\tenqueue_task_fair(rq, p, 0);\n+\tresched_task(rq->curr);\n}\n\n-#ifdef CONFIG_FAIR_GROUP_SCHED\n/* Account for a task changing its policy or group.\n*\n* This routine is mostly called to set cfs_rq->curr field when a task\n@@ -1206,26 +1099,25 @@ static void set_curr_task_fair(struct rq\nfor_each_sched_entity(se)\nset_next_entity(cfs_rq_of(se), se);\n}\n-#else\n-static void set_curr_task_fair(struct rq *rq)\n-{\n-}\n-#endif\n\n/*\n* All the scheduling class methods:\n*/\n-struct sched_class fair_sched_class __read_mostly = {\n+static const struct sched_class fair_sched_class = {\n+\t.next\t\t\t= &idle_sched_class,\n.enqueue_task\t\t= enqueue_task_fair,\n.dequeue_task\t\t= dequeue_task_fair,\n.yield_task\t\t= yield_task_fair,\n\n-\t.check_preempt_curr\t= check_preempt_curr_fair,\n+\t.check_preempt_curr\t= check_preempt_wakeup,\n\n.pick_next_task\t\t= pick_next_task_fair,\n.put_prev_task\t\t= put_prev_task_fair,\n\n+#ifdef CONFIG_SMP\n.load_balance\t\t= load_balance_fair,\n+\t.move_one_task\t\t= move_one_task_fair,\n+#endif\n\n.set_curr_task          = set_curr_task_fair,\n.task_tick\t\t= task_tick_fair,\n@@ -1237,6 +1129,9 @@ static void print_cfs_stats(struct seq_f\n{\nstruct cfs_rq *cfs_rq;\n\n+#ifdef CONFIG_FAIR_GROUP_SCHED\n+\tprint_cfs_rq(m, cpu, &cpu_rq(cpu)->cfs);\n+#endif\nfor_each_leaf_cfs_rq(cpu_rq(cpu), cfs_rq)\nprint_cfs_rq(m, cpu, cfs_rq);\n}'),
 ('linux-2.6.23.orig/kernel/sched_idletask.c',
  'linux-2.6.23/kernel/sched_idletask.c',
  'Index: linux-2.6.23/kernel/sched_idletask.c\n===================================================================\n--- linux-2.6.23.orig/kernel/sched_idletask.c\n+++ linux-2.6.23/kernel/sched_idletask.c\n@@ -37,23 +37,37 @@ static void put_prev_task_idle(struct rq\n{\n}\n\n+#ifdef CONFIG_SMP\nstatic unsigned long\nload_balance_idle(struct rq *this_rq, int this_cpu, struct rq *busiest,\n-\t\t\tunsigned long max_nr_move, unsigned long max_load_move,\n-\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n-\t\t\tint *all_pinned, int *this_best_prio)\n+\t\t  unsigned long max_load_move,\n+\t\t  struct sched_domain *sd, enum cpu_idle_type idle,\n+\t\t  int *all_pinned, int *this_best_prio)\n{\nreturn 0;\n}\n\n+static int\n+move_one_task_idle(struct rq *this_rq, int this_cpu, struct rq *busiest,\n+\t\t   struct sched_domain *sd, enum cpu_idle_type idle)\n+{\n+\treturn 0;\n+}\n+#endif\n+\nstatic void task_tick_idle(struct rq *rq, struct task_struct *curr)\n{\n}\n\n+static void set_curr_task_idle(struct rq *rq)\n+{\n+}\n+\n/*\n* Simple, special scheduling class for the per-CPU idle tasks:\n*/\n-static struct sched_class idle_sched_class __read_mostly = {\n+const struct sched_class idle_sched_class = {\n+\t/* .next is NULL */\n/* no enqueue/yield_task for idle tasks */\n\n/* dequeue is not valid, we print a debug message there: */\n@@ -64,8 +78,12 @@ static struct sched_class idle_sched_cla\n.pick_next_task\t\t= pick_next_task_idle,\n.put_prev_task\t\t= put_prev_task_idle,\n\n+#ifdef CONFIG_SMP\n.load_balance\t\t= load_balance_idle,\n+\t.move_one_task\t\t= move_one_task_idle,\n+#endif\n\n+\t.set_curr_task          = set_curr_task_idle,\n.task_tick\t\t= task_tick_idle,\n/* no .task_new for idle tasks */\n};'),
 ('linux-2.6.23.orig/kernel/sched_rt.c',
  'linux-2.6.23/kernel/sched_rt.c',
  "Index: linux-2.6.23/kernel/sched_rt.c\n===================================================================\n--- linux-2.6.23.orig/kernel/sched_rt.c\n+++ linux-2.6.23/kernel/sched_rt.c\n@@ -7,7 +7,7 @@\n* Update the current task's runtime statistics. Skip current tasks that\n* are not in our scheduling class.\n*/\n-static inline void update_curr_rt(struct rq *rq)\n+static void update_curr_rt(struct rq *rq)\n{\nstruct task_struct *curr = rq->curr;\nu64 delta_exec;\n@@ -23,6 +23,7 @@ static inline void update_curr_rt(struct\n\ncurr->se.sum_exec_runtime += delta_exec;\ncurr->se.exec_start = rq->clock;\n+\tcpuacct_charge(curr, delta_exec);\n}\n\nstatic void enqueue_task_rt(struct rq *rq, struct task_struct *p, int wakeup)\n@@ -59,9 +60,9 @@ static void requeue_task_rt(struct rq *r\n}\n\nstatic void\n-yield_task_rt(struct rq *rq, struct task_struct *p)\n+yield_task_rt(struct rq *rq)\n{\n-\trequeue_task_rt(rq, p);\n+\trequeue_task_rt(rq, rq->curr);\n}\n\n/*\n@@ -98,6 +99,7 @@ static void put_prev_task_rt(struct rq *\np->se.exec_start = 0;\n}\n\n+#ifdef CONFIG_SMP\n/*\n* Load-balancing iterator. Note: while the runqueue stays locked\n* during the whole iteration, the current task might be\n@@ -172,13 +174,11 @@ static struct task_struct *load_balance_\n\nstatic unsigned long\nload_balance_rt(struct rq *this_rq, int this_cpu, struct rq *busiest,\n-\t\t\tunsigned long max_nr_move, unsigned long max_load_move,\n-\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n-\t\t\tint *all_pinned, int *this_best_prio)\n+\t\tunsigned long max_load_move,\n+\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n+\t\tint *all_pinned, int *this_best_prio)\n{\n-\tint nr_moved;\nstruct rq_iterator rt_rq_iterator;\n-\tunsigned long load_moved;\n\nrt_rq_iterator.start = load_balance_start_rt;\nrt_rq_iterator.next = load_balance_next_rt;\n@@ -187,15 +187,29 @@ load_balance_rt(struct rq *this_rq, int\n*/\nrt_rq_iterator.arg = busiest;\n\n-\tnr_moved = balance_tasks(this_rq, this_cpu, busiest, max_nr_move,\n-\t\t\tmax_load_move, sd, idle, all_pinned, &load_moved,\n-\t\t\tthis_best_prio, &rt_rq_iterator);\n+\treturn balance_tasks(this_rq, this_cpu, busiest, max_load_move, sd,\n+\t\t\t     idle, all_pinned, this_best_prio, &rt_rq_iterator);\n+}\n+\n+static int\n+move_one_task_rt(struct rq *this_rq, int this_cpu, struct rq *busiest,\n+\t\t struct sched_domain *sd, enum cpu_idle_type idle)\n+{\n+\tstruct rq_iterator rt_rq_iterator;\n+\n+\trt_rq_iterator.start = load_balance_start_rt;\n+\trt_rq_iterator.next = load_balance_next_rt;\n+\trt_rq_iterator.arg = busiest;\n\n-\treturn load_moved;\n+\treturn iter_move_one_task(this_rq, this_cpu, busiest, sd, idle,\n+\t\t\t\t  &rt_rq_iterator);\n}\n+#endif\n\nstatic void task_tick_rt(struct rq *rq, struct task_struct *p)\n{\n+\tupdate_curr_rt(rq);\n+\n/*\n* RR tasks need a special form of timeslice management.\n* FIFO tasks have no timeslices.\n@@ -206,7 +220,7 @@ static void task_tick_rt(struct rq *rq,\nif (--p->time_slice)\nreturn;\n\n-\tp->time_slice = static_prio_timeslice(p->static_prio);\n+\tp->time_slice = DEF_TIMESLICE;\n\n/*\n* Requeue to the end of queue if we are not the only element\n@@ -218,7 +232,15 @@ static void task_tick_rt(struct rq *rq,\n}\n}\n\n-static struct sched_class rt_sched_class __read_mostly = {\n+static void set_curr_task_rt(struct rq *rq)\n+{\n+\tstruct task_struct *p = rq->curr;\n+\n+\tp->se.exec_start = rq->clock;\n+}\n+\n+const struct sched_class rt_sched_class = {\n+\t.next\t\t\t= &fair_sched_class,\n.enqueue_task\t\t= enqueue_task_rt,\n.dequeue_task\t\t= dequeue_task_rt,\n.yield_task\t\t= yield_task_rt,\n@@ -228,7 +250,11 @@ static struct sched_class rt_sched_class\n.pick_next_task\t\t= pick_next_task_rt,\n.put_prev_task\t\t= put_prev_task_rt,\n\n+#ifdef CONFIG_SMP\n.load_balance\t\t= load_balance_rt,\n+\t.move_one_task\t\t= move_one_task_rt,\n+#endif\n\n+\t.set_curr_task          = set_curr_task_rt,\n.task_tick\t\t= task_tick_rt,\n};"),
 ('linux-2.6.23.orig/kernel/sched_stats.h',
  'linux-2.6.23/kernel/sched_stats.h',
  'Index: linux-2.6.23/kernel/sched_stats.h\n===================================================================\n--- linux-2.6.23.orig/kernel/sched_stats.h\n+++ linux-2.6.23/kernel/sched_stats.h\n@@ -16,18 +16,18 @@ static int show_schedstat(struct seq_fil\nstruct rq *rq = cpu_rq(cpu);\n#ifdef CONFIG_SMP\nstruct sched_domain *sd;\n-\t\tint dcnt = 0;\n+\t\tint dcount = 0;\n#endif\n\n/* runqueue-specific stats */\nseq_printf(seq,\n-\t\t    "cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %llu %llu %lu",\n+\t\t    "cpu%d %u %u %u %u %u %u %u %u %u %llu %llu %lu",\ncpu, rq->yld_both_empty,\n-\t\t    rq->yld_act_empty, rq->yld_exp_empty, rq->yld_cnt,\n-\t\t    rq->sched_switch, rq->sched_cnt, rq->sched_goidle,\n-\t\t    rq->ttwu_cnt, rq->ttwu_local,\n+\t\t    rq->yld_act_empty, rq->yld_exp_empty, rq->yld_count,\n+\t\t    rq->sched_switch, rq->sched_count, rq->sched_goidle,\n+\t\t    rq->ttwu_count, rq->ttwu_local,\nrq->rq_sched_info.cpu_time,\n-\t\t    rq->rq_sched_info.run_delay, rq->rq_sched_info.pcnt);\n+\t\t    rq->rq_sched_info.run_delay, rq->rq_sched_info.pcount);\n\nseq_printf(seq, "\\n");\n\n@@ -39,12 +39,11 @@ static int show_schedstat(struct seq_fil\nchar mask_str[NR_CPUS];\n\ncpumask_scnprintf(mask_str, NR_CPUS, sd->span);\n-\t\t\tseq_printf(seq, "domain%d %s", dcnt++, mask_str);\n+\t\t\tseq_printf(seq, "domain%d %s", dcount++, mask_str);\nfor (itype = CPU_IDLE; itype < CPU_MAX_IDLE_TYPES;\nitype++) {\n-\t\t\t\tseq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu "\n-\t\t\t\t\t\t"%lu",\n-\t\t\t\t    sd->lb_cnt[itype],\n+\t\t\t\tseq_printf(seq, " %u %u %u %u %u %u %u %u",\n+\t\t\t\t    sd->lb_count[itype],\nsd->lb_balanced[itype],\nsd->lb_failed[itype],\nsd->lb_imbalance[itype],\n@@ -53,11 +52,11 @@ static int show_schedstat(struct seq_fil\nsd->lb_nobusyq[itype],\nsd->lb_nobusyg[itype]);\n}\n-\t\t\tseq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu %lu"\n-\t\t\t    " %lu %lu %lu\\n",\n-\t\t\t    sd->alb_cnt, sd->alb_failed, sd->alb_pushed,\n-\t\t\t    sd->sbe_cnt, sd->sbe_balanced, sd->sbe_pushed,\n-\t\t\t    sd->sbf_cnt, sd->sbf_balanced, sd->sbf_pushed,\n+\t\t\tseq_printf(seq,\n+\t\t\t\t   " %u %u %u %u %u %u %u %u %u %u %u %u\\n",\n+\t\t\t    sd->alb_count, sd->alb_failed, sd->alb_pushed,\n+\t\t\t    sd->sbe_count, sd->sbe_balanced, sd->sbe_pushed,\n+\t\t\t    sd->sbf_count, sd->sbf_balanced, sd->sbf_pushed,\nsd->ttwu_wake_remote, sd->ttwu_move_affine,\nsd->ttwu_move_balance);\n}\n@@ -101,7 +100,7 @@ rq_sched_info_arrive(struct rq *rq, unsi\n{\nif (rq) {\nrq->rq_sched_info.run_delay += delta;\n-\t\trq->rq_sched_info.pcnt++;\n+\t\trq->rq_sched_info.pcount++;\n}\n}\n\n@@ -157,14 +156,14 @@ static inline void sched_info_dequeued(s\n*/\nstatic void sched_info_arrive(struct task_struct *t)\n{\n-\tunsigned long long now = sched_clock(), delta = 0;\n+\tunsigned long long now = task_rq(t)->clock, delta = 0;\n\nif (t->sched_info.last_queued)\ndelta = now - t->sched_info.last_queued;\nsched_info_dequeued(t);\nt->sched_info.run_delay += delta;\nt->sched_info.last_arrival = now;\n-\tt->sched_info.pcnt++;\n+\tt->sched_info.pcount++;\n\nrq_sched_info_arrive(task_rq(t), delta);\n}\n@@ -188,7 +187,7 @@ static inline void sched_info_queued(str\n{\nif (unlikely(sched_info_on()))\nif (!t->sched_info.last_queued)\n-\t\t\tt->sched_info.last_queued = sched_clock();\n+\t\t\tt->sched_info.last_queued = task_rq(t)->clock;\n}\n\n/*\n@@ -197,7 +196,8 @@ static inline void sched_info_queued(str\n*/\nstatic inline void sched_info_depart(struct task_struct *t)\n{\n-\tunsigned long long delta = sched_clock() - t->sched_info.last_arrival;\n+\tunsigned long long delta = task_rq(t)->clock -\n+\t\t\t\t\tt->sched_info.last_arrival;\n\nt->sched_info.cpu_time += delta;\nrq_sched_info_depart(task_rq(t), delta);'),
 ('linux-2.6.23.orig/kernel/sysctl.c',
  'linux-2.6.23/kernel/sysctl.c',
  'Index: linux-2.6.23/kernel/sysctl.c\n===================================================================\n--- linux-2.6.23.orig/kernel/sysctl.c\n+++ linux-2.6.23/kernel/sysctl.c\n@@ -216,12 +216,12 @@ static ctl_table root_table[] = {\n\n#ifdef CONFIG_SCHED_DEBUG\nstatic unsigned long min_sched_granularity_ns = 100000;\t\t/* 100 usecs */\n-static unsigned long max_sched_granularity_ns = 1000000000;\t/* 1 second */\n+static unsigned long max_sched_granularity_ns = NSEC_PER_SEC;\t/* 1 second */\nstatic unsigned long min_wakeup_granularity_ns;\t\t\t/* 0 usecs */\n-static unsigned long max_wakeup_granularity_ns = 1000000000;\t/* 1 second */\n+static unsigned long max_wakeup_granularity_ns = NSEC_PER_SEC;\t/* 1 second */\n#endif\n\n-static ctl_table kern_table[] = {\n+static struct ctl_table kern_table[] = {\n#ifdef CONFIG_SCHED_DEBUG\n{\n.ctl_name\t= CTL_UNNUMBERED,\n@@ -229,7 +229,7 @@ static ctl_table kern_table[] = {\n.data\t\t= &sysctl_sched_min_granularity,\n.maxlen\t\t= sizeof(unsigned int),\n.mode\t\t= 0644,\n-\t\t.proc_handler\t= &proc_dointvec_minmax,\n+\t\t.proc_handler\t= &sched_nr_latency_handler,\n.strategy\t= &sysctl_intvec,\n.extra1\t\t= &min_sched_granularity_ns,\n.extra2\t\t= &max_sched_granularity_ns,\n@@ -240,7 +240,7 @@ static ctl_table kern_table[] = {\n.data\t\t= &sysctl_sched_latency,\n.maxlen\t\t= sizeof(unsigned int),\n.mode\t\t= 0644,\n-\t\t.proc_handler\t= &proc_dointvec_minmax,\n+\t\t.proc_handler\t= &sched_nr_latency_handler,\n.strategy\t= &sysctl_intvec,\n.extra1\t\t= &min_sched_granularity_ns,\n.extra2\t\t= &max_sched_granularity_ns,\n@@ -269,43 +269,39 @@ static ctl_table kern_table[] = {\n},\n{\n.ctl_name\t= CTL_UNNUMBERED,\n-\t\t.procname\t= "sched_stat_granularity_ns",\n-\t\t.data\t\t= &sysctl_sched_stat_granularity,\n+\t\t.procname\t= "sched_child_runs_first",\n+\t\t.data\t\t= &sysctl_sched_child_runs_first,\n.maxlen\t\t= sizeof(unsigned int),\n.mode\t\t= 0644,\n-\t\t.proc_handler\t= &proc_dointvec_minmax,\n-\t\t.strategy\t= &sysctl_intvec,\n-\t\t.extra1\t\t= &min_wakeup_granularity_ns,\n-\t\t.extra2\t\t= &max_wakeup_granularity_ns,\n+\t\t.proc_handler\t= &proc_dointvec,\n},\n{\n.ctl_name\t= CTL_UNNUMBERED,\n-\t\t.procname\t= "sched_runtime_limit_ns",\n-\t\t.data\t\t= &sysctl_sched_runtime_limit,\n+\t\t.procname\t= "sched_features",\n+\t\t.data\t\t= &sysctl_sched_features,\n.maxlen\t\t= sizeof(unsigned int),\n.mode\t\t= 0644,\n-\t\t.proc_handler\t= &proc_dointvec_minmax,\n-\t\t.strategy\t= &sysctl_intvec,\n-\t\t.extra1\t\t= &min_sched_granularity_ns,\n-\t\t.extra2\t\t= &max_sched_granularity_ns,\n+\t\t.proc_handler\t= &proc_dointvec,\n},\n{\n.ctl_name\t= CTL_UNNUMBERED,\n-\t\t.procname\t= "sched_child_runs_first",\n-\t\t.data\t\t= &sysctl_sched_child_runs_first,\n+\t\t.procname\t= "sched_migration_cost",\n+\t\t.data\t\t= &sysctl_sched_migration_cost,\n.maxlen\t\t= sizeof(unsigned int),\n.mode\t\t= 0644,\n.proc_handler\t= &proc_dointvec,\n},\n+#if defined(CONFIG_FAIR_GROUP_SCHED) && defined(CONFIG_SMP)\n{\n.ctl_name\t= CTL_UNNUMBERED,\n-\t\t.procname\t= "sched_features",\n-\t\t.data\t\t= &sysctl_sched_features,\n+\t\t.procname\t= "sched_nr_migrate",\n+\t\t.data\t\t= &sysctl_sched_nr_migrate,\n.maxlen\t\t= sizeof(unsigned int),\n-\t\t.mode\t\t= 0644,\n+\t\t.mode\t\t= 644,\n.proc_handler\t= &proc_dointvec,\n},\n#endif\n+#endif\n{\n.ctl_name\t= CTL_UNNUMBERED,\n.procname\t= "sched_compat_yield",'),
 ('linux-2.6.23.orig/kernel/timer.c',
  'linux-2.6.23/kernel/timer.c',
  'Index: linux-2.6.23/kernel/timer.c\n===================================================================\n--- linux-2.6.23.orig/kernel/timer.c\n+++ linux-2.6.23/kernel/timer.c\n@@ -830,10 +830,13 @@ void update_process_times(int user_tick)\nint cpu = smp_processor_id();\n\n/* Note: this timer irq context must be accounted for as well. */\n-\tif (user_tick)\n+\tif (user_tick) {\naccount_user_time(p, jiffies_to_cputime(1));\n-\telse\n+\t\taccount_user_time_scaled(p, jiffies_to_cputime(1));\n+\t} else {\naccount_system_time(p, HARDIRQ_OFFSET, jiffies_to_cputime(1));\n+\t\taccount_system_time_scaled(p, jiffies_to_cputime(1));\n+\t}\nrun_local_timers();\nif (rcu_pending(cpu))\nrcu_check_callbacks(cpu, user_tick);'),
 ('linux-2.6.23.orig/kernel/tsacct.c',
  'linux-2.6.23/kernel/tsacct.c',
  'Index: linux-2.6.23/kernel/tsacct.c\n===================================================================\n--- linux-2.6.23.orig/kernel/tsacct.c\n+++ linux-2.6.23/kernel/tsacct.c\n@@ -62,6 +62,10 @@ void bacct_add_tsk(struct taskstats *sta\nrcu_read_unlock();\nstats->ac_utime\t = cputime_to_msecs(tsk->utime) * USEC_PER_MSEC;\nstats->ac_stime\t = cputime_to_msecs(tsk->stime) * USEC_PER_MSEC;\n+\tstats->ac_utimescaled =\n+\t\tcputime_to_msecs(tsk->utimescaled) * USEC_PER_MSEC;\n+\tstats->ac_stimescaled =\n+\t\tcputime_to_msecs(tsk->stimescaled) * USEC_PER_MSEC;\nstats->ac_minflt = tsk->min_flt;\nstats->ac_majflt = tsk->maj_flt;\n'),
 ('linux-2.6.23.orig/kernel/user.c',
  'linux-2.6.23/kernel/user.c',
  'Index: linux-2.6.23/kernel/user.c\n===================================================================\n--- linux-2.6.23.orig/kernel/user.c\n+++ linux-2.6.23/kernel/user.c\n@@ -50,12 +50,16 @@ struct user_struct root_user = {\n.uid_keyring\t= &root_user_keyring,\n.session_keyring = &root_session_keyring,\n#endif\n+#ifdef CONFIG_FAIR_USER_SCHED\n+\t.tg\t\t= &init_task_group,\n+#endif\n};\n\n/*\n* These routines must be called with the uidhash spinlock held!\n*/\n-static inline void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)\n+static inline void uid_hash_insert(struct user_struct *up,\n+\t\t\t\t\t\tstruct hlist_head *hashent)\n{\nhlist_add_head(&up->uidhash_node, hashent);\n}\n@@ -65,13 +69,14 @@ static inline void uid_hash_remove(struc\nhlist_del_init(&up->uidhash_node);\n}\n\n-static inline struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)\n+static inline struct user_struct *uid_hash_find(uid_t uid,\n+\t\t\t\t\t\tstruct hlist_head *hashent)\n{\nstruct user_struct *user;\nstruct hlist_node *h;\n\nhlist_for_each_entry(user, h, hashent, uidhash_node) {\n-\t\tif(user->uid == uid) {\n+\t\tif (user->uid == uid) {\natomic_inc(&user->__count);\nreturn user;\n}\n@@ -80,6 +85,203 @@ static inline struct user_struct *uid_ha\nreturn NULL;\n}\n\n+#ifdef CONFIG_FAIR_USER_SCHED\n+\n+static struct kobject uids_kobject; /* represents /sys/kernel/uids directory */\n+static DEFINE_MUTEX(uids_mutex);\n+\n+static void sched_destroy_user(struct user_struct *up)\n+{\n+\tsched_destroy_group(up->tg);\n+}\n+\n+static int sched_create_user(struct user_struct *up)\n+{\n+\tint rc = 0;\n+\n+\tup->tg = sched_create_group();\n+\tif (IS_ERR(up->tg))\n+\t\trc = -ENOMEM;\n+\n+\treturn rc;\n+}\n+\n+static void sched_switch_user(struct task_struct *p)\n+{\n+\tsched_move_task(p);\n+}\n+\n+static inline void uids_mutex_lock(void)\n+{\n+\tmutex_lock(&uids_mutex);\n+}\n+\n+static inline void uids_mutex_unlock(void)\n+{\n+\tmutex_unlock(&uids_mutex);\n+}\n+\n+/* return cpu shares held by the user */\n+ssize_t cpu_shares_show(struct kset *kset, char *buffer)\n+{\n+\tstruct user_struct *up = container_of(kset, struct user_struct, kset);\n+\n+\treturn sprintf(buffer, "%lu\\n", sched_group_shares(up->tg));\n+}\n+\n+/* modify cpu shares held by the user */\n+ssize_t cpu_shares_store(struct kset *kset, const char *buffer, size_t size)\n+{\n+\tstruct user_struct *up = container_of(kset, struct user_struct, kset);\n+\tunsigned long shares;\n+\tint rc;\n+\n+\tsscanf(buffer, "%lu", &shares);\n+\n+\trc = sched_group_set_shares(up->tg, shares);\n+\n+\treturn (rc ? rc : size);\n+}\n+\n+static void user_attr_init(struct subsys_attribute *sa, char *name, int mode)\n+{\n+\tsa->attr.name = name; sa->attr.owner = NULL;\n+\tsa->attr.mode = mode;\n+\tsa->show = cpu_shares_show;\n+\tsa->store = cpu_shares_store;\n+}\n+\n+/* Create "/sys/kernel/uids/<uid>" directory and\n+ *  "/sys/kernel/uids/<uid>/cpu_share" file for this user.\n+ */\n+static int user_kobject_create(struct user_struct *up)\n+{\n+\tstruct kset *kset = &up->kset;\n+\tstruct kobject *kobj = &kset->kobj;\n+\tint error;\n+\n+\tmemset(kset, 0, sizeof(struct kset));\n+\tkobj->parent = &uids_kobject;\t/* create under /sys/kernel/uids dir */\n+\tkobject_set_name(kobj, "%d", up->uid);\n+\tkset_init(kset);\n+\tuser_attr_init(&up->user_attr, "cpu_share", 0644);\n+\n+\terror = kobject_add(kobj);\n+\tif (error)\n+\t\tgoto done;\n+\n+\terror = sysfs_create_file(kobj, &up->user_attr.attr);\n+\tif (error)\n+\t\tkobject_del(kobj);\n+\n+\tkobject_uevent(kobj, KOBJ_ADD);\n+\n+done:\n+\treturn error;\n+}\n+\n+/* create these in sysfs filesystem:\n+ * \t"/sys/kernel/uids" directory\n+ * \t"/sys/kernel/uids/0" directory (for root user)\n+ * \t"/sys/kernel/uids/0/cpu_share" file (for root user)\n+ */\n+int __init uids_kobject_init(void)\n+{\n+\tint error;\n+\n+\t/* create under /sys/kernel dir */\n+\tuids_kobject.parent = &kernel_subsys.kobj;\n+\tuids_kobject.kset = &kernel_subsys;\n+\tkobject_set_name(&uids_kobject, "uids");\n+\tkobject_init(&uids_kobject);\n+\n+\terror = kobject_add(&uids_kobject);\n+\tif (!error)\n+\t\terror = user_kobject_create(&root_user);\n+\n+\treturn error;\n+}\n+\n+/* work function to remove sysfs directory for a user and free up\n+ * corresponding structures.\n+ */\n+static void remove_user_sysfs_dir(struct work_struct *w)\n+{\n+\tstruct user_struct *up = container_of(w, struct user_struct, work);\n+\tstruct kobject *kobj = &up->kset.kobj;\n+\tunsigned long flags;\n+\tint remove_user = 0;\n+\n+\t/* Make uid_hash_remove() + sysfs_remove_file() + kobject_del()\n+\t * atomic.\n+\t */\n+\tuids_mutex_lock();\n+\n+\tlocal_irq_save(flags);\n+\n+\tif (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {\n+\t\tuid_hash_remove(up);\n+\t\tremove_user = 1;\n+\t\tspin_unlock_irqrestore(&uidhash_lock, flags);\n+\t} else {\n+\t\tlocal_irq_restore(flags);\n+\t}\n+\n+\tif (!remove_user)\n+\t\tgoto done;\n+\n+\tsysfs_remove_file(kobj, &up->user_attr.attr);\n+\tkobject_uevent(kobj, KOBJ_REMOVE);\n+\tkobject_del(kobj);\n+\n+\tsched_destroy_user(up);\n+\tkey_put(up->uid_keyring);\n+\tkey_put(up->session_keyring);\n+\tkmem_cache_free(uid_cachep, up);\n+\n+done:\n+\tuids_mutex_unlock();\n+}\n+\n+/* IRQs are disabled and uidhash_lock is held upon function entry.\n+ * IRQ state (as stored in flags) is restored and uidhash_lock released\n+ * upon function exit.\n+ */\n+static inline void free_user(struct user_struct *up, unsigned long flags)\n+{\n+\t/* restore back the count */\n+\tatomic_inc(&up->__count);\n+\tspin_unlock_irqrestore(&uidhash_lock, flags);\n+\n+\tINIT_WORK(&up->work, remove_user_sysfs_dir);\n+\tschedule_work(&up->work);\n+}\n+\n+#else\t/* CONFIG_FAIR_USER_SCHED */\n+\n+static void sched_destroy_user(struct user_struct *up) { }\n+static int sched_create_user(struct user_struct *up) { return 0; }\n+static void sched_switch_user(struct task_struct *p) { }\n+static inline int user_kobject_create(struct user_struct *up) { return 0; }\n+static inline void uids_mutex_lock(void) { }\n+static inline void uids_mutex_unlock(void) { }\n+\n+/* IRQs are disabled and uidhash_lock is held upon function entry.\n+ * IRQ state (as stored in flags) is restored and uidhash_lock released\n+ * upon function exit.\n+ */\n+static inline void free_user(struct user_struct *up, unsigned long flags)\n+{\n+\tuid_hash_remove(up);\n+\tspin_unlock_irqrestore(&uidhash_lock, flags);\n+\tsched_destroy_user(up);\n+\tkey_put(up->uid_keyring);\n+\tkey_put(up->session_keyring);\n+\tkmem_cache_free(uid_cachep, up);\n+}\n+\n+#endif\t/* CONFIG_FAIR_USER_SCHED */\n+\n/*\n* Locate the user_struct for the passed UID.  If found, take a ref on it.  The\n* caller must undo that ref with free_uid().\n@@ -106,15 +308,10 @@ void free_uid(struct user_struct *up)\nreturn;\n\nlocal_irq_save(flags);\n-\tif (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {\n-\t\tuid_hash_remove(up);\n-\t\tspin_unlock_irqrestore(&uidhash_lock, flags);\n-\t\tkey_put(up->uid_keyring);\n-\t\tkey_put(up->session_keyring);\n-\t\tkmem_cache_free(uid_cachep, up);\n-\t} else {\n+\tif (atomic_dec_and_lock(&up->__count, &uidhash_lock))\n+\t\tfree_user(up, flags);\n+\telse\nlocal_irq_restore(flags);\n-\t}\n}\n\nstruct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)\n@@ -122,6 +319,11 @@ struct user_struct * alloc_uid(struct us\nstruct hlist_head *hashent = uidhashentry(ns, uid);\nstruct user_struct *up;\n\n+\t/* Make uid_hash_find() + user_kobject_create() + uid_hash_insert()\n+\t * atomic.\n+\t */\n+\tuids_mutex_lock();\n+\nspin_lock_irq(&uidhash_lock);\nup = uid_hash_find(uid, hashent);\nspin_unlock_irq(&uidhash_lock);\n@@ -150,6 +352,22 @@ struct user_struct * alloc_uid(struct us\nreturn NULL;\n}\n\n+\t\tif (sched_create_user(new) < 0) {\n+\t\t\tkey_put(new->uid_keyring);\n+\t\t\tkey_put(new->session_keyring);\n+\t\t\tkmem_cache_free(uid_cachep, new);\n+\t\t\treturn NULL;\n+\t\t}\n+\n+\t\tif (user_kobject_create(new)) {\n+\t\t\tsched_destroy_user(new);\n+\t\t\tkey_put(new->uid_keyring);\n+\t\t\tkey_put(new->session_keyring);\n+\t\t\tkmem_cache_free(uid_cachep, new);\n+\t\t\tuids_mutex_unlock();\n+\t\t\treturn NULL;\n+\t\t}\n+\n/*\n* Before adding this, check whether we raced\n* on adding the same user already..\n@@ -157,6 +375,11 @@ struct user_struct * alloc_uid(struct us\nspin_lock_irq(&uidhash_lock);\nup = uid_hash_find(uid, hashent);\nif (up) {\n+\t\t\t/* This case is not possible when CONFIG_FAIR_USER_SCHED\n+\t\t\t * is defined, since we serialize alloc_uid() using\n+\t\t\t * uids_mutex. Hence no need to call\n+\t\t\t * sched_destroy_user() or remove_user_sysfs_dir().\n+\t\t\t */\nkey_put(new->uid_keyring);\nkey_put(new->session_keyring);\nkmem_cache_free(uid_cachep, new);\n@@ -167,6 +390,9 @@ struct user_struct * alloc_uid(struct us\nspin_unlock_irq(&uidhash_lock);\n\n}\n+\n+\tuids_mutex_unlock();\n+\nreturn up;\n}\n\n@@ -184,6 +410,7 @@ void switch_uid(struct user_struct *new_\natomic_dec(&old_user->processes);\nswitch_uid_keyring(new_user);\ncurrent->user = new_user;\n+\tsched_switch_user(current);\n\n/*\n* We need to synchronize with __sigqueue_alloc()'),
 ('linux-2.6.23.orig/mm/memory_hotplug.c',
  'linux-2.6.23/mm/memory_hotplug.c',
  "Index: linux-2.6.23/mm/memory_hotplug.c\n===================================================================\n--- linux-2.6.23.orig/mm/memory_hotplug.c\n+++ linux-2.6.23/mm/memory_hotplug.c\n@@ -217,6 +217,10 @@ int online_pages(unsigned long pfn, unsi\nzone->zone_pgdat->node_present_pages += onlined_pages;\n\nsetup_per_zone_pages_min();\n+\tif (onlined_pages) {\n+\t\tkswapd_run(zone_to_nid(zone));\n+\t\tnode_set_state(zone_to_nid(zone), N_HIGH_MEMORY);\n+\t}\n\nif (need_zonelists_rebuild)\nbuild_all_zonelists();\n@@ -271,9 +275,6 @@ int add_memory(int nid, u64 start, u64 s\nif (!pgdat)\nreturn -ENOMEM;\nnew_pgdat = 1;\n-\t\tret = kswapd_run(nid);\n-\t\tif (ret)\n-\t\t\tgoto error;\n}\n\n/* call arch's memory hotadd */"),
 ('linux-2.6.23.orig/mm/page_alloc.c',
  'linux-2.6.23/mm/page_alloc.c',
  'Index: linux-2.6.23/mm/page_alloc.c\n===================================================================\n--- linux-2.6.23.orig/mm/page_alloc.c\n+++ linux-2.6.23/mm/page_alloc.c\n@@ -47,13 +47,21 @@\n#include "internal.h"\n\n/*\n- * MCD - HACK: Find somewhere to initialize this EARLY, or make this\n- * initializer cleaner\n+ * Array of node states.\n*/\n-nodemask_t node_online_map __read_mostly = { { [0] = 1UL } };\n-EXPORT_SYMBOL(node_online_map);\n-nodemask_t node_possible_map __read_mostly = NODE_MASK_ALL;\n-EXPORT_SYMBOL(node_possible_map);\n+nodemask_t node_states[NR_NODE_STATES] __read_mostly = {\n+\t[N_POSSIBLE] = NODE_MASK_ALL,\n+\t[N_ONLINE] = { { [0] = 1UL } },\n+#ifndef CONFIG_NUMA\n+\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },\n+#ifdef CONFIG_HIGHMEM\n+\t[N_HIGH_MEMORY] = { { [0] = 1UL } },\n+#endif\n+\t[N_CPU] = { { [0] = 1UL } },\n+#endif\t/* NUMA */\n+};\n+EXPORT_SYMBOL(node_states);\n+\nunsigned long totalram_pages __read_mostly;\nunsigned long totalreserve_pages __read_mostly;\nlong nr_swap_pages;\n@@ -2077,14 +2085,35 @@ static void build_zonelist_cache(pg_data\n\n#endif\t/* CONFIG_NUMA */\n\n+/* Any regular memory on that node ? */\n+static void check_for_regular_memory(pg_data_t *pgdat)\n+{\n+#ifdef CONFIG_HIGHMEM\n+\tenum zone_type zone_type;\n+\n+\tfor (zone_type = 0; zone_type <= ZONE_NORMAL; zone_type++) {\n+\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n+\t\tif (zone->present_pages)\n+\t\t\tnode_set_state(zone_to_nid(zone), N_NORMAL_MEMORY);\n+\t}\n+#endif\n+}\n+\n/* return values int ....just for stop_machine_run() */\nstatic int __build_all_zonelists(void *dummy)\n{\nint nid;\n\nfor_each_online_node(nid) {\n-\t\tbuild_zonelists(NODE_DATA(nid));\n-\t\tbuild_zonelist_cache(NODE_DATA(nid));\n+\t\tpg_data_t *pgdat = NODE_DATA(nid);\n+\n+\t\tbuild_zonelists(pgdat);\n+\t\tbuild_zonelist_cache(pgdat);\n+\n+\t\t/* Any memory on that node */\n+\t\tif (pgdat->node_present_pages)\n+\t\t\tnode_set_state(nid, N_HIGH_MEMORY);\n+\t\tcheck_for_regular_memory(pgdat);\n}\nreturn 0;\n}\n@@ -2329,6 +2358,9 @@ static struct per_cpu_pageset boot_pages\nstatic int __cpuinit process_zones(int cpu)\n{\nstruct zone *zone, *dzone;\n+\tint node = cpu_to_node(cpu);\n+\n+\tnode_set_state(node, N_CPU);\t/* this node has a cpu */\n\nfor_each_zone(zone) {\n\n@@ -2336,7 +2368,7 @@ static int __cpuinit process_zones(int c\ncontinue;\n\nzone_pcp(zone, cpu) = kmalloc_node(sizeof(struct per_cpu_pageset),\n-\t\t\t\t\t GFP_KERNEL, cpu_to_node(cpu));\n+\t\t\t\t\t GFP_KERNEL, node);\nif (!zone_pcp(zone, cpu))\ngoto bad;\n'),
 ('linux-2.6.23.orig/mm/vmscan.c',
  'linux-2.6.23/mm/vmscan.c',
  'Index: linux-2.6.23/mm/vmscan.c\n===================================================================\n--- linux-2.6.23.orig/mm/vmscan.c\n+++ linux-2.6.23/mm/vmscan.c\n@@ -1847,7 +1847,6 @@ static int __zone_reclaim(struct zone *z\n\nint zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)\n{\n-\tcpumask_t mask;\nint node_id;\n\n/*\n@@ -1884,8 +1883,7 @@ int zone_reclaim(struct zone *zone, gfp_\n* as wide as possible.\n*/\nnode_id = zone_to_nid(zone);\n-\tmask = node_to_cpumask(node_id);\n-\tif (!cpus_empty(mask) && node_id != numa_node_id())\n+\tif (node_state(node_id, N_CPU) && node_id != numa_node_id())\nreturn 0;\nreturn __zone_reclaim(zone, gfp_mask, order);\n}'),
 ('linux-2.6.23.orig/net/unix/af_unix.c',
  'linux-2.6.23/net/unix/af_unix.c',
  'Index: linux-2.6.23/net/unix/af_unix.c\n===================================================================\n--- linux-2.6.23.orig/net/unix/af_unix.c\n+++ linux-2.6.23/net/unix/af_unix.c\n@@ -333,7 +333,7 @@ static void unix_write_space(struct sock\nread_lock(&sk->sk_callback_lock);\nif (unix_writable(sk)) {\nif (sk->sk_sleep && waitqueue_active(sk->sk_sleep))\n-\t\t\twake_up_interruptible(sk->sk_sleep);\n+\t\t\twake_up_interruptible_sync(sk->sk_sleep);\nsk_wake_async(sk, 2, POLL_OUT);\n}\nread_unlock(&sk->sk_callback_lock);\n@@ -1642,7 +1642,7 @@ static int unix_dgram_recvmsg(struct kio\ngoto out_unlock;\n}\n\n-\twake_up_interruptible(&u->peer_wait);\n+\twake_up_interruptible_sync(&u->peer_wait);\n\nif (msg->msg_name)\nunix_copy_addr(msg, skb->sk);')]