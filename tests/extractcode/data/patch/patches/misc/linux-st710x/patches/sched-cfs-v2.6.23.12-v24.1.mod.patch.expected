[
  [
    "linux-2.6.23.orig/Documentation/sched-design-CFS.txt", 
    "linux-2.6.23/Documentation/sched-design-CFS.txt", 
    [
      "Index: linux-2.6.23/Documentation/sched-design-CFS.txt", 
      "===================================================================", 
      "--- linux-2.6.23.orig/Documentation/sched-design-CFS.txt", 
      "+++ linux-2.6.23/Documentation/sched-design-CFS.txt", 
      "@@ -117,3 +117,70 @@ Some implementation details:", 
      "iterators of the scheduling modules are used. The balancing code got", 
      "quite a bit simpler as a result.", 
      "", 
      "+", 
      "+Group scheduler extension to CFS", 
      "+================================", 
      "+", 
      "+Normally the scheduler operates on individual tasks and strives to provide", 
      "+fair CPU time to each task. Sometimes, it may be desirable to group tasks", 
      "+and provide fair CPU time to each such task group. For example, it may", 
      "+be desirable to first provide fair CPU time to each user on the system", 
      "+and then to each task belonging to a user.", 
      "+", 
      "+CONFIG_FAIR_GROUP_SCHED strives to achieve exactly that. It lets", 
      "+SCHED_NORMAL/BATCH tasks be be grouped and divides CPU time fairly among such", 
      "+groups. At present, there are two (mutually exclusive) mechanisms to group", 
      "+tasks for CPU bandwidth control purpose:", 
      "+", 
      "+\t- Based on user id (CONFIG_FAIR_USER_SCHED)", 
      "+\t\tIn this option, tasks are grouped according to their user id.", 
      "+\t- Based on \"cgroup\" pseudo filesystem (CONFIG_FAIR_CGROUP_SCHED)", 
      "+\t\tThis options lets the administrator create arbitrary groups", 
      "+\t\tof tasks, using the \"cgroup\" pseudo filesystem. See", 
      "+\t\tDocumentation/cgroups.txt for more information about this", 
      "+\t\tfilesystem.", 
      "+", 
      "+Only one of these options to group tasks can be chosen and not both.", 
      "+", 
      "+Group scheduler tunables:", 
      "+", 
      "+When CONFIG_FAIR_USER_SCHED is defined, a directory is created in sysfs for", 
      "+each new user and a \"cpu_share\" file is added in that directory.", 
      "+", 
      "+\t# cd /sys/kernel/uids", 
      "+\t# cat 512/cpu_share\t\t# Display user 512's CPU share", 
      "+\t1024", 
      "+\t# echo 2048 > 512/cpu_share\t# Modify user 512's CPU share", 
      "+\t# cat 512/cpu_share\t\t# Display user 512's CPU share", 
      "+\t2048", 
      "+\t#", 
      "+", 
      "+CPU bandwidth between two users are divided in the ratio of their CPU shares.", 
      "+For ex: if you would like user \"root\" to get twice the bandwidth of user", 
      "+\"guest\", then set the cpu_share for both the users such that \"root\"'s", 
      "+cpu_share is twice \"guest\"'s cpu_share", 
      "+", 
      "+", 
      "+When CONFIG_FAIR_CGROUP_SCHED is defined, a \"cpu.shares\" file is created", 
      "+for each group created using the pseudo filesystem. See example steps", 
      "+below to create task groups and modify their CPU share using the \"cgroups\"", 
      "+pseudo filesystem", 
      "+", 
      "+\t# mkdir /dev/cpuctl", 
      "+\t# mount -t cgroup -ocpu none /dev/cpuctl", 
      "+\t# cd /dev/cpuctl", 
      "+", 
      "+\t# mkdir multimedia\t# create \"multimedia\" group of tasks", 
      "+\t# mkdir browser\t\t# create \"browser\" group of tasks", 
      "+", 
      "+\t# #Configure the multimedia group to receive twice the CPU bandwidth", 
      "+\t# #that of browser group", 
      "+", 
      "+\t# echo 2048 > multimedia/cpu.shares", 
      "+\t# echo 1024 > browser/cpu.shares", 
      "+", 
      "+\t# firefox &\t# Launch firefox and move it to \"browser\" group", 
      "+\t# echo <firefox_pid> > browser/tasks", 
      "+", 
      "+\t# #Launch gmplayer (or your favourite movie player)", 
      "+\t# echo <movie_player_pid> > multimedia/tasks"
    ]
  ], 
  [
    "linux-2.6.23.orig/arch/i386/Kconfig", 
    "linux-2.6.23/arch/i386/Kconfig", 
    [
      "Index: linux-2.6.23/arch/i386/Kconfig", 
      "===================================================================", 
      "--- linux-2.6.23.orig/arch/i386/Kconfig", 
      "+++ linux-2.6.23/arch/i386/Kconfig", 
      "@@ -214,6 +214,17 @@ config X86_ES7000", 
      "", 
      "endchoice", 
      "", 
      "+config SCHED_NO_NO_OMIT_FRAME_POINTER", 
      "+\tbool \"Single-depth WCHAN output\"", 
      "+\tdefault y", 
      "+\thelp", 
      "+\t  Calculate simpler /proc/<PID>/wchan values. If this option", 
      "+\t  is disabled then wchan values will recurse back to the", 
      "+\t  caller function. This provides more accurate wchan values,", 
      "+\t  at the expense of slightly more scheduling overhead.", 
      "+", 
      "+\t  If in doubt, say \"Y\".", 
      "+", 
      "config PARAVIRT", 
      "bool \"Paravirtualization support (EXPERIMENTAL)\"", 
      "depends on EXPERIMENTAL"
    ]
  ], 
  [
    "linux-2.6.23.orig/drivers/kvm/kvm.h", 
    "linux-2.6.23/drivers/kvm/kvm.h", 
    [
      "Index: linux-2.6.23/drivers/kvm/kvm.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/drivers/kvm/kvm.h", 
      "+++ linux-2.6.23/drivers/kvm/kvm.h", 
      "@@ -625,6 +625,16 @@ void kvm_mmu_unload(struct kvm_vcpu *vcp", 
      "", 
      "int kvm_hypercall(struct kvm_vcpu *vcpu, struct kvm_run *run);", 
      "", 
      "+static inline void kvm_guest_enter(void)", 
      "+{", 
      "+\tcurrent->flags |= PF_VCPU;", 
      "+}", 
      "+", 
      "+static inline void kvm_guest_exit(void)", 
      "+{", 
      "+\tcurrent->flags &= ~PF_VCPU;", 
      "+}", 
      "+", 
      "static inline int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva,", 
      "u32 error_code)", 
      "{"
    ]
  ], 
  [
    "linux-2.6.23.orig/fs/pipe.c", 
    "linux-2.6.23/fs/pipe.c", 
    [
      "Index: linux-2.6.23/fs/pipe.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/fs/pipe.c", 
      "+++ linux-2.6.23/fs/pipe.c", 
      "@@ -45,8 +45,7 @@ void pipe_wait(struct pipe_inode_info *p", 
      "* Pipes are system-local resources, so sleeping on them", 
      "* is considered a noninteractive wait:", 
      "*/", 
      "-\tprepare_to_wait(&pipe->wait, &wait,", 
      "-\t\t\tTASK_INTERRUPTIBLE | TASK_NONINTERACTIVE);", 
      "+\tprepare_to_wait(&pipe->wait, &wait, TASK_INTERRUPTIBLE);", 
      "if (pipe->inode)", 
      "mutex_unlock(&pipe->inode->i_mutex);", 
      "schedule();", 
      "@@ -383,7 +382,7 @@ redo:", 
      "", 
      "/* Signal writers asynchronously that there is more room. */", 
      "if (do_wakeup) {", 
      "-\t\twake_up_interruptible(&pipe->wait);", 
      "+\t\twake_up_interruptible_sync(&pipe->wait);", 
      "kill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);", 
      "}", 
      "if (ret > 0)", 
      "@@ -556,7 +555,7 @@ redo2:", 
      "out:", 
      "mutex_unlock(&inode->i_mutex);", 
      "if (do_wakeup) {", 
      "-\t\twake_up_interruptible(&pipe->wait);", 
      "+\t\twake_up_interruptible_sync(&pipe->wait);", 
      "kill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);", 
      "}", 
      "if (ret > 0)", 
      "@@ -650,7 +649,7 @@ pipe_release(struct inode *inode, int de", 
      "if (!pipe->readers && !pipe->writers) {", 
      "free_pipe_info(inode);", 
      "} else {", 
      "-\t\twake_up_interruptible(&pipe->wait);", 
      "+\t\twake_up_interruptible_sync(&pipe->wait);", 
      "kill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);", 
      "kill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);", 
      "}"
    ]
  ], 
  [
    "linux-2.6.23.orig/fs/proc/array.c", 
    "linux-2.6.23/fs/proc/array.c", 
    [
      "Index: linux-2.6.23/fs/proc/array.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/fs/proc/array.c", 
      "+++ linux-2.6.23/fs/proc/array.c", 
      "@@ -367,11 +367,18 @@ static cputime_t task_stime(struct task_", 
      "stime = nsec_to_clock_t(p->se.sum_exec_runtime) -", 
      "cputime_to_clock_t(task_utime(p));", 
      "", 
      "-\tp->prev_stime = max(p->prev_stime, clock_t_to_cputime(stime));", 
      "+\tif (stime >= 0)", 
      "+\t\tp->prev_stime = max(p->prev_stime, clock_t_to_cputime(stime));", 
      "+", 
      "return p->prev_stime;", 
      "}", 
      "#endif", 
      "", 
      "+static cputime_t task_gtime(struct task_struct *p)", 
      "+{", 
      "+\treturn p->gtime;", 
      "+}", 
      "+", 
      "static int do_task_stat(struct task_struct *task, char *buffer, int whole)", 
      "{", 
      "unsigned long vsize, eip, esp, wchan = ~0UL;", 
      "@@ -387,6 +394,7 @@ static int do_task_stat(struct task_stru", 
      "unsigned long cmin_flt = 0, cmaj_flt = 0;", 
      "unsigned long  min_flt = 0,  maj_flt = 0;", 
      "cputime_t cutime, cstime, utime, stime;", 
      "+\tcputime_t cgtime, gtime;", 
      "unsigned long rsslim = 0;", 
      "char tcomm[sizeof(task->comm)];", 
      "unsigned long flags;", 
      "@@ -405,6 +413,7 @@ static int do_task_stat(struct task_stru", 
      "sigemptyset(&sigign);", 
      "sigemptyset(&sigcatch);", 
      "cutime = cstime = utime = stime = cputime_zero;", 
      "+\tcgtime = gtime = cputime_zero;", 
      "", 
      "rcu_read_lock();", 
      "if (lock_task_sighand(task, &flags)) {", 
      "@@ -422,6 +431,7 @@ static int do_task_stat(struct task_stru", 
      "cmaj_flt = sig->cmaj_flt;", 
      "cutime = sig->cutime;", 
      "cstime = sig->cstime;", 
      "+\t\tcgtime = sig->cgtime;", 
      "rsslim = sig->rlim[RLIMIT_RSS].rlim_cur;", 
      "", 
      "/* add up live thread stats at the group level */", 
      "@@ -432,6 +442,7 @@ static int do_task_stat(struct task_stru", 
      "maj_flt += t->maj_flt;", 
      "utime = cputime_add(utime, task_utime(t));", 
      "stime = cputime_add(stime, task_stime(t));", 
      "+\t\t\t\tgtime = cputime_add(gtime, task_gtime(t));", 
      "t = next_thread(t);", 
      "} while (t != task);", 
      "", 
      "@@ -439,6 +450,7 @@ static int do_task_stat(struct task_stru", 
      "maj_flt += sig->maj_flt;", 
      "utime = cputime_add(utime, sig->utime);", 
      "stime = cputime_add(stime, sig->stime);", 
      "+\t\t\tgtime = cputime_add(gtime, sig->gtime);", 
      "}", 
      "", 
      "sid = signal_session(sig);", 
      "@@ -456,6 +468,7 @@ static int do_task_stat(struct task_stru", 
      "maj_flt = task->maj_flt;", 
      "utime = task_utime(task);", 
      "stime = task_stime(task);", 
      "+\t\tgtime = task_gtime(task);", 
      "}", 
      "", 
      "/* scale priority and nice values from timeslices to -20..20 */", 
      "@@ -473,7 +486,7 @@ static int do_task_stat(struct task_stru", 
      "", 
      "res = sprintf(buffer, \"%d (%s) %c %d %d %d %d %d %u %lu \\", 
      "%lu %lu %lu %lu %lu %ld %ld %ld %ld %d 0 %llu %lu %ld %lu %lu %lu %lu %lu \\", 
      "-%lu %lu %lu %lu %lu %lu %lu %lu %d %d %u %u %llu\\n\",", 
      "+%lu %lu %lu %lu %lu %lu %lu %lu %d %d %u %u %llu %lu %ld\\n\",", 
      "task->pid,", 
      "tcomm,", 
      "state,", 
      "@@ -518,7 +531,9 @@ static int do_task_stat(struct task_stru", 
      "task_cpu(task),", 
      "task->rt_priority,", 
      "task->policy,", 
      "-\t\t(unsigned long long)delayacct_blkio_ticks(task));", 
      "+\t\t(unsigned long long)delayacct_blkio_ticks(task),", 
      "+\t\tcputime_to_clock_t(gtime),", 
      "+\t\tcputime_to_clock_t(cgtime));", 
      "if (mm)", 
      "mmput(mm);", 
      "return res;"
    ]
  ], 
  [
    "linux-2.6.23.orig/fs/proc/base.c", 
    "linux-2.6.23/fs/proc/base.c", 
    [
      "Index: linux-2.6.23/fs/proc/base.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/fs/proc/base.c", 
      "+++ linux-2.6.23/fs/proc/base.c", 
      "@@ -304,7 +304,7 @@ static int proc_pid_schedstat(struct tas", 
      "return sprintf(buffer, \"%llu %llu %lu\\n\",", 
      "task->sched_info.cpu_time,", 
      "task->sched_info.run_delay,", 
      "-\t\t\ttask->sched_info.pcnt);", 
      "+\t\t\ttask->sched_info.pcount);", 
      "}", 
      "#endif", 
      ""
    ]
  ], 
  [
    "linux-2.6.23.orig/fs/proc/proc_misc.c", 
    "linux-2.6.23/fs/proc/proc_misc.c", 
    [
      "Index: linux-2.6.23/fs/proc/proc_misc.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/fs/proc/proc_misc.c", 
      "+++ linux-2.6.23/fs/proc/proc_misc.c", 
      "@@ -443,6 +443,7 @@ static int show_stat(struct seq_file *p,", 
      "int i;", 
      "unsigned long jif;", 
      "cputime64_t user, nice, system, idle, iowait, irq, softirq, steal;", 
      "+\tcputime64_t guest;", 
      "u64 sum = 0;", 
      "struct timespec boottime;", 
      "unsigned int *per_irq_sum;", 
      "@@ -453,6 +454,7 @@ static int show_stat(struct seq_file *p,", 
      "", 
      "user = nice = system = idle = iowait =", 
      "irq = softirq = steal = cputime64_zero;", 
      "+\tguest = cputime64_zero;", 
      "getboottime(&boottime);", 
      "jif = boottime.tv_sec;", 
      "", 
      "@@ -467,6 +469,7 @@ static int show_stat(struct seq_file *p,", 
      "irq = cputime64_add(irq, kstat_cpu(i).cpustat.irq);", 
      "softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);", 
      "steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);", 
      "+\t\tguest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);", 
      "for (j = 0; j < NR_IRQS; j++) {", 
      "unsigned int temp = kstat_cpu(i).irqs[j];", 
      "sum += temp;", 
      "@@ -474,7 +477,7 @@ static int show_stat(struct seq_file *p,", 
      "}", 
      "}", 
      "", 
      "-\tseq_printf(p, \"cpu  %llu %llu %llu %llu %llu %llu %llu %llu\\n\",", 
      "+\tseq_printf(p, \"cpu  %llu %llu %llu %llu %llu %llu %llu %llu %llu\\n\",", 
      "(unsigned long long)cputime64_to_clock_t(user),", 
      "(unsigned long long)cputime64_to_clock_t(nice),", 
      "(unsigned long long)cputime64_to_clock_t(system),", 
      "@@ -482,7 +485,8 @@ static int show_stat(struct seq_file *p,", 
      "(unsigned long long)cputime64_to_clock_t(iowait),", 
      "(unsigned long long)cputime64_to_clock_t(irq),", 
      "(unsigned long long)cputime64_to_clock_t(softirq),", 
      "-\t\t(unsigned long long)cputime64_to_clock_t(steal));", 
      "+\t\t(unsigned long long)cputime64_to_clock_t(steal),", 
      "+\t\t(unsigned long long)cputime64_to_clock_t(guest));", 
      "for_each_online_cpu(i) {", 
      "", 
      "/* Copy values here to work around gcc-2.95.3, gcc-2.96 */", 
      "@@ -494,7 +498,9 @@ static int show_stat(struct seq_file *p,", 
      "irq = kstat_cpu(i).cpustat.irq;", 
      "softirq = kstat_cpu(i).cpustat.softirq;", 
      "steal = kstat_cpu(i).cpustat.steal;", 
      "-\t\tseq_printf(p, \"cpu%d %llu %llu %llu %llu %llu %llu %llu %llu\\n\",", 
      "+\t\tguest = kstat_cpu(i).cpustat.guest;", 
      "+\t\tseq_printf(p,", 
      "+\t\t\t\"cpu%d %llu %llu %llu %llu %llu %llu %llu %llu %llu\\n\",", 
      "i,", 
      "(unsigned long long)cputime64_to_clock_t(user),", 
      "(unsigned long long)cputime64_to_clock_t(nice),", 
      "@@ -503,7 +509,8 @@ static int show_stat(struct seq_file *p,", 
      "(unsigned long long)cputime64_to_clock_t(iowait),", 
      "(unsigned long long)cputime64_to_clock_t(irq),", 
      "(unsigned long long)cputime64_to_clock_t(softirq),", 
      "-\t\t\t(unsigned long long)cputime64_to_clock_t(steal));", 
      "+\t\t\t(unsigned long long)cputime64_to_clock_t(steal),", 
      "+\t\t\t(unsigned long long)cputime64_to_clock_t(guest));", 
      "}", 
      "seq_printf(p, \"intr %llu\", (unsigned long long)sum);", 
      ""
    ]
  ], 
  [
    "dev/null", 
    "linux-2.6.23/include/linux/cgroup.h", 
    [
      "Index: linux-2.6.23/include/linux/cgroup.h", 
      "===================================================================", 
      "--- dev/null", 
      "+++ linux-2.6.23/include/linux/cgroup.h", 
      "@@ -0,0 +1,12 @@", 
      "+#ifndef _LINUX_CGROUP_H", 
      "+#define _LINUX_CGROUP_H", 
      "+", 
      "+/*", 
      "+ * Control groups are not backported - we use a few compatibility", 
      "+ * defines to be able to use the upstream sched.c as-is:", 
      "+ */", 
      "+#define task_pid_nr(task)\t\t(task)->pid", 
      "+#define task_pid_vnr(task)\t\t(task)->pid", 
      "+#define find_task_by_vpid(pid)\t\tfind_task_by_pid(pid)", 
      "+", 
      "+#endif"
    ]
  ], 
  [
    "linux-2.6.23.orig/include/linux/cpuset.h", 
    "linux-2.6.23/include/linux/cpuset.h", 
    [
      "Index: linux-2.6.23/include/linux/cpuset.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/include/linux/cpuset.h", 
      "+++ linux-2.6.23/include/linux/cpuset.h", 
      "@@ -146,6 +146,11 @@ static inline int cpuset_do_slab_mem_spr", 
      "", 
      "static inline void cpuset_track_online_nodes(void) {}", 
      "", 
      "+static inline cpumask_t cpuset_cpus_allowed_locked(struct task_struct *p)", 
      "+{", 
      "+\treturn cpu_possible_map;", 
      "+}", 
      "+", 
      "#endif /* !CONFIG_CPUSETS */", 
      "", 
      "#endif /* _LINUX_CPUSET_H */"
    ]
  ], 
  [
    "linux-2.6.23.orig/include/linux/kernel.h", 
    "linux-2.6.23/include/linux/kernel.h", 
    [
      "Index: linux-2.6.23/include/linux/kernel.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/include/linux/kernel.h", 
      "+++ linux-2.6.23/include/linux/kernel.h", 
      "@@ -62,6 +62,13 @@ extern const char linux_proc_banner[];", 
      "#define\tKERN_INFO\t\"<6>\"\t/* informational\t\t\t*/", 
      "#define\tKERN_DEBUG\t\"<7>\"\t/* debug-level messages\t\t\t*/", 
      "", 
      "+/*", 
      "+ * Annotation for a \"continued\" line of log printout (only done after a", 
      "+ * line that had no enclosing \\n). Only to be used by core/arch code", 
      "+ * during early bootup (a continued line is not SMP-safe otherwise).", 
      "+ */", 
      "+#define\tKERN_CONT\t\"\"", 
      "+", 
      "extern int console_printk[];", 
      "", 
      "#define console_loglevel (console_printk[0])"
    ]
  ], 
  [
    "linux-2.6.23.orig/include/linux/kernel_stat.h", 
    "linux-2.6.23/include/linux/kernel_stat.h", 
    [
      "Index: linux-2.6.23/include/linux/kernel_stat.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/include/linux/kernel_stat.h", 
      "+++ linux-2.6.23/include/linux/kernel_stat.h", 
      "@@ -23,6 +23,7 @@ struct cpu_usage_stat {", 
      "cputime64_t idle;", 
      "cputime64_t iowait;", 
      "cputime64_t steal;", 
      "+\tcputime64_t guest;", 
      "};", 
      "", 
      "struct kernel_stat {", 
      "@@ -52,7 +53,9 @@ static inline int kstat_irqs(int irq)", 
      "}", 
      "", 
      "extern void account_user_time(struct task_struct *, cputime_t);", 
      "+extern void account_user_time_scaled(struct task_struct *, cputime_t);", 
      "extern void account_system_time(struct task_struct *, int, cputime_t);", 
      "+extern void account_system_time_scaled(struct task_struct *, cputime_t);", 
      "extern void account_steal_time(struct task_struct *, cputime_t);", 
      "", 
      "#endif /* _LINUX_KERNEL_STAT_H */"
    ]
  ], 
  [
    "linux-2.6.23.orig/include/linux/nodemask.h", 
    "linux-2.6.23/include/linux/nodemask.h", 
    [
      "Index: linux-2.6.23/include/linux/nodemask.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/include/linux/nodemask.h", 
      "+++ linux-2.6.23/include/linux/nodemask.h", 
      "@@ -338,31 +338,88 @@ static inline void __nodes_remap(nodemas", 
      "#endif /* MAX_NUMNODES */", 
      "", 
      "/*", 
      "+ * Bitmasks that are kept for all the nodes.", 
      "+ */", 
      "+enum node_states {", 
      "+\tN_POSSIBLE,\t\t/* The node could become online at some point */", 
      "+\tN_ONLINE,\t\t/* The node is online */", 
      "+\tN_NORMAL_MEMORY,\t/* The node has regular memory */", 
      "+#ifdef CONFIG_HIGHMEM", 
      "+\tN_HIGH_MEMORY,\t\t/* The node has regular or high memory */", 
      "+#else", 
      "+\tN_HIGH_MEMORY = N_NORMAL_MEMORY,", 
      "+#endif", 
      "+\tN_CPU,\t\t/* The node has one or more cpus */", 
      "+\tNR_NODE_STATES", 
      "+};", 
      "+", 
      "+/*", 
      "* The following particular system nodemasks and operations", 
      "* on them manage all possible and online nodes.", 
      "*/", 
      "", 
      "-extern nodemask_t node_online_map;", 
      "-extern nodemask_t node_possible_map;", 
      "+extern nodemask_t node_states[NR_NODE_STATES];", 
      "", 
      "#if MAX_NUMNODES > 1", 
      "-#define num_online_nodes()\tnodes_weight(node_online_map)", 
      "-#define num_possible_nodes()\tnodes_weight(node_possible_map)", 
      "-#define node_online(node)\tnode_isset((node), node_online_map)", 
      "-#define node_possible(node)\tnode_isset((node), node_possible_map)", 
      "-#define first_online_node\tfirst_node(node_online_map)", 
      "-#define next_online_node(nid)\tnext_node((nid), node_online_map)", 
      "+static inline int node_state(int node, enum node_states state)", 
      "+{", 
      "+\treturn node_isset(node, node_states[state]);", 
      "+}", 
      "+", 
      "+static inline void node_set_state(int node, enum node_states state)", 
      "+{", 
      "+\t__node_set(node, &node_states[state]);", 
      "+}", 
      "+", 
      "+static inline void node_clear_state(int node, enum node_states state)", 
      "+{", 
      "+\t__node_clear(node, &node_states[state]);", 
      "+}", 
      "+", 
      "+static inline int num_node_state(enum node_states state)", 
      "+{", 
      "+\treturn nodes_weight(node_states[state]);", 
      "+}", 
      "+", 
      "+#define for_each_node_state(__node, __state) \\", 
      "+\tfor_each_node_mask((__node), node_states[__state])", 
      "+", 
      "+#define first_online_node\tfirst_node(node_states[N_ONLINE])", 
      "+#define next_online_node(nid)\tnext_node((nid), node_states[N_ONLINE])", 
      "+", 
      "extern int nr_node_ids;", 
      "#else", 
      "-#define num_online_nodes()\t1", 
      "-#define num_possible_nodes()\t1", 
      "-#define node_online(node)\t((node) == 0)", 
      "-#define node_possible(node)\t((node) == 0)", 
      "+", 
      "+static inline int node_state(int node, enum node_states state)", 
      "+{", 
      "+\treturn node == 0;", 
      "+}", 
      "+", 
      "+static inline void node_set_state(int node, enum node_states state)", 
      "+{", 
      "+}", 
      "+", 
      "+static inline void node_clear_state(int node, enum node_states state)", 
      "+{", 
      "+}", 
      "+", 
      "+static inline int num_node_state(enum node_states state)", 
      "+{", 
      "+\treturn 1;", 
      "+}", 
      "+", 
      "+#define for_each_node_state(node, __state) \\", 
      "+\tfor ( (node) = 0; (node) == 0; (node) = 1)", 
      "+", 
      "#define first_online_node\t0", 
      "#define next_online_node(nid)\t(MAX_NUMNODES)", 
      "#define nr_node_ids\t\t1", 
      "+", 
      "#endif", 
      "", 
      "+#define node_online_map \tnode_states[N_ONLINE]", 
      "+#define node_possible_map \tnode_states[N_POSSIBLE]", 
      "+", 
      "#define any_online_node(mask)\t\t\t\\", 
      "({\t\t\t\t\t\t\\", 
      "int node;\t\t\t\t\\", 
      "@@ -372,10 +429,15 @@ extern int nr_node_ids;", 
      "node;\t\t\t\t\t\\", 
      "})", 
      "", 
      "-#define node_set_online(node)\t   set_bit((node), node_online_map.bits)", 
      "-#define node_set_offline(node)\t   clear_bit((node), node_online_map.bits)", 
      "+#define num_online_nodes()\tnum_node_state(N_ONLINE)", 
      "+#define num_possible_nodes()\tnum_node_state(N_POSSIBLE)", 
      "+#define node_online(node)\tnode_state((node), N_ONLINE)", 
      "+#define node_possible(node)\tnode_state((node), N_POSSIBLE)", 
      "+", 
      "+#define node_set_online(node)\t   node_set_state((node), N_ONLINE)", 
      "+#define node_set_offline(node)\t   node_clear_state((node), N_ONLINE)", 
      "", 
      "-#define for_each_node(node)\t   for_each_node_mask((node), node_possible_map)", 
      "-#define for_each_online_node(node) for_each_node_mask((node), node_online_map)", 
      "+#define for_each_node(node)\t   for_each_node_state(node, N_POSSIBLE)", 
      "+#define for_each_online_node(node) for_each_node_state(node, N_ONLINE)", 
      "", 
      "#endif /* __LINUX_NODEMASK_H */"
    ]
  ], 
  [
    "linux-2.6.23.orig/include/linux/sched.h", 
    "linux-2.6.23/include/linux/sched.h", 
    [
      "Index: linux-2.6.23/include/linux/sched.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/include/linux/sched.h", 
      "+++ linux-2.6.23/include/linux/sched.h", 
      "@@ -3,6 +3,17 @@", 
      "", 
      "#include <linux/auxvec.h>\t/* For AT_VECTOR_SIZE */", 
      "", 
      "+/* backporting helper macro: */", 
      "+#define cpu_sibling_map(cpu) cpu_sibling_map[cpu]", 
      "+", 
      "+/*", 
      "+ *  * Control groups are not backported - we use a few compatibility", 
      "+ *   * defines to be able to use the upstream sched.c as-is:", 
      "+ *    */", 
      "+#define task_pid_nr(task)               (task)->pid", 
      "+#define task_pid_vnr(task)              (task)->pid", 
      "+#define find_task_by_vpid(pid)          find_task_by_pid(pid)", 
      "+", 
      "/*", 
      "* cloning flags:", 
      "*/", 
      "@@ -86,6 +97,7 @@ struct sched_param {", 
      "#include <linux/timer.h>", 
      "#include <linux/hrtimer.h>", 
      "#include <linux/task_io_accounting.h>", 
      "+#include <linux/kobject.h>", 
      "", 
      "#include <asm/processor.h>", 
      "", 
      "@@ -135,6 +147,7 @@ extern unsigned long weighted_cpuload(co", 
      "", 
      "struct seq_file;", 
      "struct cfs_rq;", 
      "+struct task_group;", 
      "#ifdef CONFIG_SCHED_DEBUG", 
      "extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);", 
      "extern void proc_sched_set_task(struct task_struct *p);", 
      "@@ -173,8 +186,7 @@ print_cfs_rq(struct seq_file *m, int cpu", 
      "#define EXIT_ZOMBIE\t\t16", 
      "#define EXIT_DEAD\t\t32", 
      "/* in tsk->state again */", 
      "-#define TASK_NONINTERACTIVE\t64", 
      "-#define TASK_DEAD\t\t128", 
      "+#define TASK_DEAD\t\t64", 
      "", 
      "#define __set_task_state(tsk, state_value)\t\t\\", 
      "do { (tsk)->state = (state_value); } while (0)", 
      "@@ -278,6 +290,10 @@ static inline void touch_all_softlockup_", 
      "", 
      "/* Attach to any functions which should be ignored in wchan output. */", 
      "#define __sched\t\t__attribute__((__section__(\".sched.text\")))", 
      "+", 
      "+/* Linker adds these: start and end of __sched functions */", 
      "+extern char __sched_text_start[], __sched_text_end[];", 
      "+", 
      "/* Is this address in the __sched functions? */", 
      "extern int in_sched_functions(unsigned long addr);", 
      "", 
      "@@ -515,6 +531,8 @@ struct signal_struct {", 
      "* in __exit_signal, except for the group leader.", 
      "*/", 
      "cputime_t utime, stime, cutime, cstime;", 
      "+\tcputime_t gtime;", 
      "+\tcputime_t cgtime;", 
      "unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;", 
      "unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;", 
      "unsigned long inblock, oublock, cinblock, coublock;", 
      "@@ -595,8 +613,23 @@ struct user_struct {", 
      "/* Hash table maintenance information */", 
      "struct hlist_node uidhash_node;", 
      "uid_t uid;", 
      "+", 
      "+#ifdef CONFIG_FAIR_USER_SCHED", 
      "+\tstruct task_group *tg;", 
      "+#ifdef CONFIG_SYSFS", 
      "+\tstruct kset kset;", 
      "+\tstruct subsys_attribute user_attr;", 
      "+\tstruct work_struct work;", 
      "+#endif", 
      "+#endif", 
      "};", 
      "", 
      "+#ifdef CONFIG_FAIR_USER_SCHED", 
      "+extern int uids_kobject_init(void);", 
      "+#else", 
      "+static inline int uids_kobject_init(void) { return 0; }", 
      "+#endif", 
      "+", 
      "extern struct user_struct *find_user(uid_t);", 
      "", 
      "extern struct user_struct root_user;", 
      "@@ -608,13 +641,17 @@ struct reclaim_state;", 
      "#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)", 
      "struct sched_info {", 
      "/* cumulative counters */", 
      "-\tunsigned long pcnt;\t      /* # of times run on this cpu */", 
      "+\tunsigned long pcount;\t      /* # of times run on this cpu */", 
      "unsigned long long cpu_time,  /* time spent on the cpu */", 
      "run_delay; /* time spent waiting on a runqueue */", 
      "", 
      "/* timestamps */", 
      "unsigned long long last_arrival,/* when we last ran on a cpu */", 
      "last_queued;\t/* when we were last queued to run */", 
      "+#ifdef CONFIG_SCHEDSTATS", 
      "+\t/* BKL stats */", 
      "+\tunsigned int bkl_count;", 
      "+#endif", 
      "};", 
      "#endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */", 
      "", 
      "@@ -749,39 +786,38 @@ struct sched_domain {", 
      "", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "/* load_balance() stats */", 
      "-\tunsigned long lb_cnt[CPU_MAX_IDLE_TYPES];", 
      "-\tunsigned long lb_failed[CPU_MAX_IDLE_TYPES];", 
      "-\tunsigned long lb_balanced[CPU_MAX_IDLE_TYPES];", 
      "-\tunsigned long lb_imbalance[CPU_MAX_IDLE_TYPES];", 
      "-\tunsigned long lb_gained[CPU_MAX_IDLE_TYPES];", 
      "-\tunsigned long lb_hot_gained[CPU_MAX_IDLE_TYPES];", 
      "-\tunsigned long lb_nobusyg[CPU_MAX_IDLE_TYPES];", 
      "-\tunsigned long lb_nobusyq[CPU_MAX_IDLE_TYPES];", 
      "+\tunsigned int lb_count[CPU_MAX_IDLE_TYPES];", 
      "+\tunsigned int lb_failed[CPU_MAX_IDLE_TYPES];", 
      "+\tunsigned int lb_balanced[CPU_MAX_IDLE_TYPES];", 
      "+\tunsigned int lb_imbalance[CPU_MAX_IDLE_TYPES];", 
      "+\tunsigned int lb_gained[CPU_MAX_IDLE_TYPES];", 
      "+\tunsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];", 
      "+\tunsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];", 
      "+\tunsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];", 
      "", 
      "/* Active load balancing */", 
      "-\tunsigned long alb_cnt;", 
      "-\tunsigned long alb_failed;", 
      "-\tunsigned long alb_pushed;", 
      "+\tunsigned int alb_count;", 
      "+\tunsigned int alb_failed;", 
      "+\tunsigned int alb_pushed;", 
      "", 
      "/* SD_BALANCE_EXEC stats */", 
      "-\tunsigned long sbe_cnt;", 
      "-\tunsigned long sbe_balanced;", 
      "-\tunsigned long sbe_pushed;", 
      "+\tunsigned int sbe_count;", 
      "+\tunsigned int sbe_balanced;", 
      "+\tunsigned int sbe_pushed;", 
      "", 
      "/* SD_BALANCE_FORK stats */", 
      "-\tunsigned long sbf_cnt;", 
      "-\tunsigned long sbf_balanced;", 
      "-\tunsigned long sbf_pushed;", 
      "+\tunsigned int sbf_count;", 
      "+\tunsigned int sbf_balanced;", 
      "+\tunsigned int sbf_pushed;", 
      "", 
      "/* try_to_wake_up() stats */", 
      "-\tunsigned long ttwu_wake_remote;", 
      "-\tunsigned long ttwu_move_affine;", 
      "-\tunsigned long ttwu_move_balance;", 
      "+\tunsigned int ttwu_wake_remote;", 
      "+\tunsigned int ttwu_move_affine;", 
      "+\tunsigned int ttwu_move_balance;", 
      "#endif", 
      "};", 
      "", 
      "-extern int partition_sched_domains(cpumask_t *partition1,", 
      "-\t\t\t\t    cpumask_t *partition2);", 
      "+extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new);", 
      "", 
      "#endif\t/* CONFIG_SMP */", 
      "", 
      "@@ -853,23 +889,28 @@ struct rq;", 
      "struct sched_domain;", 
      "", 
      "struct sched_class {", 
      "-\tstruct sched_class *next;", 
      "+\tconst struct sched_class *next;", 
      "", 
      "void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);", 
      "void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);", 
      "-\tvoid (*yield_task) (struct rq *rq, struct task_struct *p);", 
      "+\tvoid (*yield_task) (struct rq *rq);", 
      "", 
      "void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);", 
      "", 
      "struct task_struct * (*pick_next_task) (struct rq *rq);", 
      "void (*put_prev_task) (struct rq *rq, struct task_struct *p);", 
      "", 
      "+#ifdef CONFIG_SMP", 
      "unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,", 
      "-\t\t\tstruct rq *busiest,", 
      "-\t\t\tunsigned long max_nr_move, unsigned long max_load_move,", 
      "+\t\t\tstruct rq *busiest, unsigned long max_load_move,", 
      "struct sched_domain *sd, enum cpu_idle_type idle,", 
      "int *all_pinned, int *this_best_prio);", 
      "", 
      "+\tint (*move_one_task) (struct rq *this_rq, int this_cpu,", 
      "+\t\t\t      struct rq *busiest, struct sched_domain *sd,", 
      "+\t\t\t      enum cpu_idle_type idle);", 
      "+#endif", 
      "+", 
      "void (*set_curr_task) (struct rq *rq);", 
      "void (*task_tick) (struct rq *rq, struct task_struct *p);", 
      "void (*task_new) (struct rq *rq, struct task_struct *p);", 
      "@@ -887,31 +928,21 @@ struct load_weight {", 
      "*     4 se->block_start", 
      "*     4 se->run_node", 
      "*     4 se->sleep_start", 
      "- *     4 se->sleep_start_fair", 
      "*     6 se->load.weight", 
      "- *     7 se->delta_fair", 
      "- *    15 se->wait_runtime", 
      "*/", 
      "struct sched_entity {", 
      "-\tlong\t\t\twait_runtime;", 
      "-\tunsigned long\t\tdelta_fair_run;", 
      "-\tunsigned long\t\tdelta_fair_sleep;", 
      "-\tunsigned long\t\tdelta_exec;", 
      "-\ts64\t\t\tfair_key;", 
      "struct load_weight\tload;\t\t/* for load-balancing */", 
      "struct rb_node\t\trun_node;", 
      "unsigned int\t\ton_rq;", 
      "", 
      "u64\t\t\texec_start;", 
      "u64\t\t\tsum_exec_runtime;", 
      "+\tu64\t\t\tvruntime;", 
      "u64\t\t\tprev_sum_exec_runtime;", 
      "-\tu64\t\t\twait_start_fair;", 
      "-\tu64\t\t\tsleep_start_fair;", 
      "", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "u64\t\t\twait_start;", 
      "u64\t\t\twait_max;", 
      "-\ts64\t\t\tsum_wait_runtime;", 
      "", 
      "u64\t\t\tsleep_start;", 
      "u64\t\t\tsleep_max;", 
      "@@ -920,9 +951,25 @@ struct sched_entity {", 
      "u64\t\t\tblock_start;", 
      "u64\t\t\tblock_max;", 
      "u64\t\t\texec_max;", 
      "+\tu64\t\t\tslice_max;", 
      "", 
      "-\tunsigned long\t\twait_runtime_overruns;", 
      "-\tunsigned long\t\twait_runtime_underruns;", 
      "+\tu64\t\t\tnr_migrations;", 
      "+\tu64\t\t\tnr_migrations_cold;", 
      "+\tu64\t\t\tnr_failed_migrations_affine;", 
      "+\tu64\t\t\tnr_failed_migrations_running;", 
      "+\tu64\t\t\tnr_failed_migrations_hot;", 
      "+\tu64\t\t\tnr_forced_migrations;", 
      "+\tu64\t\t\tnr_forced2_migrations;", 
      "+", 
      "+\tu64\t\t\tnr_wakeups;", 
      "+\tu64\t\t\tnr_wakeups_sync;", 
      "+\tu64\t\t\tnr_wakeups_migrate;", 
      "+\tu64\t\t\tnr_wakeups_local;", 
      "+\tu64\t\t\tnr_wakeups_remote;", 
      "+\tu64\t\t\tnr_wakeups_affine;", 
      "+\tu64\t\t\tnr_wakeups_affine_attempts;", 
      "+\tu64\t\t\tnr_wakeups_passive;", 
      "+\tu64\t\t\tnr_wakeups_idle;", 
      "#endif", 
      "", 
      "#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "@@ -951,7 +998,7 @@ struct task_struct {", 
      "", 
      "int prio, static_prio, normal_prio;", 
      "struct list_head run_list;", 
      "-\tstruct sched_class *sched_class;", 
      "+\tconst struct sched_class *sched_class;", 
      "struct sched_entity se;", 
      "", 
      "#ifdef CONFIG_PREEMPT_NOTIFIERS", 
      "@@ -1021,7 +1068,8 @@ struct task_struct {", 
      "int __user *clear_child_tid;\t\t/* CLONE_CHILD_CLEARTID */", 
      "", 
      "unsigned int rt_priority;", 
      "-\tcputime_t utime, stime;", 
      "+\tcputime_t utime, stime, utimescaled, stimescaled;", 
      "+\tcputime_t gtime;", 
      "cputime_t prev_utime, prev_stime;", 
      "unsigned long nvcsw, nivcsw; /* context switch counts */", 
      "struct timespec start_time; \t\t/* monotonic time */", 
      "@@ -1314,6 +1362,7 @@ static inline void put_task_struct(struc", 
      "#define PF_STARTING\t0x00000002\t/* being created */", 
      "#define PF_EXITING\t0x00000004\t/* getting shut down */", 
      "#define PF_EXITPIDONE\t0x00000008\t/* pi exit done on shut down */", 
      "+#define PF_VCPU\t\t0x00000010\t/* I'm a virtual CPU */", 
      "#define PF_FORKNOEXEC\t0x00000040\t/* forked but didn't exec */", 
      "#define PF_SUPERPRIV\t0x00000100\t/* used super-user privileges */", 
      "#define PF_DUMPCORE\t0x00000200\t/* dumped core */", 
      "@@ -1401,15 +1450,26 @@ static inline void idle_task_exit(void)", 
      "", 
      "extern void sched_idle_next(void);", 
      "", 
      "+#ifdef CONFIG_SCHED_DEBUG", 
      "extern unsigned int sysctl_sched_latency;", 
      "extern unsigned int sysctl_sched_min_granularity;", 
      "extern unsigned int sysctl_sched_wakeup_granularity;", 
      "extern unsigned int sysctl_sched_batch_wakeup_granularity;", 
      "-extern unsigned int sysctl_sched_stat_granularity;", 
      "-extern unsigned int sysctl_sched_runtime_limit;", 
      "-extern unsigned int sysctl_sched_compat_yield;", 
      "extern unsigned int sysctl_sched_child_runs_first;", 
      "extern unsigned int sysctl_sched_features;", 
      "+extern unsigned int sysctl_sched_migration_cost;", 
      "+extern unsigned int sysctl_sched_nr_migrate;", 
      "+#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "+extern unsigned int sysctl_sched_min_bal_int_shares;", 
      "+extern unsigned int sysctl_sched_max_bal_int_shares;", 
      "+#endif", 
      "+", 
      "+int sched_nr_latency_handler(struct ctl_table *table, int write,", 
      "+\t\tstruct file *file, void __user *buffer, size_t *length,", 
      "+\t\tloff_t *ppos);", 
      "+#endif", 
      "+", 
      "+extern unsigned int sysctl_sched_compat_yield;", 
      "", 
      "#ifdef CONFIG_RT_MUTEXES", 
      "extern int rt_mutex_getprio(struct task_struct *p);", 
      "@@ -1843,6 +1903,18 @@ extern int sched_mc_power_savings, sched", 
      "", 
      "extern void normalize_rt_tasks(void);", 
      "", 
      "+#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "+", 
      "+extern struct task_group init_task_group;", 
      "+", 
      "+extern struct task_group *sched_create_group(void);", 
      "+extern void sched_destroy_group(struct task_group *tg);", 
      "+extern void sched_move_task(struct task_struct *tsk);", 
      "+extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);", 
      "+extern unsigned long sched_group_shares(struct task_group *tg);", 
      "+", 
      "+#endif", 
      "+", 
      "#ifdef CONFIG_TASK_XACCT", 
      "static inline void add_rchar(struct task_struct *tsk, ssize_t amt)", 
      "{", 
      "@@ -1884,6 +1956,14 @@ static inline void inc_syscw(struct task", 
      "extern void clear_kernel_trace_flag_all_tasks(void);", 
      "extern void set_kernel_trace_flag_all_tasks(void);", 
      "", 
      "+#ifdef CONFIG_SMP", 
      "+void migration_init(void);", 
      "+#else", 
      "+static inline void migration_init(void)", 
      "+{", 
      "+}", 
      "+#endif", 
      "+", 
      "#endif /* __KERNEL__ */", 
      "", 
      "#endif"
    ]
  ], 
  [
    "linux-2.6.23.orig/include/linux/taskstats.h", 
    "linux-2.6.23/include/linux/taskstats.h", 
    [
      "Index: linux-2.6.23/include/linux/taskstats.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/include/linux/taskstats.h", 
      "+++ linux-2.6.23/include/linux/taskstats.h", 
      "@@ -31,7 +31,7 @@", 
      "*/", 
      "", 
      "", 
      "-#define TASKSTATS_VERSION\t5", 
      "+#define TASKSTATS_VERSION\t6", 
      "#define TS_COMM_LEN\t\t32\t/* should be >= TASK_COMM_LEN", 
      "* in linux/sched.h */", 
      "", 
      "@@ -152,6 +152,11 @@ struct taskstats {", 
      "", 
      "__u64  nvcsw;\t\t\t/* voluntary_ctxt_switches */", 
      "__u64  nivcsw;\t\t\t/* nonvoluntary_ctxt_switches */", 
      "+", 
      "+\t/* time accounting for SMT machines */", 
      "+\t__u64\tac_utimescaled;\t\t/* utime scaled on frequency etc */", 
      "+\t__u64\tac_stimescaled;\t\t/* stime scaled on frequency etc */", 
      "+\t__u64\tcpu_scaled_run_real_total; /* scaled cpu_run_real_total */", 
      "};", 
      "", 
      ""
    ]
  ], 
  [
    "linux-2.6.23.orig/include/linux/topology.h", 
    "linux-2.6.23/include/linux/topology.h", 
    [
      "Index: linux-2.6.23/include/linux/topology.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/include/linux/topology.h", 
      "+++ linux-2.6.23/include/linux/topology.h", 
      "@@ -159,15 +159,14 @@", 
      ".imbalance_pct\t\t= 125,\t\t\t\\", 
      ".cache_nice_tries\t= 1,\t\t\t\\", 
      ".busy_idx\t\t= 2,\t\t\t\\", 
      "-\t.idle_idx\t\t= 0,\t\t\t\\", 
      "-\t.newidle_idx\t\t= 0,\t\t\t\\", 
      "+\t.idle_idx\t\t= 1,\t\t\t\\", 
      "+\t.newidle_idx\t\t= 2,\t\t\t\\", 
      ".wake_idx\t\t= 1,\t\t\t\\", 
      ".forkexec_idx\t\t= 1,\t\t\t\\", 
      ".flags\t\t\t= SD_LOAD_BALANCE\t\\", 
      "| SD_BALANCE_NEWIDLE\t\\", 
      "| SD_BALANCE_EXEC\t\\", 
      "| SD_WAKE_AFFINE\t\\", 
      "-\t\t\t\t| SD_WAKE_IDLE\t\t\\", 
      "| BALANCE_FOR_PKG_POWER,\\", 
      ".last_balance\t\t= jiffies,\t\t\\", 
      ".balance_interval\t= 1,\t\t\t\\"
    ]
  ], 
  [
    "linux-2.6.23.orig/init/Kconfig", 
    "linux-2.6.23/init/Kconfig", 
    [
      "Index: linux-2.6.23/init/Kconfig", 
      "===================================================================", 
      "--- linux-2.6.23.orig/init/Kconfig", 
      "+++ linux-2.6.23/init/Kconfig", 
      "@@ -273,6 +273,11 @@ config LOG_BUF_SHIFT", 
      "config CPUSETS", 
      "bool \"Cpuset support\"", 
      "depends on SMP", 
      "+\t#", 
      "+\t# disabled for now - depends on control groups, which", 
      "+\t# are hard to backport:", 
      "+\t#", 
      "+\tdepends on 0", 
      "help", 
      "This option will let you create and manage CPUSETs which", 
      "allow dynamically partitioning a system into sets of CPUs and", 
      "@@ -281,6 +286,27 @@ config CPUSETS", 
      "", 
      "Say N if unsure.", 
      "", 
      "+config FAIR_GROUP_SCHED", 
      "+\tbool \"Fair group CPU scheduler\"", 
      "+\tdefault y", 
      "+\tdepends on EXPERIMENTAL", 
      "+\thelp", 
      "+\t  This feature lets CPU scheduler recognize task groups and control CPU", 
      "+\t  bandwidth allocation to such task groups.", 
      "+", 
      "+choice", 
      "+\tdepends on FAIR_GROUP_SCHED", 
      "+\tprompt \"Basis for grouping tasks\"", 
      "+\tdefault FAIR_USER_SCHED", 
      "+", 
      "+config FAIR_USER_SCHED", 
      "+\tbool \"user id\"", 
      "+\thelp", 
      "+\t  This option will choose userid as the basis for grouping", 
      "+\t  tasks, thus providing equal CPU bandwidth to each user.", 
      "+", 
      "+endchoice", 
      "+", 
      "config SYSFS_DEPRECATED", 
      "bool \"Create deprecated sysfs files\"", 
      "default y"
    ]
  ], 
  [
    "linux-2.6.23.orig/init/main.c", 
    "linux-2.6.23/init/main.c", 
    [
      "Index: linux-2.6.23/init/main.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/init/main.c", 
      "+++ linux-2.6.23/init/main.c", 
      "@@ -794,11 +794,8 @@ __setup(\"nosoftlockup\", nosoftlockup_set", 
      "static void __init do_pre_smp_initcalls(void)", 
      "{", 
      "extern int spawn_ksoftirqd(void);", 
      "-#ifdef CONFIG_SMP", 
      "-\textern int migration_init(void);", 
      "", 
      "migration_init();", 
      "-#endif", 
      "spawn_ksoftirqd();", 
      "if (!nosoftlockup)", 
      "spawn_softlockup_task();"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/delayacct.c", 
    "linux-2.6.23/kernel/delayacct.c", 
    [
      "Index: linux-2.6.23/kernel/delayacct.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/delayacct.c", 
      "+++ linux-2.6.23/kernel/delayacct.c", 
      "@@ -115,11 +115,17 @@ int __delayacct_add_tsk(struct taskstats", 
      "tmp += timespec_to_ns(&ts);", 
      "d->cpu_run_real_total = (tmp < (s64)d->cpu_run_real_total) ? 0 : tmp;", 
      "", 
      "+\ttmp = (s64)d->cpu_scaled_run_real_total;", 
      "+\tcputime_to_timespec(tsk->utimescaled + tsk->stimescaled, &ts);", 
      "+\ttmp += timespec_to_ns(&ts);", 
      "+\td->cpu_scaled_run_real_total =", 
      "+\t\t(tmp < (s64)d->cpu_scaled_run_real_total) ? 0 : tmp;", 
      "+", 
      "/*", 
      "* No locking available for sched_info (and too expensive to add one)", 
      "* Mitigate by taking snapshot of values", 
      "*/", 
      "-\tt1 = tsk->sched_info.pcnt;", 
      "+\tt1 = tsk->sched_info.pcount;", 
      "t2 = tsk->sched_info.run_delay;", 
      "t3 = tsk->sched_info.cpu_time;", 
      ""
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/exit.c", 
    "linux-2.6.23/kernel/exit.c", 
    [
      "Index: linux-2.6.23/kernel/exit.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/exit.c", 
      "+++ linux-2.6.23/kernel/exit.c", 
      "@@ -111,6 +111,7 @@ static void __exit_signal(struct task_st", 
      "*/", 
      "sig->utime = cputime_add(sig->utime, tsk->utime);", 
      "sig->stime = cputime_add(sig->stime, tsk->stime);", 
      "+\t\tsig->gtime = cputime_add(sig->gtime, tsk->gtime);", 
      "sig->min_flt += tsk->min_flt;", 
      "sig->maj_flt += tsk->maj_flt;", 
      "sig->nvcsw += tsk->nvcsw;", 
      "@@ -1245,6 +1246,11 @@ static int wait_task_zombie(struct task_", 
      "cputime_add(p->stime,", 
      "cputime_add(sig->stime,", 
      "sig->cstime)));", 
      "+\t\tpsig->cgtime =", 
      "+\t\t\tcputime_add(psig->cgtime,", 
      "+\t\t\tcputime_add(p->gtime,", 
      "+\t\t\tcputime_add(sig->gtime,", 
      "+\t\t\t\t    sig->cgtime)));", 
      "psig->cmin_flt +=", 
      "p->min_flt + sig->min_flt + sig->cmin_flt;", 
      "psig->cmaj_flt +="
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/fork.c", 
    "linux-2.6.23/kernel/fork.c", 
    [
      "Index: linux-2.6.23/kernel/fork.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/fork.c", 
      "+++ linux-2.6.23/kernel/fork.c", 
      "@@ -878,6 +878,8 @@ static inline int copy_signal(unsigned l", 
      "sig->tty_old_pgrp = NULL;", 
      "", 
      "sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;", 
      "+\tsig->gtime = cputime_zero;", 
      "+\tsig->cgtime = cputime_zero;", 
      "sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;", 
      "sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;", 
      "sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;", 
      "@@ -1048,6 +1050,9 @@ static struct task_struct *copy_process(", 
      "p->stime = cputime_zero;", 
      "p->prev_utime = cputime_zero;", 
      "p->prev_stime = cputime_zero;", 
      "+\tp->gtime = cputime_zero;", 
      "+\tp->utimescaled = cputime_zero;", 
      "+\tp->stimescaled = cputime_zero;", 
      "", 
      "#ifdef CONFIG_TASK_XACCT", 
      "p->rchar = 0;\t\t/* I/O counter: bytes read */"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/ksysfs.c", 
    "linux-2.6.23/kernel/ksysfs.c", 
    [
      "Index: linux-2.6.23/kernel/ksysfs.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/ksysfs.c", 
      "+++ linux-2.6.23/kernel/ksysfs.c", 
      "@@ -14,6 +14,7 @@", 
      "#include <linux/module.h>", 
      "#include <linux/init.h>", 
      "#include <linux/kexec.h>", 
      "+#include <linux/sched.h>", 
      "", 
      "#define KERNEL_ATTR_RO(_name) \\", 
      "static struct subsys_attribute _name##_attr = __ATTR_RO(_name)", 
      "@@ -116,6 +117,13 @@ static int __init ksysfs_init(void)", 
      "&notes_attr);", 
      "}", 
      "", 
      "+\t/*", 
      "+\t * Create \"/sys/kernel/uids\" directory and corresponding root user's", 
      "+\t * directory under it.", 
      "+\t */", 
      "+\tif (!error)", 
      "+\t\terror = uids_kobject_init();", 
      "+", 
      "return error;", 
      "}", 
      ""
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/sched.c", 
    "linux-2.6.23/kernel/sched.c", 
    [
      "Index: linux-2.6.23/kernel/sched.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/sched.c", 
      "+++ linux-2.6.23/kernel/sched.c", 
      "@@ -44,6 +44,7 @@", 
      "#include <linux/vmalloc.h>", 
      "#include <linux/blkdev.h>", 
      "#include <linux/delay.h>", 
      "+#include <linux/pid_namespace.h>", 
      "#include <linux/smp.h>", 
      "#include <linux/threads.h>", 
      "#include <linux/timer.h>", 
      "@@ -62,8 +63,10 @@", 
      "#include <linux/delayacct.h>", 
      "#include <linux/reciprocal_div.h>", 
      "#include <linux/unistd.h>", 
      "+#include <linux/pagemap.h>", 
      "", 
      "#include <asm/tlb.h>", 
      "+#include <asm/irq_regs.h>", 
      "", 
      "/*", 
      "* Scheduler clock - returns current time in nanosec units.", 
      "@@ -72,7 +75,7 @@", 
      "*/", 
      "unsigned long long __attribute__((weak)) sched_clock(void)", 
      "{", 
      "-\treturn (unsigned long long)jiffies * (1000000000 / HZ);", 
      "+\treturn (unsigned long long)jiffies * (NSEC_PER_SEC / HZ);", 
      "}", 
      "", 
      "/*", 
      "@@ -96,8 +99,8 @@ unsigned long long __attribute__((weak))", 
      "/*", 
      "* Some helpers for converting nanosecond timing to jiffy resolution", 
      "*/", 
      "-#define NS_TO_JIFFIES(TIME)\t((TIME) / (1000000000 / HZ))", 
      "-#define JIFFIES_TO_NS(TIME)\t((TIME) * (1000000000 / HZ))", 
      "+#define NS_TO_JIFFIES(TIME)\t((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))", 
      "+#define JIFFIES_TO_NS(TIME)\t((TIME) * (NSEC_PER_SEC / HZ))", 
      "", 
      "#define NICE_0_LOAD\t\tSCHED_LOAD_SCALE", 
      "#define NICE_0_SHIFT\t\tSCHED_LOAD_SHIFT", 
      "@@ -105,11 +108,9 @@ unsigned long long __attribute__((weak))", 
      "/*", 
      "* These are the 'tuning knobs' of the scheduler:", 
      "*", 
      "- * Minimum timeslice is 5 msecs (or 1 jiffy, whichever is larger),", 
      "- * default timeslice is 100 msecs, maximum timeslice is 800 msecs.", 
      "+ * default timeslice is 100 msecs (used only for SCHED_RR tasks).", 
      "* Timeslices get refilled after they expire.", 
      "*/", 
      "-#define MIN_TIMESLICE\t\tmax(5 * HZ / 1000, 1)", 
      "#define DEF_TIMESLICE\t\t(100 * HZ / 1000)", 
      "", 
      "#ifdef CONFIG_SMP", 
      "@@ -133,24 +134,6 @@ static inline void sg_inc_cpu_power(stru", 
      "}", 
      "#endif", 
      "", 
      "-#define SCALE_PRIO(x, prio) \\", 
      "-\tmax(x * (MAX_PRIO - prio) / (MAX_USER_PRIO / 2), MIN_TIMESLICE)", 
      "-", 
      "-/*", 
      "- * static_prio_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]", 
      "- * to time slice values: [800ms ... 100ms ... 5ms]", 
      "- */", 
      "-static unsigned int static_prio_timeslice(int static_prio)", 
      "-{", 
      "-\tif (static_prio == NICE_TO_PRIO(19))", 
      "-\t\treturn 1;", 
      "-", 
      "-\tif (static_prio < NICE_TO_PRIO(0))", 
      "-\t\treturn SCALE_PRIO(DEF_TIMESLICE * 4, static_prio);", 
      "-\telse", 
      "-\t\treturn SCALE_PRIO(DEF_TIMESLICE, static_prio);", 
      "-}", 
      "-", 
      "static inline int rt_policy(int policy)", 
      "{", 
      "if (unlikely(policy == SCHED_FIFO) || unlikely(policy == SCHED_RR))", 
      "@@ -171,41 +154,111 @@ struct rt_prio_array {", 
      "struct list_head queue[MAX_RT_PRIO];", 
      "};", 
      "", 
      "-struct load_stat {", 
      "-\tstruct load_weight load;", 
      "-\tu64 load_update_start, load_update_last;", 
      "-\tunsigned long delta_fair, delta_exec, delta_stat;", 
      "+#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "+", 
      "+#include <linux/cgroup.h>", 
      "+", 
      "+struct cfs_rq;", 
      "+", 
      "+/* task group related information */", 
      "+struct task_group {", 
      "+#ifdef CONFIG_FAIR_CGROUP_SCHED", 
      "+\tstruct cgroup_subsys_state css;", 
      "+#endif", 
      "+\t/* schedulable entities of this group on each cpu */", 
      "+\tstruct sched_entity **se;", 
      "+\t/* runqueue \"owned\" by this group on each cpu */", 
      "+\tstruct cfs_rq **cfs_rq;", 
      "+\tunsigned long shares;", 
      "+\t/* spinlock to serialize modification to shares */", 
      "+\tspinlock_t lock;", 
      "+\tstruct rcu_head rcu;", 
      "+};", 
      "+", 
      "+/* Default task group's sched entity on each cpu */", 
      "+static DEFINE_PER_CPU(struct sched_entity, init_sched_entity);", 
      "+/* Default task group's cfs_rq on each cpu */", 
      "+static DEFINE_PER_CPU(struct cfs_rq, init_cfs_rq) ____cacheline_aligned_in_smp;", 
      "+", 
      "+static struct sched_entity *init_sched_entity_p[NR_CPUS];", 
      "+static struct cfs_rq *init_cfs_rq_p[NR_CPUS];", 
      "+", 
      "+/* Default task group.", 
      "+ *\tEvery task in system belong to this group at bootup.", 
      "+ */", 
      "+struct task_group init_task_group = {", 
      "+\t.se     = init_sched_entity_p,", 
      "+\t.cfs_rq = init_cfs_rq_p,", 
      "};", 
      "", 
      "+#ifdef CONFIG_FAIR_USER_SCHED", 
      "+# define INIT_TASK_GRP_LOAD\t2*NICE_0_LOAD", 
      "+#else", 
      "+# define INIT_TASK_GRP_LOAD\tNICE_0_LOAD", 
      "+#endif", 
      "+", 
      "+static int init_task_group_load = INIT_TASK_GRP_LOAD;", 
      "+", 
      "+/* return group to which a task belongs */", 
      "+static inline struct task_group *task_group(struct task_struct *p)", 
      "+{", 
      "+\tstruct task_group *tg;", 
      "+", 
      "+#ifdef CONFIG_FAIR_USER_SCHED", 
      "+\ttg = p->user->tg;", 
      "+#elif defined(CONFIG_FAIR_CGROUP_SCHED)", 
      "+\ttg = container_of(task_subsys_state(p, cpu_cgroup_subsys_id),", 
      "+\t\t\t\tstruct task_group, css);", 
      "+#else", 
      "+\ttg = &init_task_group;", 
      "+#endif", 
      "+\treturn tg;", 
      "+}", 
      "+", 
      "+/* Change a task's cfs_rq and parent entity if it moves across CPUs/groups */", 
      "+static inline void set_task_cfs_rq(struct task_struct *p, unsigned int cpu)", 
      "+{", 
      "+\tp->se.cfs_rq = task_group(p)->cfs_rq[cpu];", 
      "+\tp->se.parent = task_group(p)->se[cpu];", 
      "+}", 
      "+", 
      "+#else", 
      "+", 
      "+static inline void set_task_cfs_rq(struct task_struct *p, unsigned int cpu) { }", 
      "+", 
      "+#endif\t/* CONFIG_FAIR_GROUP_SCHED */", 
      "+", 
      "/* CFS-related fields in a runqueue */", 
      "struct cfs_rq {", 
      "struct load_weight load;", 
      "unsigned long nr_running;", 
      "", 
      "-\ts64 fair_clock;", 
      "u64 exec_clock;", 
      "-\ts64 wait_runtime;", 
      "-\tu64 sleeper_bonus;", 
      "-\tunsigned long wait_runtime_overruns, wait_runtime_underruns;", 
      "+\tu64 min_vruntime;", 
      "", 
      "struct rb_root tasks_timeline;", 
      "struct rb_node *rb_leftmost;", 
      "struct rb_node *rb_load_balance_curr;", 
      "-#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "/* 'curr' points to currently running entity on this cfs_rq.", 
      "* It is set to NULL otherwise (i.e when none are currently running).", 
      "*/", 
      "struct sched_entity *curr;", 
      "+", 
      "+\tunsigned long nr_spread_over;", 
      "+", 
      "+#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "struct rq *rq;\t/* cpu runqueue to which this cfs_rq is attached */", 
      "", 
      "-\t/* leaf cfs_rqs are those that hold tasks (lowest schedulable entity in", 
      "+\t/*", 
      "+\t * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in", 
      "* a hierarchy). Non-leaf lrqs hold other higher schedulable entities", 
      "* (like users, containers etc.)", 
      "*", 
      "* leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This", 
      "* list is used during load balance.", 
      "*/", 
      "-\tstruct list_head leaf_cfs_rq_list; /* Better name : task_cfs_rq_list? */", 
      "+\tstruct list_head leaf_cfs_rq_list;", 
      "+\tstruct task_group *tg;\t/* group that \"owns\" this runqueue */", 
      "#endif", 
      "};", 
      "", 
      "@@ -224,7 +277,8 @@ struct rt_rq {", 
      "* acquire operations must be ordered by ascending &runqueue.", 
      "*/", 
      "struct rq {", 
      "-\tspinlock_t lock;\t/* runqueue lock */", 
      "+\t/* runqueue lock: */", 
      "+\tspinlock_t lock;", 
      "", 
      "/*", 
      "* nr_running and cpu_load should be in the same cacheline because", 
      "@@ -237,15 +291,17 @@ struct rq {", 
      "#ifdef CONFIG_NO_HZ", 
      "unsigned char in_nohz_recently;", 
      "#endif", 
      "-\tstruct load_stat ls;\t/* capture load from *all* tasks on this cpu */", 
      "+\t/* capture load from *all* tasks on this cpu: */", 
      "+\tstruct load_weight load;", 
      "unsigned long nr_load_updates;", 
      "u64 nr_switches;", 
      "", 
      "struct cfs_rq cfs;", 
      "#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "-\tstruct list_head leaf_cfs_rq_list; /* list of leaf cfs_rq on this cpu */", 
      "+\t/* list of leaf cfs_rq on this cpu: */", 
      "+\tstruct list_head leaf_cfs_rq_list;", 
      "#endif", 
      "-\tstruct rt_rq  rt;", 
      "+\tstruct rt_rq rt;", 
      "", 
      "/*", 
      "* This is part of a global counter where only the total sum", 
      "@@ -275,7 +331,8 @@ struct rq {", 
      "/* For active balancing */", 
      "int active_balance;", 
      "int push_cpu;", 
      "-\tint cpu;\t\t/* cpu of this runqueue */", 
      "+\t/* cpu of this runqueue: */", 
      "+\tint cpu;", 
      "", 
      "struct task_struct *migration_thread;", 
      "struct list_head migration_queue;", 
      "@@ -286,19 +343,22 @@ struct rq {", 
      "struct sched_info rq_sched_info;", 
      "", 
      "/* sys_sched_yield() stats */", 
      "-\tunsigned long yld_exp_empty;", 
      "-\tunsigned long yld_act_empty;", 
      "-\tunsigned long yld_both_empty;", 
      "-\tunsigned long yld_cnt;", 
      "+\tunsigned int yld_exp_empty;", 
      "+\tunsigned int yld_act_empty;", 
      "+\tunsigned int yld_both_empty;", 
      "+\tunsigned int yld_count;", 
      "", 
      "/* schedule() stats */", 
      "-\tunsigned long sched_switch;", 
      "-\tunsigned long sched_cnt;", 
      "-\tunsigned long sched_goidle;", 
      "+\tunsigned int sched_switch;", 
      "+\tunsigned int sched_count;", 
      "+\tunsigned int sched_goidle;", 
      "", 
      "/* try_to_wake_up() stats */", 
      "-\tunsigned long ttwu_cnt;", 
      "-\tunsigned long ttwu_local;", 
      "+\tunsigned int ttwu_count;", 
      "+\tunsigned int ttwu_local;", 
      "+", 
      "+\t/* BKL stats */", 
      "+\tunsigned int bkl_count;", 
      "#endif", 
      "struct lock_class_key rq_lock_key;", 
      "};", 
      "@@ -383,6 +443,41 @@ static void update_rq_clock(struct rq *r", 
      "#define cpu_curr(cpu)\t\t(cpu_rq(cpu)->curr)", 
      "", 
      "/*", 
      "+ * Tunables that become constants when CONFIG_SCHED_DEBUG is off:", 
      "+ */", 
      "+#ifdef CONFIG_SCHED_DEBUG", 
      "+# define const_debug __read_mostly", 
      "+#else", 
      "+# define const_debug static const", 
      "+#endif", 
      "+", 
      "+/*", 
      "+ * Debugging: various feature bits", 
      "+ */", 
      "+enum {", 
      "+\tSCHED_FEAT_NEW_FAIR_SLEEPERS\t= 1,", 
      "+\tSCHED_FEAT_WAKEUP_PREEMPT\t= 2,", 
      "+\tSCHED_FEAT_START_DEBIT\t\t= 4,", 
      "+\tSCHED_FEAT_TREE_AVG\t\t= 8,", 
      "+\tSCHED_FEAT_APPROX_AVG\t\t= 16,", 
      "+};", 
      "+", 
      "+const_debug unsigned int sysctl_sched_features =", 
      "+\t\tSCHED_FEAT_NEW_FAIR_SLEEPERS\t* 1 |", 
      "+\t\tSCHED_FEAT_WAKEUP_PREEMPT\t* 1 |", 
      "+\t\tSCHED_FEAT_START_DEBIT\t\t* 1 |", 
      "+\t\tSCHED_FEAT_TREE_AVG\t\t* 0 |", 
      "+\t\tSCHED_FEAT_APPROX_AVG\t\t* 0;", 
      "+", 
      "+#define sched_feat(x) (sysctl_sched_features & SCHED_FEAT_##x)", 
      "+", 
      "+/*", 
      "+ * Number of tasks to iterate in a single balance run.", 
      "+ * Limited because this is done with IRQs disabled.", 
      "+ */", 
      "+const_debug unsigned int sysctl_sched_nr_migrate = 32;", 
      "+", 
      "+/*", 
      "* For kernel-internal use: high-speed (but slightly incorrect) per-cpu", 
      "* clock constructed from sched_clock():", 
      "*/", 
      "@@ -394,24 +489,18 @@ unsigned long long cpu_clock(int cpu)", 
      "", 
      "local_irq_save(flags);", 
      "rq = cpu_rq(cpu);", 
      "-\tupdate_rq_clock(rq);", 
      "+\t/*", 
      "+\t * Only call sched_clock() if the scheduler has already been", 
      "+\t * initialized (some code might call cpu_clock() very early):", 
      "+\t */", 
      "+\tif (rq->idle)", 
      "+\t\tupdate_rq_clock(rq);", 
      "now = rq->clock;", 
      "local_irq_restore(flags);", 
      "", 
      "return now;", 
      "}", 
      "-", 
      "-#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "-/* Change a task's ->cfs_rq if it moves across CPUs */", 
      "-static inline void set_task_cfs_rq(struct task_struct *p)", 
      "-{", 
      "-\tp->se.cfs_rq = &task_rq(p)->cfs;", 
      "-}", 
      "-#else", 
      "-static inline void set_task_cfs_rq(struct task_struct *p)", 
      "-{", 
      "-}", 
      "-#endif", 
      "+EXPORT_SYMBOL_GPL(cpu_clock);", 
      "", 
      "#ifndef prepare_arch_switch", 
      "# define prepare_arch_switch(next)\tdo { } while (0)", 
      "@@ -420,10 +509,15 @@ static inline void set_task_cfs_rq(struc", 
      "# define finish_arch_switch(prev)\tdo { } while (0)", 
      "#endif", 
      "", 
      "+static inline int task_current(struct rq *rq, struct task_struct *p)", 
      "+{", 
      "+\treturn rq->curr == p;", 
      "+}", 
      "+", 
      "#ifndef __ARCH_WANT_UNLOCKED_CTXSW", 
      "static inline int task_running(struct rq *rq, struct task_struct *p)", 
      "{", 
      "-\treturn rq->curr == p;", 
      "+\treturn task_current(rq, p);", 
      "}", 
      "", 
      "static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)", 
      "@@ -452,7 +546,7 @@ static inline int task_running(struct rq", 
      "#ifdef CONFIG_SMP", 
      "return p->oncpu;", 
      "#else", 
      "-\treturn rq->curr == p;", 
      "+\treturn task_current(rq, p);", 
      "#endif", 
      "}", 
      "", 
      "@@ -497,21 +591,18 @@ static inline void finish_lock_switch(st", 
      "static inline struct rq *__task_rq_lock(struct task_struct *p)", 
      "__acquires(rq->lock)", 
      "{", 
      "-\tstruct rq *rq;", 
      "-", 
      "-repeat_lock_task:", 
      "-\trq = task_rq(p);", 
      "-\tspin_lock(&rq->lock);", 
      "-\tif (unlikely(rq != task_rq(p))) {", 
      "+\tfor (;;) {", 
      "+\t\tstruct rq *rq = task_rq(p);", 
      "+\t\tspin_lock(&rq->lock);", 
      "+\t\tif (likely(rq == task_rq(p)))", 
      "+\t\t\treturn rq;", 
      "spin_unlock(&rq->lock);", 
      "-\t\tgoto repeat_lock_task;", 
      "}", 
      "-\treturn rq;", 
      "}", 
      "", 
      "/*", 
      "* task_rq_lock - lock the runqueue a given task resides on and disable", 
      "- * interrupts.  Note the ordering: we can safely lookup the task_rq without", 
      "+ * interrupts. Note the ordering: we can safely lookup the task_rq without", 
      "* explicitly disabling preemption.", 
      "*/", 
      "static struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)", 
      "@@ -519,18 +610,17 @@ static struct rq *task_rq_lock(struct ta", 
      "{", 
      "struct rq *rq;", 
      "", 
      "-repeat_lock_task:", 
      "-\tlocal_irq_save(*flags);", 
      "-\trq = task_rq(p);", 
      "-\tspin_lock(&rq->lock);", 
      "-\tif (unlikely(rq != task_rq(p))) {", 
      "+\tfor (;;) {", 
      "+\t\tlocal_irq_save(*flags);", 
      "+\t\trq = task_rq(p);", 
      "+\t\tspin_lock(&rq->lock);", 
      "+\t\tif (likely(rq == task_rq(p)))", 
      "+\t\t\treturn rq;", 
      "spin_unlock_irqrestore(&rq->lock, *flags);", 
      "-\t\tgoto repeat_lock_task;", 
      "}", 
      "-\treturn rq;", 
      "}", 
      "", 
      "-static inline void __task_rq_unlock(struct rq *rq)", 
      "+static void __task_rq_unlock(struct rq *rq)", 
      "__releases(rq->lock)", 
      "{", 
      "spin_unlock(&rq->lock);", 
      "@@ -545,7 +635,7 @@ static inline void task_rq_unlock(struct", 
      "/*", 
      "* this_rq_lock - lock this runqueue and disable interrupts.", 
      "*/", 
      "-static inline struct rq *this_rq_lock(void)", 
      "+static struct rq *this_rq_lock(void)", 
      "__acquires(rq->lock)", 
      "{", 
      "struct rq *rq;", 
      "@@ -579,6 +669,7 @@ void sched_clock_idle_wakeup_event(u64 d", 
      "struct rq *rq = cpu_rq(smp_processor_id());", 
      "u64 now = sched_clock();", 
      "", 
      "+\ttouch_softlockup_watchdog();", 
      "rq->idle_clock += delta_ns;", 
      "/*", 
      "* Override the previous timestamp and ignore all", 
      "@@ -645,19 +736,6 @@ static inline void resched_task(struct t", 
      "}", 
      "#endif", 
      "", 
      "-static u64 div64_likely32(u64 divident, unsigned long divisor)", 
      "-{", 
      "-#if BITS_PER_LONG == 32", 
      "-\tif (likely(divident <= 0xffffffffULL))", 
      "-\t\treturn (u32)divident / divisor;", 
      "-\tdo_div(divident, divisor);", 
      "-", 
      "-\treturn divident;", 
      "-#else", 
      "-\treturn divident / divisor;", 
      "-#endif", 
      "-}", 
      "-", 
      "#if BITS_PER_LONG == 32", 
      "# define WMULT_CONST\t(~0UL)", 
      "#else", 
      "@@ -699,23 +777,21 @@ calc_delta_fair(unsigned long delta_exec", 
      "return calc_delta_mine(delta_exec, NICE_0_LOAD, lw);", 
      "}", 
      "", 
      "-static void update_load_add(struct load_weight *lw, unsigned long inc)", 
      "+static inline void update_load_add(struct load_weight *lw, unsigned long inc)", 
      "{", 
      "lw->weight += inc;", 
      "-\tlw->inv_weight = 0;", 
      "}", 
      "", 
      "-static void update_load_sub(struct load_weight *lw, unsigned long dec)", 
      "+static inline void update_load_sub(struct load_weight *lw, unsigned long dec)", 
      "{", 
      "lw->weight -= dec;", 
      "-\tlw->inv_weight = 0;", 
      "}", 
      "", 
      "/*", 
      "* To aid in avoiding the subversion of \"niceness\" due to uneven distribution", 
      "* of tasks with abnormal \"nice\" values across CPUs the contribution that", 
      "* each task makes to its run queue's load is weighted according to its", 
      "- * scheduling class and \"nice\" value.  For SCHED_NORMAL tasks this is just a", 
      "+ * scheduling class and \"nice\" value. For SCHED_NORMAL tasks this is just a", 
      "* scaled version of the new time slice allocation that they receive on time", 
      "* slice expiry etc.", 
      "*/", 
      "@@ -777,36 +853,40 @@ struct rq_iterator {", 
      "struct task_struct *(*next)(void *);", 
      "};", 
      "", 
      "-static int balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "-\t\t      unsigned long max_nr_move, unsigned long max_load_move,", 
      "-\t\t      struct sched_domain *sd, enum cpu_idle_type idle,", 
      "-\t\t      int *all_pinned, unsigned long *load_moved,", 
      "-\t\t      int *this_best_prio, struct rq_iterator *iterator);", 
      "+#ifdef CONFIG_SMP", 
      "+static unsigned long", 
      "+balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "+\t      unsigned long max_load_move, struct sched_domain *sd,", 
      "+\t      enum cpu_idle_type idle, int *all_pinned,", 
      "+\t      int *this_best_prio, struct rq_iterator *iterator);", 
      "+", 
      "+static int", 
      "+iter_move_one_task(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "+\t\t   struct sched_domain *sd, enum cpu_idle_type idle,", 
      "+\t\t   struct rq_iterator *iterator);", 
      "+#endif", 
      "+", 
      "+#ifdef CONFIG_CGROUP_CPUACCT", 
      "+static void cpuacct_charge(struct task_struct *tsk, u64 cputime);", 
      "+#else", 
      "+static inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}", 
      "+#endif", 
      "", 
      "#include \"sched_stats.h\"", 
      "-#include \"sched_rt.c\"", 
      "-#include \"sched_fair.c\"", 
      "#include \"sched_idletask.c\"", 
      "+#include \"sched_fair.c\"", 
      "+#include \"sched_rt.c\"", 
      "#ifdef CONFIG_SCHED_DEBUG", 
      "# include \"sched_debug.c\"", 
      "#endif", 
      "", 
      "#define sched_class_highest (&rt_sched_class)", 
      "", 
      "-static void __update_curr_load(struct rq *rq, struct load_stat *ls)", 
      "-{", 
      "-\tif (rq->curr != rq->idle && ls->load.weight) {", 
      "-\t\tls->delta_exec += ls->delta_stat;", 
      "-\t\tls->delta_fair += calc_delta_fair(ls->delta_stat, &ls->load);", 
      "-\t\tls->delta_stat = 0;", 
      "-\t}", 
      "-}", 
      "-", 
      "/*", 
      "* Update delta_exec, delta_fair fields for rq.", 
      "*", 
      "* delta_fair clock advances at a rate inversely proportional to", 
      "- * total load (rq->ls.load.weight) on the runqueue, while", 
      "+ * total load (rq->load.weight) on the runqueue, while", 
      "* delta_exec advances at the same rate as wall-clock (provided", 
      "* cpu is not idle).", 
      "*", 
      "@@ -814,35 +894,17 @@ static void __update_curr_load(struct rq", 
      "* runqueue over any given interval. This (smoothened) load is used", 
      "* during load balance.", 
      "*", 
      "- * This function is called /before/ updating rq->ls.load", 
      "+ * This function is called /before/ updating rq->load", 
      "* and when switching tasks.", 
      "*/", 
      "-static void update_curr_load(struct rq *rq)", 
      "-{", 
      "-\tstruct load_stat *ls = &rq->ls;", 
      "-\tu64 start;", 
      "-", 
      "-\tstart = ls->load_update_start;", 
      "-\tls->load_update_start = rq->clock;", 
      "-\tls->delta_stat += rq->clock - start;", 
      "-\t/*", 
      "-\t * Stagger updates to ls->delta_fair. Very frequent updates", 
      "-\t * can be expensive.", 
      "-\t */", 
      "-\tif (ls->delta_stat >= sysctl_sched_stat_granularity)", 
      "-\t\t__update_curr_load(rq, ls);", 
      "-}", 
      "-", 
      "static inline void inc_load(struct rq *rq, const struct task_struct *p)", 
      "{", 
      "-\tupdate_curr_load(rq);", 
      "-\tupdate_load_add(&rq->ls.load, p->se.load.weight);", 
      "+\tupdate_load_add(&rq->load, p->se.load.weight);", 
      "}", 
      "", 
      "static inline void dec_load(struct rq *rq, const struct task_struct *p)", 
      "{", 
      "-\tupdate_curr_load(rq);", 
      "-\tupdate_load_sub(&rq->ls.load, p->se.load.weight);", 
      "+\tupdate_load_sub(&rq->load, p->se.load.weight);", 
      "}", 
      "", 
      "static void inc_nr_running(struct task_struct *p, struct rq *rq)", 
      "@@ -859,8 +921,6 @@ static void dec_nr_running(struct task_s", 
      "", 
      "static void set_load_weight(struct task_struct *p)", 
      "{", 
      "-\tp->se.wait_runtime = 0;", 
      "-", 
      "if (task_has_rt_policy(p)) {", 
      "p->se.load.weight = prio_to_weight[0] * 2;", 
      "p->se.load.inv_weight = prio_to_wmult[0] >> 1;", 
      "@@ -952,20 +1012,6 @@ static void activate_task(struct rq *rq,", 
      "}", 
      "", 
      "/*", 
      "- * activate_idle_task - move idle task to the _front_ of runqueue.", 
      "- */", 
      "-static inline void activate_idle_task(struct task_struct *p, struct rq *rq)", 
      "-{", 
      "-\tupdate_rq_clock(rq);", 
      "-", 
      "-\tif (p->state == TASK_UNINTERRUPTIBLE)", 
      "-\t\trq->nr_uninterruptible--;", 
      "-", 
      "-\tenqueue_task(rq, p, 0);", 
      "-\tinc_nr_running(p, rq);", 
      "-}", 
      "-", 
      "-/*", 
      "* deactivate_task - remove a task from the runqueue.", 
      "*/", 
      "static void deactivate_task(struct rq *rq, struct task_struct *p, int sleep)", 
      "@@ -989,32 +1035,56 @@ inline int task_curr(const struct task_s", 
      "/* Used instead of source_load when we know the type == 0 */", 
      "unsigned long weighted_cpuload(const int cpu)", 
      "{", 
      "-\treturn cpu_rq(cpu)->ls.load.weight;", 
      "+\treturn cpu_rq(cpu)->load.weight;", 
      "}", 
      "", 
      "static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)", 
      "{", 
      "+\tset_task_cfs_rq(p, cpu);", 
      "#ifdef CONFIG_SMP", 
      "+\t/*", 
      "+\t * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be", 
      "+\t * successfuly executed on another CPU. We must ensure that updates of", 
      "+\t * per-task data have been completed by this moment.", 
      "+\t */", 
      "+\tsmp_wmb();", 
      "task_thread_info(p)->cpu = cpu;", 
      "-\tset_task_cfs_rq(p);", 
      "#endif", 
      "}", 
      "", 
      "#ifdef CONFIG_SMP", 
      "", 
      "+/*", 
      "+ * Is this task likely cache-hot:", 
      "+ */", 
      "+static inline int", 
      "+task_hot(struct task_struct *p, u64 now, struct sched_domain *sd)", 
      "+{", 
      "+\ts64 delta;", 
      "+", 
      "+\tif (p->sched_class != &fair_sched_class)", 
      "+\t\treturn 0;", 
      "+", 
      "+\tif (sysctl_sched_migration_cost == -1)", 
      "+\t\treturn 1;", 
      "+\tif (sysctl_sched_migration_cost == 0)", 
      "+\t\treturn 0;", 
      "+", 
      "+\tdelta = now - p->se.exec_start;", 
      "+", 
      "+\treturn delta < (s64)sysctl_sched_migration_cost;", 
      "+}", 
      "+", 
      "+", 
      "void set_task_cpu(struct task_struct *p, unsigned int new_cpu)", 
      "{", 
      "int old_cpu = task_cpu(p);", 
      "struct rq *old_rq = cpu_rq(old_cpu), *new_rq = cpu_rq(new_cpu);", 
      "-\tu64 clock_offset, fair_clock_offset;", 
      "+\tstruct cfs_rq *old_cfsrq = task_cfs_rq(p),", 
      "+\t\t      *new_cfsrq = cpu_cfs_rq(old_cfsrq, new_cpu);", 
      "+\tu64 clock_offset;", 
      "", 
      "clock_offset = old_rq->clock - new_rq->clock;", 
      "-\tfair_clock_offset = old_rq->cfs.fair_clock - new_rq->cfs.fair_clock;", 
      "-", 
      "-\tif (p->se.wait_start_fair)", 
      "-\t\tp->se.wait_start_fair -= fair_clock_offset;", 
      "-\tif (p->se.sleep_start_fair)", 
      "-\t\tp->se.sleep_start_fair -= fair_clock_offset;", 
      "", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "if (p->se.wait_start)", 
      "@@ -1023,7 +1093,14 @@ void set_task_cpu(struct task_struct *p,", 
      "p->se.sleep_start -= clock_offset;", 
      "if (p->se.block_start)", 
      "p->se.block_start -= clock_offset;", 
      "+\tif (old_cpu != new_cpu) {", 
      "+\t\tschedstat_inc(p, se.nr_migrations);", 
      "+\t\tif (task_hot(p, old_rq->clock, NULL))", 
      "+\t\t\tschedstat_inc(p, se.nr_forced2_migrations);", 
      "+\t}", 
      "#endif", 
      "+\tp->se.vruntime -= old_cfsrq->min_vruntime -", 
      "+\t\t\t\t\t new_cfsrq->min_vruntime;", 
      "", 
      "__set_task_cpu(p, new_cpu);", 
      "}", 
      "@@ -1078,69 +1155,71 @@ void wait_task_inactive(struct task_stru", 
      "int running, on_rq;", 
      "struct rq *rq;", 
      "", 
      "-repeat:", 
      "-\t/*", 
      "-\t * We do the initial early heuristics without holding", 
      "-\t * any task-queue locks at all. We'll only try to get", 
      "-\t * the runqueue lock when things look like they will", 
      "-\t * work out!", 
      "-\t */", 
      "-\trq = task_rq(p);", 
      "+\tfor (;;) {", 
      "+\t\t/*", 
      "+\t\t * We do the initial early heuristics without holding", 
      "+\t\t * any task-queue locks at all. We'll only try to get", 
      "+\t\t * the runqueue lock when things look like they will", 
      "+\t\t * work out!", 
      "+\t\t */", 
      "+\t\trq = task_rq(p);", 
      "", 
      "-\t/*", 
      "-\t * If the task is actively running on another CPU", 
      "-\t * still, just relax and busy-wait without holding", 
      "-\t * any locks.", 
      "-\t *", 
      "-\t * NOTE! Since we don't hold any locks, it's not", 
      "-\t * even sure that \"rq\" stays as the right runqueue!", 
      "-\t * But we don't care, since \"task_running()\" will", 
      "-\t * return false if the runqueue has changed and p", 
      "-\t * is actually now running somewhere else!", 
      "-\t */", 
      "-\twhile (task_running(rq, p))", 
      "-\t\tcpu_relax();", 
      "+\t\t/*", 
      "+\t\t * If the task is actively running on another CPU", 
      "+\t\t * still, just relax and busy-wait without holding", 
      "+\t\t * any locks.", 
      "+\t\t *", 
      "+\t\t * NOTE! Since we don't hold any locks, it's not", 
      "+\t\t * even sure that \"rq\" stays as the right runqueue!", 
      "+\t\t * But we don't care, since \"task_running()\" will", 
      "+\t\t * return false if the runqueue has changed and p", 
      "+\t\t * is actually now running somewhere else!", 
      "+\t\t */", 
      "+\t\twhile (task_running(rq, p))", 
      "+\t\t\tcpu_relax();", 
      "", 
      "-\t/*", 
      "-\t * Ok, time to look more closely! We need the rq", 
      "-\t * lock now, to be *sure*. If we're wrong, we'll", 
      "-\t * just go back and repeat.", 
      "-\t */", 
      "-\trq = task_rq_lock(p, &flags);", 
      "-\trunning = task_running(rq, p);", 
      "-\ton_rq = p->se.on_rq;", 
      "-\ttask_rq_unlock(rq, &flags);", 
      "+\t\t/*", 
      "+\t\t * Ok, time to look more closely! We need the rq", 
      "+\t\t * lock now, to be *sure*. If we're wrong, we'll", 
      "+\t\t * just go back and repeat.", 
      "+\t\t */", 
      "+\t\trq = task_rq_lock(p, &flags);", 
      "+\t\trunning = task_running(rq, p);", 
      "+\t\ton_rq = p->se.on_rq;", 
      "+\t\ttask_rq_unlock(rq, &flags);", 
      "", 
      "-\t/*", 
      "-\t * Was it really running after all now that we", 
      "-\t * checked with the proper locks actually held?", 
      "-\t *", 
      "-\t * Oops. Go back and try again..", 
      "-\t */", 
      "-\tif (unlikely(running)) {", 
      "-\t\tcpu_relax();", 
      "-\t\tgoto repeat;", 
      "-\t}", 
      "+\t\t/*", 
      "+\t\t * Was it really running after all now that we", 
      "+\t\t * checked with the proper locks actually held?", 
      "+\t\t *", 
      "+\t\t * Oops. Go back and try again..", 
      "+\t\t */", 
      "+\t\tif (unlikely(running)) {", 
      "+\t\t\tcpu_relax();", 
      "+\t\t\tcontinue;", 
      "+\t\t}", 
      "", 
      "-\t/*", 
      "-\t * It's not enough that it's not actively running,", 
      "-\t * it must be off the runqueue _entirely_, and not", 
      "-\t * preempted!", 
      "-\t *", 
      "-\t * So if it wa still runnable (but just not actively", 
      "-\t * running right now), it's preempted, and we should", 
      "-\t * yield - it could be a while.", 
      "-\t */", 
      "-\tif (unlikely(on_rq)) {", 
      "-\t\tyield();", 
      "-\t\tgoto repeat;", 
      "-\t}", 
      "+\t\t/*", 
      "+\t\t * It's not enough that it's not actively running,", 
      "+\t\t * it must be off the runqueue _entirely_, and not", 
      "+\t\t * preempted!", 
      "+\t\t *", 
      "+\t\t * So if it wa still runnable (but just not actively", 
      "+\t\t * running right now), it's preempted, and we should", 
      "+\t\t * yield - it could be a while.", 
      "+\t\t */", 
      "+\t\tif (unlikely(on_rq)) {", 
      "+\t\t\tschedule_timeout_uninterruptible(1);", 
      "+\t\t\tcontinue;", 
      "+\t\t}", 
      "", 
      "-\t/*", 
      "-\t * Ahh, all good. It wasn't running, and it wasn't", 
      "-\t * runnable, which means that it will never become", 
      "-\t * running in the future either. We're all done!", 
      "-\t */", 
      "+\t\t/*", 
      "+\t\t * Ahh, all good. It wasn't running, and it wasn't", 
      "+\t\t * runnable, which means that it will never become", 
      "+\t\t * running in the future either. We're all done!", 
      "+\t\t */", 
      "+\t\tbreak;", 
      "+\t}", 
      "}", 
      "", 
      "/***", 
      "@@ -1174,7 +1253,7 @@ void kick_process(struct task_struct *p)", 
      "* We want to under-estimate the load of migration sources, to", 
      "* balance conservatively.", 
      "*/", 
      "-static inline unsigned long source_load(int cpu, int type)", 
      "+static unsigned long source_load(int cpu, int type)", 
      "{", 
      "struct rq *rq = cpu_rq(cpu);", 
      "unsigned long total = weighted_cpuload(cpu);", 
      "@@ -1189,7 +1268,7 @@ static inline unsigned long source_load(", 
      "* Return a high guess at the load of a migration-target cpu weighted", 
      "* according to the scheduling class and \"nice\" value.", 
      "*/", 
      "-static inline unsigned long target_load(int cpu, int type)", 
      "+static unsigned long target_load(int cpu, int type)", 
      "{", 
      "struct rq *rq = cpu_rq(cpu);", 
      "unsigned long total = weighted_cpuload(cpu);", 
      "@@ -1231,7 +1310,7 @@ find_idlest_group(struct sched_domain *s", 
      "", 
      "/* Skip over this group if it has no CPUs allowed */", 
      "if (!cpus_intersects(group->cpumask, p->cpus_allowed))", 
      "-\t\t\tgoto nextgroup;", 
      "+\t\t\tcontinue;", 
      "", 
      "local_group = cpu_isset(this_cpu, group->cpumask);", 
      "", 
      "@@ -1259,9 +1338,7 @@ find_idlest_group(struct sched_domain *s", 
      "min_load = avg_load;", 
      "idlest = group;", 
      "}", 
      "-nextgroup:", 
      "-\t\tgroup = group->next;", 
      "-\t} while (group != sd->groups);", 
      "+\t} while (group = group->next, group != sd->groups);", 
      "", 
      "if (!idlest || 100*this_load < imbalance*min_load)", 
      "return NULL;", 
      "@@ -1393,8 +1470,13 @@ static int wake_idle(int cpu, struct tas", 
      "if (sd->flags & SD_WAKE_IDLE) {", 
      "cpus_and(tmp, sd->span, p->cpus_allowed);", 
      "for_each_cpu_mask(i, tmp) {", 
      "-\t\t\t\tif (idle_cpu(i))", 
      "+\t\t\t\tif (idle_cpu(i)) {", 
      "+\t\t\t\t\tif (i != task_cpu(p)) {", 
      "+\t\t\t\t\t\tschedstat_inc(p,", 
      "+\t\t\t\t\t\t\tse.nr_wakeups_idle);", 
      "+\t\t\t\t\t}", 
      "return i;", 
      "+\t\t\t\t}", 
      "}", 
      "} else {", 
      "break;", 
      "@@ -1425,7 +1507,7 @@ static inline int wake_idle(int cpu, str", 
      "*/", 
      "static int try_to_wake_up(struct task_struct *p, unsigned int state, int sync)", 
      "{", 
      "-\tint cpu, this_cpu, success = 0;", 
      "+\tint cpu, orig_cpu, this_cpu, success = 0;", 
      "unsigned long flags;", 
      "long old_state;", 
      "struct rq *rq;", 
      "@@ -1446,6 +1528,7 @@ static int try_to_wake_up(struct task_st", 
      "goto out_running;", 
      "", 
      "cpu = task_cpu(p);", 
      "+\torig_cpu = cpu;", 
      "this_cpu = smp_processor_id();", 
      "", 
      "#ifdef CONFIG_SMP", 
      "@@ -1454,7 +1537,7 @@ static int try_to_wake_up(struct task_st", 
      "", 
      "new_cpu = cpu;", 
      "", 
      "-\tschedstat_inc(rq, ttwu_cnt);", 
      "+\tschedstat_inc(rq, ttwu_count);", 
      "if (cpu == this_cpu) {", 
      "schedstat_inc(rq, ttwu_local);", 
      "goto out_set_cpu;", 
      "@@ -1489,6 +1572,13 @@ static int try_to_wake_up(struct task_st", 
      "unsigned long tl = this_load;", 
      "unsigned long tl_per_task;", 
      "", 
      "+\t\t\t/*", 
      "+\t\t\t * Attract cache-cold tasks on sync wakeups:", 
      "+\t\t\t */", 
      "+\t\t\tif (sync && !task_hot(p, rq->clock, this_sd))", 
      "+\t\t\t\tgoto out_set_cpu;", 
      "+", 
      "+\t\t\tschedstat_inc(p, se.nr_wakeups_affine_attempts);", 
      "tl_per_task = cpu_avg_load_per_task(this_cpu);", 
      "", 
      "/*", 
      "@@ -1508,6 +1598,7 @@ static int try_to_wake_up(struct task_st", 
      "* there is no bad imbalance.", 
      "*/", 
      "schedstat_inc(this_sd, ttwu_move_affine);", 
      "+\t\t\t\tschedstat_inc(p, se.nr_wakeups_affine);", 
      "goto out_set_cpu;", 
      "}", 
      "}", 
      "@@ -1519,6 +1610,7 @@ static int try_to_wake_up(struct task_st", 
      "if (this_sd->flags & SD_WAKE_BALANCE) {", 
      "if (imbalance*this_load <= 100*load) {", 
      "schedstat_inc(this_sd, ttwu_move_balance);", 
      "+\t\t\t\tschedstat_inc(p, se.nr_wakeups_passive);", 
      "goto out_set_cpu;", 
      "}", 
      "}", 
      "@@ -1544,18 +1636,18 @@ out_set_cpu:", 
      "", 
      "out_activate:", 
      "#endif /* CONFIG_SMP */", 
      "+\tschedstat_inc(p, se.nr_wakeups);", 
      "+\tif (sync)", 
      "+\t\tschedstat_inc(p, se.nr_wakeups_sync);", 
      "+\tif (orig_cpu != cpu)", 
      "+\t\tschedstat_inc(p, se.nr_wakeups_migrate);", 
      "+\tif (cpu == this_cpu)", 
      "+\t\tschedstat_inc(p, se.nr_wakeups_local);", 
      "+\telse", 
      "+\t\tschedstat_inc(p, se.nr_wakeups_remote);", 
      "update_rq_clock(rq);", 
      "activate_task(rq, p, 1);", 
      "-\t/*", 
      "-\t * Sync wakeups (i.e. those types of wakeups where the waker", 
      "-\t * has indicated that it will leave the CPU in short order)", 
      "-\t * don't trigger a preemption, if the woken up task will run on", 
      "-\t * this cpu. (in this case the 'I will reschedule' promise of", 
      "-\t * the waker guarantees that the freshly woken up task is going", 
      "-\t * to be considered on this CPU.)", 
      "-\t */", 
      "-\tif (!sync || cpu != this_cpu)", 
      "-\t\tcheck_preempt_curr(rq, p);", 
      "+\tcheck_preempt_curr(rq, p);", 
      "success = 1;", 
      "", 
      "out_running:", 
      "@@ -1586,28 +1678,20 @@ int fastcall wake_up_state(struct task_s", 
      "*/", 
      "static void __sched_fork(struct task_struct *p)", 
      "{", 
      "-\tp->se.wait_start_fair\t\t= 0;", 
      "p->se.exec_start\t\t= 0;", 
      "p->se.sum_exec_runtime\t\t= 0;", 
      "p->se.prev_sum_exec_runtime\t= 0;", 
      "-\tp->se.delta_exec\t\t= 0;", 
      "-\tp->se.delta_fair_run\t\t= 0;", 
      "-\tp->se.delta_fair_sleep\t\t= 0;", 
      "-\tp->se.wait_runtime\t\t= 0;", 
      "-\tp->se.sleep_start_fair\t\t= 0;", 
      "", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "p->se.wait_start\t\t= 0;", 
      "-\tp->se.sum_wait_runtime\t\t= 0;", 
      "p->se.sum_sleep_runtime\t\t= 0;", 
      "p->se.sleep_start\t\t= 0;", 
      "p->se.block_start\t\t= 0;", 
      "p->se.sleep_max\t\t\t= 0;", 
      "p->se.block_max\t\t\t= 0;", 
      "p->se.exec_max\t\t\t= 0;", 
      "+\tp->se.slice_max\t\t\t= 0;", 
      "p->se.wait_max\t\t\t= 0;", 
      "-\tp->se.wait_runtime_overruns\t= 0;", 
      "-\tp->se.wait_runtime_underruns\t= 0;", 
      "#endif", 
      "", 
      "INIT_LIST_HEAD(&p->run_list);", 
      "@@ -1638,12 +1722,14 @@ void sched_fork(struct task_struct *p, i", 
      "#ifdef CONFIG_SMP", 
      "cpu = sched_balance_self(cpu, SD_BALANCE_FORK);", 
      "#endif", 
      "-\t__set_task_cpu(p, cpu);", 
      "+\tset_task_cpu(p, cpu);", 
      "", 
      "/*", 
      "* Make sure we do not leak PI boosting priority to the child:", 
      "*/", 
      "p->prio = current->normal_prio;", 
      "+\tif (!rt_prio(p->prio))", 
      "+\t\tp->sched_class = &fair_sched_class;", 
      "", 
      "#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)", 
      "if (likely(sched_info_on()))", 
      "@@ -1660,12 +1746,6 @@ void sched_fork(struct task_struct *p, i", 
      "}", 
      "", 
      "/*", 
      "- * After fork, child runs first. (default) If set to 0 then", 
      "- * parent will (try to) run first.", 
      "- */", 
      "-unsigned int __read_mostly sysctl_sched_child_runs_first = 1;", 
      "-", 
      "-/*", 
      "* wake_up_new_task - wake up a newly created task for the first time.", 
      "*", 
      "* This function will do some initial scheduler statistics housekeeping", 
      "@@ -1676,26 +1756,16 @@ void fastcall wake_up_new_task(struct ta", 
      "{", 
      "unsigned long flags;", 
      "struct rq *rq;", 
      "-\tint this_cpu;", 
      "", 
      "rq = task_rq_lock(p, &flags);", 
      "trace_mark(kernel_sched_wakeup_new_task, \"pid %d state %ld\",", 
      "p->pid, p->state);", 
      "BUG_ON(p->state != TASK_RUNNING);", 
      "-\tthis_cpu = smp_processor_id(); /* parent's CPU */", 
      "update_rq_clock(rq);", 
      "", 
      "p->prio = effective_prio(p);", 
      "", 
      "-\tif (rt_prio(p->prio))", 
      "-\t\tp->sched_class = &rt_sched_class;", 
      "-\telse", 
      "-\t\tp->sched_class = &fair_sched_class;", 
      "-", 
      "-\tif (!p->sched_class->task_new || !sysctl_sched_child_runs_first ||", 
      "-\t\t\t(clone_flags & CLONE_VM) || task_cpu(p) != this_cpu ||", 
      "-\t\t\t!current->se.on_rq) {", 
      "-", 
      "+\tif (!p->sched_class->task_new || !current->se.on_rq) {", 
      "activate_task(rq, p, 0);", 
      "} else {", 
      "/*", 
      "@@ -1800,11 +1870,11 @@ prepare_task_switch(struct rq *rq, struc", 
      "* and do any other architecture-specific cleanup actions.", 
      "*", 
      "* Note that we may have delayed dropping an mm in context_switch(). If", 
      "- * so, we finish that here outside of the runqueue lock.  (Doing it", 
      "+ * so, we finish that here outside of the runqueue lock. (Doing it", 
      "* with the lock held can cause deadlocks; see schedule() for", 
      "* details.)", 
      "*/", 
      "-static inline void finish_task_switch(struct rq *rq, struct task_struct *prev)", 
      "+static void finish_task_switch(struct rq *rq, struct task_struct *prev)", 
      "__releases(rq->lock)", 
      "{", 
      "struct mm_struct *mm = rq->prev_mm;", 
      "@@ -1854,7 +1924,7 @@ asmlinkage void schedule_tail(struct tas", 
      "preempt_enable();", 
      "#endif", 
      "if (current->set_child_tid)", 
      "-\t\tput_user(current->pid, current->set_child_tid);", 
      "+\t\tput_user(task_pid_vnr(current), current->set_child_tid);", 
      "}", 
      "", 
      "/*", 
      "@@ -1989,42 +2059,10 @@ unsigned long nr_active(void)", 
      "*/", 
      "static void update_cpu_load(struct rq *this_rq)", 
      "{", 
      "-\tu64 fair_delta64, exec_delta64, idle_delta64, sample_interval64, tmp64;", 
      "-\tunsigned long total_load = this_rq->ls.load.weight;", 
      "-\tunsigned long this_load =  total_load;", 
      "-\tstruct load_stat *ls = &this_rq->ls;", 
      "+\tunsigned long this_load = this_rq->load.weight;", 
      "int i, scale;", 
      "", 
      "this_rq->nr_load_updates++;", 
      "-\tif (unlikely(!(sysctl_sched_features & SCHED_FEAT_PRECISE_CPU_LOAD)))", 
      "-\t\tgoto do_avg;", 
      "-", 
      "-\t/* Update delta_fair/delta_exec fields first */", 
      "-\tupdate_curr_load(this_rq);", 
      "-", 
      "-\tfair_delta64 = ls->delta_fair + 1;", 
      "-\tls->delta_fair = 0;", 
      "-", 
      "-\texec_delta64 = ls->delta_exec + 1;", 
      "-\tls->delta_exec = 0;", 
      "-", 
      "-\tsample_interval64 = this_rq->clock - ls->load_update_last;", 
      "-\tls->load_update_last = this_rq->clock;", 
      "-", 
      "-\tif ((s64)sample_interval64 < (s64)TICK_NSEC)", 
      "-\t\tsample_interval64 = TICK_NSEC;", 
      "-", 
      "-\tif (exec_delta64 > sample_interval64)", 
      "-\t\texec_delta64 = sample_interval64;", 
      "-", 
      "-\tidle_delta64 = sample_interval64 - exec_delta64;", 
      "-", 
      "-\ttmp64 = div64_64(SCHED_LOAD_SCALE * exec_delta64, fair_delta64);", 
      "-\ttmp64 = div64_64(tmp64 * exec_delta64, sample_interval64);", 
      "-", 
      "-\tthis_load = (unsigned long)tmp64;", 
      "-", 
      "-do_avg:", 
      "", 
      "/* Update our load: */", 
      "for (i = 0, scale = 1; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {", 
      "@@ -2034,7 +2072,13 @@ do_avg:", 
      "", 
      "old_load = this_rq->cpu_load[i];", 
      "new_load = this_load;", 
      "-", 
      "+\t\t/*", 
      "+\t\t * Round up the averaging division if load is increasing. This", 
      "+\t\t * prevents us from getting stuck on 9 if the load is 10, for", 
      "+\t\t * example.", 
      "+\t\t */", 
      "+\t\tif (new_load > old_load)", 
      "+\t\t\tnew_load += scale-1;", 
      "this_rq->cpu_load[i] = (old_load*(scale-1) + new_load) >> i;", 
      "}", 
      "}", 
      "@@ -2111,7 +2155,7 @@ static void double_lock_balance(struct r", 
      "/*", 
      "* If dest_cpu is allowed for this process, migrate the task to it.", 
      "* This is accomplished by forcing the cpu_allowed mask to only", 
      "- * allow dest_cpu, which will force the cpu onto dest_cpu.  Then", 
      "+ * allow dest_cpu, which will force the cpu onto dest_cpu. Then", 
      "* the cpu_allowed mask is restored.", 
      "*/", 
      "static void sched_migrate_task(struct task_struct *p, int dest_cpu)", 
      "@@ -2188,27 +2232,52 @@ int can_migrate_task(struct task_struct", 
      "* 2) cannot be migrated to this CPU due to cpus_allowed, or", 
      "* 3) are cache-hot on their current CPU.", 
      "*/", 
      "-\tif (!cpu_isset(this_cpu, p->cpus_allowed))", 
      "+\tif (!cpu_isset(this_cpu, p->cpus_allowed)) {", 
      "+\t\tschedstat_inc(p, se.nr_failed_migrations_affine);", 
      "return 0;", 
      "+\t}", 
      "*all_pinned = 0;", 
      "", 
      "-\tif (task_running(rq, p))", 
      "+\tif (task_running(rq, p)) {", 
      "+\t\tschedstat_inc(p, se.nr_failed_migrations_running);", 
      "return 0;", 
      "+\t}", 
      "+", 
      "+\t/*", 
      "+\t * Aggressive migration if:", 
      "+\t * 1) task is cache cold, or", 
      "+\t * 2) too many balance attempts have failed.", 
      "+\t */", 
      "+", 
      "+\tif (!task_hot(p, rq->clock, sd) ||", 
      "+\t\t\tsd->nr_balance_failed > sd->cache_nice_tries) {", 
      "+#ifdef CONFIG_SCHEDSTATS", 
      "+\t\tif (task_hot(p, rq->clock, sd)) {", 
      "+\t\t\tschedstat_inc(sd, lb_hot_gained[idle]);", 
      "+\t\t\tschedstat_inc(p, se.nr_forced_migrations);", 
      "+\t\t}", 
      "+#endif", 
      "+\t\treturn 1;", 
      "+\t}", 
      "", 
      "+\tif (task_hot(p, rq->clock, sd)) {", 
      "+\t\tschedstat_inc(p, se.nr_failed_migrations_hot);", 
      "+\t\treturn 0;", 
      "+\t}", 
      "return 1;", 
      "}", 
      "", 
      "-static int balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "-\t\t      unsigned long max_nr_move, unsigned long max_load_move,", 
      "-\t\t      struct sched_domain *sd, enum cpu_idle_type idle,", 
      "-\t\t      int *all_pinned, unsigned long *load_moved,", 
      "-\t\t      int *this_best_prio, struct rq_iterator *iterator)", 
      "+static unsigned long", 
      "+balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "+\t      unsigned long max_load_move, struct sched_domain *sd,", 
      "+\t      enum cpu_idle_type idle, int *all_pinned,", 
      "+\t      int *this_best_prio, struct rq_iterator *iterator)", 
      "{", 
      "-\tint pulled = 0, pinned = 0, skip_for_load;", 
      "+\tint loops = 0, pulled = 0, pinned = 0, skip_for_load;", 
      "struct task_struct *p;", 
      "long rem_load_move = max_load_move;", 
      "", 
      "-\tif (max_nr_move == 0 || max_load_move == 0)", 
      "+\tif (max_load_move == 0)", 
      "goto out;", 
      "", 
      "pinned = 1;", 
      "@@ -2218,10 +2287,10 @@ static int balance_tasks(struct rq *this", 
      "*/", 
      "p = iterator->start(iterator->arg);", 
      "next:", 
      "-\tif (!p)", 
      "+\tif (!p || loops++ > sysctl_sched_nr_migrate)", 
      "goto out;", 
      "/*", 
      "-\t * To help distribute high priority tasks accross CPUs we don't", 
      "+\t * To help distribute high priority tasks across CPUs we don't", 
      "* skip a task if it will be the highest priority task (i.e. smallest", 
      "* prio value) on its new queue regardless of its load weight", 
      "*/", 
      "@@ -2238,10 +2307,9 @@ next:", 
      "rem_load_move -= p->se.load.weight;", 
      "", 
      "/*", 
      "-\t * We only want to steal up to the prescribed number of tasks", 
      "-\t * and the prescribed amount of weighted load.", 
      "+\t * We only want to steal up to the prescribed amount of weighted load.", 
      "*/", 
      "-\tif (pulled < max_nr_move && rem_load_move > 0) {", 
      "+\tif (rem_load_move > 0) {", 
      "if (p->prio < *this_best_prio)", 
      "*this_best_prio = p->prio;", 
      "p = iterator->next(iterator->arg);", 
      "@@ -2249,7 +2317,7 @@ next:", 
      "}", 
      "out:", 
      "/*", 
      "-\t * Right now, this is the only place pull_task() is called,", 
      "+\t * Right now, this is one of only two places pull_task() is called,", 
      "* so we can safely collect pull_task() stats here rather than", 
      "* inside pull_task().", 
      "*/", 
      "@@ -2257,8 +2325,8 @@ out:", 
      "", 
      "if (all_pinned)", 
      "*all_pinned = pinned;", 
      "-\t*load_moved = max_load_move - rem_load_move;", 
      "-\treturn pulled;", 
      "+", 
      "+\treturn max_load_move - rem_load_move;", 
      "}", 
      "", 
      "/*", 
      "@@ -2273,14 +2341,14 @@ static int move_tasks(struct rq *this_rq", 
      "struct sched_domain *sd, enum cpu_idle_type idle,", 
      "int *all_pinned)", 
      "{", 
      "-\tstruct sched_class *class = sched_class_highest;", 
      "+\tconst struct sched_class *class = sched_class_highest;", 
      "unsigned long total_load_moved = 0;", 
      "int this_best_prio = this_rq->curr->prio;", 
      "", 
      "do {", 
      "total_load_moved +=", 
      "class->load_balance(this_rq, this_cpu, busiest,", 
      "-\t\t\t\tULONG_MAX, max_load_move - total_load_moved,", 
      "+\t\t\t\tmax_load_move - total_load_moved,", 
      "sd, idle, all_pinned, &this_best_prio);", 
      "class = class->next;", 
      "} while (class && max_load_move > total_load_moved);", 
      "@@ -2288,6 +2356,32 @@ static int move_tasks(struct rq *this_rq", 
      "return total_load_moved > 0;", 
      "}", 
      "", 
      "+static int", 
      "+iter_move_one_task(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "+\t\t   struct sched_domain *sd, enum cpu_idle_type idle,", 
      "+\t\t   struct rq_iterator *iterator)", 
      "+{", 
      "+\tstruct task_struct *p = iterator->start(iterator->arg);", 
      "+\tint pinned = 0;", 
      "+", 
      "+\twhile (p) {", 
      "+\t\tif (can_migrate_task(p, busiest, this_cpu, sd, idle, &pinned)) {", 
      "+\t\t\tpull_task(busiest, p, this_rq, this_cpu);", 
      "+\t\t\t/*", 
      "+\t\t\t * Right now, this is only the second place pull_task()", 
      "+\t\t\t * is called, so we can safely collect pull_task()", 
      "+\t\t\t * stats here rather than inside pull_task().", 
      "+\t\t\t */", 
      "+\t\t\tschedstat_inc(sd, lb_gained[idle]);", 
      "+", 
      "+\t\t\treturn 1;", 
      "+\t\t}", 
      "+\t\tp = iterator->next(iterator->arg);", 
      "+\t}", 
      "+", 
      "+\treturn 0;", 
      "+}", 
      "+", 
      "/*", 
      "* move_one_task tries to move exactly one task from busiest to this_rq, as", 
      "* part of active balancing operations within \"domain\".", 
      "@@ -2298,13 +2392,10 @@ static int move_tasks(struct rq *this_rq", 
      "static int move_one_task(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "struct sched_domain *sd, enum cpu_idle_type idle)", 
      "{", 
      "-\tstruct sched_class *class;", 
      "-\tint this_best_prio = MAX_PRIO;", 
      "+\tconst struct sched_class *class;", 
      "", 
      "for (class = sched_class_highest; class; class = class->next)", 
      "-\t\tif (class->load_balance(this_rq, this_cpu, busiest,", 
      "-\t\t\t\t\t1, ULONG_MAX, sd, idle, NULL,", 
      "-\t\t\t\t\t&this_best_prio))", 
      "+\t\tif (class->move_one_task(this_rq, this_cpu, busiest, sd, idle))", 
      "return 1;", 
      "", 
      "return 0;", 
      "@@ -2325,7 +2416,7 @@ find_busiest_group(struct sched_domain *", 
      "unsigned long max_pull;", 
      "unsigned long busiest_load_per_task, busiest_nr_running;", 
      "unsigned long this_load_per_task, this_nr_running;", 
      "-\tint load_idx;", 
      "+\tint load_idx, group_imb = 0;", 
      "#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)", 
      "int power_savings_balance = 1;", 
      "unsigned long leader_nr_running = 0, min_load_per_task = 0;", 
      "@@ -2344,9 +2435,10 @@ find_busiest_group(struct sched_domain *", 
      "load_idx = sd->idle_idx;", 
      "", 
      "do {", 
      "-\t\tunsigned long load, group_capacity;", 
      "+\t\tunsigned long load, group_capacity, max_cpu_load, min_cpu_load;", 
      "int local_group;", 
      "int i;", 
      "+\t\tint __group_imb = 0;", 
      "unsigned int balance_cpu = -1, first_idle_cpu = 0;", 
      "unsigned long sum_nr_running, sum_weighted_load;", 
      "", 
      "@@ -2357,6 +2449,8 @@ find_busiest_group(struct sched_domain *", 
      "", 
      "/* Tally up the load of all CPUs in the group */", 
      "sum_weighted_load = sum_nr_running = avg_load = 0;", 
      "+\t\tmax_cpu_load = 0;", 
      "+\t\tmin_cpu_load = ~0UL;", 
      "", 
      "for_each_cpu_mask(i, group->cpumask) {", 
      "struct rq *rq;", 
      "@@ -2377,8 +2471,13 @@ find_busiest_group(struct sched_domain *", 
      "}", 
      "", 
      "load = target_load(i, load_idx);", 
      "-\t\t\t} else", 
      "+\t\t\t} else {", 
      "load = source_load(i, load_idx);", 
      "+\t\t\t\tif (load > max_cpu_load)", 
      "+\t\t\t\t\tmax_cpu_load = load;", 
      "+\t\t\t\tif (min_cpu_load > load)", 
      "+\t\t\t\t\tmin_cpu_load = load;", 
      "+\t\t\t}", 
      "", 
      "avg_load += load;", 
      "sum_nr_running += rq->nr_running;", 
      "@@ -2404,6 +2503,9 @@ find_busiest_group(struct sched_domain *", 
      "avg_load = sg_div_cpu_power(group,", 
      "avg_load * SCHED_LOAD_SCALE);", 
      "", 
      "+\t\tif ((max_cpu_load - min_cpu_load) > SCHED_LOAD_SCALE)", 
      "+\t\t\t__group_imb = 1;", 
      "+", 
      "group_capacity = group->__cpu_power / SCHED_LOAD_SCALE;", 
      "", 
      "if (local_group) {", 
      "@@ -2412,11 +2514,12 @@ find_busiest_group(struct sched_domain *", 
      "this_nr_running = sum_nr_running;", 
      "this_load_per_task = sum_weighted_load;", 
      "} else if (avg_load > max_load &&", 
      "-\t\t\t   sum_nr_running > group_capacity) {", 
      "+\t\t\t   (sum_nr_running > group_capacity || __group_imb)) {", 
      "max_load = avg_load;", 
      "busiest = group;", 
      "busiest_nr_running = sum_nr_running;", 
      "busiest_load_per_task = sum_weighted_load;", 
      "+\t\t\tgroup_imb = __group_imb;", 
      "}", 
      "", 
      "#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)", 
      "@@ -2488,6 +2591,9 @@ group_next:", 
      "goto out_balanced;", 
      "", 
      "busiest_load_per_task /= busiest_nr_running;", 
      "+\tif (group_imb)", 
      "+\t\tbusiest_load_per_task = min(busiest_load_per_task, avg_load);", 
      "+", 
      "/*", 
      "* We're trying to get all the cpus to the average_load, so we don't", 
      "* want to push ourselves above the average load, nor do we wish to", 
      "@@ -2496,7 +2602,7 @@ group_next:", 
      "* tasks around. Thus we look for the minimum possible imbalance.", 
      "* Negative imbalances (*we* are more loaded than anyone else) will", 
      "* be counted as no imbalance for these purposes -- we can't fix that", 
      "-\t * by pulling tasks to us.  Be careful of negative numbers as they'll", 
      "+\t * by pulling tasks to us. Be careful of negative numbers as they'll", 
      "* appear as very large values with unsigned longs.", 
      "*/", 
      "if (max_load <= busiest_load_per_task)", 
      "@@ -2662,7 +2768,7 @@ static int load_balance(int this_cpu, st", 
      "!test_sd_parent(sd, SD_POWERSAVINGS_BALANCE))", 
      "sd_idle = 1;", 
      "", 
      "-\tschedstat_inc(sd, lb_cnt[idle]);", 
      "+\tschedstat_inc(sd, lb_count[idle]);", 
      "", 
      "redo:", 
      "group = find_busiest_group(sd, this_cpu, &imbalance, idle, &sd_idle,", 
      "@@ -2815,7 +2921,7 @@ load_balance_newidle(int this_cpu, struc", 
      "!test_sd_parent(sd, SD_POWERSAVINGS_BALANCE))", 
      "sd_idle = 1;", 
      "", 
      "-\tschedstat_inc(sd, lb_cnt[CPU_NEWLY_IDLE]);", 
      "+\tschedstat_inc(sd, lb_count[CPU_NEWLY_IDLE]);", 
      "redo:", 
      "group = find_busiest_group(sd, this_cpu, &imbalance, CPU_NEWLY_IDLE,", 
      "&sd_idle, &cpus, NULL);", 
      "@@ -2931,7 +3037,7 @@ static void active_load_balance(struct r", 
      "", 
      "/*", 
      "* This condition is \"impossible\", if it occurs", 
      "-\t * we need to fix it.  Originally reported by", 
      "+\t * we need to fix it. Originally reported by", 
      "* Bjorn Helgaas on a 128-cpu setup.", 
      "*/", 
      "BUG_ON(busiest_rq == target_rq);", 
      "@@ -2949,7 +3055,7 @@ static void active_load_balance(struct r", 
      "}", 
      "", 
      "if (likely(sd)) {", 
      "-\t\tschedstat_inc(sd, alb_cnt);", 
      "+\t\tschedstat_inc(sd, alb_count);", 
      "", 
      "if (move_one_task(target_rq, target_cpu, busiest_rq,", 
      "sd, CPU_IDLE))", 
      "@@ -2963,7 +3069,7 @@ static void active_load_balance(struct r", 
      "#ifdef CONFIG_NO_HZ", 
      "static struct {", 
      "atomic_t load_balancer;", 
      "-\tcpumask_t  cpu_mask;", 
      "+\tcpumask_t cpu_mask;", 
      "} nohz ____cacheline_aligned = {", 
      ".load_balancer = ATOMIC_INIT(-1),", 
      ".cpu_mask = CPU_MASK_NONE,", 
      "@@ -3042,7 +3148,7 @@ static DEFINE_SPINLOCK(balancing);", 
      "*", 
      "* Balancing parameters are set up in arch_init_sched_domains.", 
      "*/", 
      "-static inline void rebalance_domains(int cpu, enum cpu_idle_type idle)", 
      "+static void rebalance_domains(int cpu, enum cpu_idle_type idle)", 
      "{", 
      "int balance = 1;", 
      "struct rq *rq = cpu_rq(cpu);", 
      "@@ -3226,18 +3332,6 @@ static inline void idle_balance(int cpu,", 
      "{", 
      "}", 
      "", 
      "-/* Avoid \"used but not defined\" warning on UP */", 
      "-static int balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "-\t\t      unsigned long max_nr_move, unsigned long max_load_move,", 
      "-\t\t      struct sched_domain *sd, enum cpu_idle_type idle,", 
      "-\t\t      int *all_pinned, unsigned long *load_moved,", 
      "-\t\t      int *this_best_prio, struct rq_iterator *iterator)", 
      "-{", 
      "-\t*load_moved = 0;", 
      "-", 
      "-\treturn 0;", 
      "-}", 
      "-", 
      "#endif", 
      "", 
      "DEFINE_PER_CPU(struct kernel_stat, kstat);", 
      "@@ -3256,7 +3350,7 @@ unsigned long long task_sched_runtime(st", 
      "", 
      "rq = task_rq_lock(p, &flags);", 
      "ns = p->se.sum_exec_runtime;", 
      "-\tif (rq->curr == p) {", 
      "+\tif (task_current(rq, p)) {", 
      "update_rq_clock(rq);", 
      "delta_exec = rq->clock - p->se.exec_start;", 
      "if ((s64)delta_exec > 0)", 
      "@@ -3270,7 +3364,6 @@ unsigned long long task_sched_runtime(st", 
      "/*", 
      "* Account user cpu time to a process.", 
      "* @p: the process that the cpu time gets accounted to", 
      "- * @hardirq_offset: the offset to subtract from hardirq_count()", 
      "* @cputime: the cpu time spent in user space since the last update", 
      "*/", 
      "void account_user_time(struct task_struct *p, cputime_t cputime)", 
      "@@ -3289,6 +3382,35 @@ void account_user_time(struct task_struc", 
      "}", 
      "", 
      "/*", 
      "+ * Account guest cpu time to a process.", 
      "+ * @p: the process that the cpu time gets accounted to", 
      "+ * @cputime: the cpu time spent in virtual machine since the last update", 
      "+ */", 
      "+static void account_guest_time(struct task_struct *p, cputime_t cputime)", 
      "+{", 
      "+\tcputime64_t tmp;", 
      "+\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;", 
      "+", 
      "+\ttmp = cputime_to_cputime64(cputime);", 
      "+", 
      "+\tp->utime = cputime_add(p->utime, cputime);", 
      "+\tp->gtime = cputime_add(p->gtime, cputime);", 
      "+", 
      "+\tcpustat->user = cputime64_add(cpustat->user, tmp);", 
      "+\tcpustat->guest = cputime64_add(cpustat->guest, tmp);", 
      "+}", 
      "+", 
      "+/*", 
      "+ * Account scaled user cpu time to a process.", 
      "+ * @p: the process that the cpu time gets accounted to", 
      "+ * @cputime: the cpu time spent in user space since the last update", 
      "+ */", 
      "+void account_user_time_scaled(struct task_struct *p, cputime_t cputime)", 
      "+{", 
      "+\tp->utimescaled = cputime_add(p->utimescaled, cputime);", 
      "+}", 
      "+", 
      "+/*", 
      "* Account system cpu time to a process.", 
      "* @p: the process that the cpu time gets accounted to", 
      "* @hardirq_offset: the offset to subtract from hardirq_count()", 
      "@@ -3301,6 +3423,9 @@ void account_system_time(struct task_str", 
      "struct rq *rq = this_rq();", 
      "cputime64_t tmp;", 
      "", 
      "+\tif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0))", 
      "+\t\treturn account_guest_time(p, cputime);", 
      "+", 
      "p->stime = cputime_add(p->stime, cputime);", 
      "", 
      "/* Add system time to cpustat. */", 
      "@@ -3320,6 +3445,17 @@ void account_system_time(struct task_str", 
      "}", 
      "", 
      "/*", 
      "+ * Account scaled system cpu time to a process.", 
      "+ * @p: the process that the cpu time gets accounted to", 
      "+ * @hardirq_offset: the offset to subtract from hardirq_count()", 
      "+ * @cputime: the cpu time spent in kernel space since the last update", 
      "+ */", 
      "+void account_system_time_scaled(struct task_struct *p, cputime_t cputime)", 
      "+{", 
      "+\tp->stimescaled = cputime_add(p->stimescaled, cputime);", 
      "+}", 
      "+", 
      "+/*", 
      "* Account for involuntary wait time.", 
      "* @p: the process from which the cpu time has been stolen", 
      "* @steal: the cpu time spent in involuntary wait", 
      "@@ -3416,12 +3552,19 @@ EXPORT_SYMBOL(sub_preempt_count);", 
      "*/", 
      "static noinline void __schedule_bug(struct task_struct *prev)", 
      "{", 
      "-\tprintk(KERN_ERR \"BUG: scheduling while atomic: %s/0x%08x/%d\\n\",", 
      "-\t\tprev->comm, preempt_count(), prev->pid);", 
      "+\tstruct pt_regs *regs = get_irq_regs();", 
      "+", 
      "+\tprintk(KERN_ERR \"BUG: scheduling while atomic: %s/%d/0x%08x\\n\",", 
      "+\t\tprev->comm, prev->pid, preempt_count());", 
      "+", 
      "debug_show_held_locks(prev);", 
      "if (irqs_disabled())", 
      "print_irqtrace_events(prev);", 
      "-\tdump_stack();", 
      "+", 
      "+\tif (regs)", 
      "+\t\tshow_regs(regs);", 
      "+\telse", 
      "+\t\tdump_stack();", 
      "}", 
      "", 
      "/*", 
      "@@ -3430,7 +3573,7 @@ static noinline void __schedule_bug(stru", 
      "static inline void schedule_debug(struct task_struct *prev)", 
      "{", 
      "/*", 
      "-\t * Test if we are atomic.  Since do_exit() needs to call into", 
      "+\t * Test if we are atomic. Since do_exit() needs to call into", 
      "* schedule() atomically, we ignore that path for now.", 
      "* Otherwise, whine if we are scheduling when we should not be.", 
      "*/", 
      "@@ -3439,7 +3582,13 @@ static inline void schedule_debug(struct", 
      "", 
      "profile_hit(SCHED_PROFILING, __builtin_return_address(0));", 
      "", 
      "-\tschedstat_inc(this_rq(), sched_cnt);", 
      "+\tschedstat_inc(this_rq(), sched_count);", 
      "+#ifdef CONFIG_SCHEDSTATS", 
      "+\tif (unlikely(prev->lock_depth >= 0)) {", 
      "+\t\tschedstat_inc(this_rq(), bkl_count);", 
      "+\t\tschedstat_inc(prev, sched_info.bkl_count);", 
      "+\t}", 
      "+#endif", 
      "}", 
      "", 
      "/*", 
      "@@ -3448,7 +3597,7 @@ static inline void schedule_debug(struct", 
      "static inline struct task_struct *", 
      "pick_next_task(struct rq *rq, struct task_struct *prev)", 
      "{", 
      "-\tstruct sched_class *class;", 
      "+\tconst struct sched_class *class;", 
      "struct task_struct *p;", 
      "", 
      "/*", 
      "@@ -3497,9 +3646,13 @@ need_resched_nonpreemptible:", 
      "", 
      "schedule_debug(prev);", 
      "", 
      "-\tspin_lock_irq(&rq->lock);", 
      "-\tclear_tsk_need_resched(prev);", 
      "+\t/*", 
      "+\t * Do the rq-clock update outside the rq lock:", 
      "+\t */", 
      "+\tlocal_irq_disable();", 
      "__update_rq_clock(rq);", 
      "+\tspin_lock(&rq->lock);", 
      "+\tclear_tsk_need_resched(prev);", 
      "", 
      "if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {", 
      "if (unlikely((prev->state & TASK_INTERRUPTIBLE) &&", 
      "@@ -3542,7 +3695,7 @@ EXPORT_SYMBOL(schedule);", 
      "#ifdef CONFIG_PREEMPT", 
      "/*", 
      "* this is the entry point to schedule() from in-kernel preemption", 
      "- * off of preempt_enable.  Kernel preemptions off return from interrupt", 
      "+ * off of preempt_enable. Kernel preemptions off return from interrupt", 
      "* occur there and call schedule directly.", 
      "*/", 
      "asmlinkage void __sched preempt_schedule(void)", 
      "@@ -3554,32 +3707,35 @@ asmlinkage void __sched preempt_schedule", 
      "#endif", 
      "/*", 
      "* If there is a non-zero preempt_count or interrupts are disabled,", 
      "-\t * we do not want to preempt the current task.  Just return..", 
      "+\t * we do not want to preempt the current task. Just return..", 
      "*/", 
      "if (likely(ti->preempt_count || irqs_disabled()))", 
      "return;", 
      "", 
      "-need_resched:", 
      "-\tadd_preempt_count(PREEMPT_ACTIVE);", 
      "-\t/*", 
      "-\t * We keep the big kernel semaphore locked, but we", 
      "-\t * clear ->lock_depth so that schedule() doesnt", 
      "-\t * auto-release the semaphore:", 
      "-\t */", 
      "+\tdo {", 
      "+\t\tadd_preempt_count(PREEMPT_ACTIVE);", 
      "+", 
      "+\t\t/*", 
      "+\t\t * We keep the big kernel semaphore locked, but we", 
      "+\t\t * clear ->lock_depth so that schedule() doesnt", 
      "+\t\t * auto-release the semaphore:", 
      "+\t\t */", 
      "#ifdef CONFIG_PREEMPT_BKL", 
      "-\tsaved_lock_depth = task->lock_depth;", 
      "-\ttask->lock_depth = -1;", 
      "+\t\tsaved_lock_depth = task->lock_depth;", 
      "+\t\ttask->lock_depth = -1;", 
      "#endif", 
      "-\tschedule();", 
      "+\t\tschedule();", 
      "#ifdef CONFIG_PREEMPT_BKL", 
      "-\ttask->lock_depth = saved_lock_depth;", 
      "+\t\ttask->lock_depth = saved_lock_depth;", 
      "#endif", 
      "-\tsub_preempt_count(PREEMPT_ACTIVE);", 
      "+\t\tsub_preempt_count(PREEMPT_ACTIVE);", 
      "", 
      "-\t/* we could miss a preemption opportunity between schedule and now */", 
      "-\tbarrier();", 
      "-\tif (unlikely(test_thread_flag(TIF_NEED_RESCHED)))", 
      "-\t\tgoto need_resched;", 
      "+\t\t/*", 
      "+\t\t * Check again in case we missed a preemption opportunity", 
      "+\t\t * between schedule and now.", 
      "+\t\t */", 
      "+\t\tbarrier();", 
      "+\t} while (unlikely(test_thread_flag(TIF_NEED_RESCHED)));", 
      "}", 
      "EXPORT_SYMBOL(preempt_schedule);", 
      "", 
      "@@ -3599,29 +3755,32 @@ asmlinkage void __sched preempt_schedule", 
      "/* Catch callers which need to be fixed */", 
      "BUG_ON(ti->preempt_count || !irqs_disabled());", 
      "", 
      "-need_resched:", 
      "-\tadd_preempt_count(PREEMPT_ACTIVE);", 
      "-\t/*", 
      "-\t * We keep the big kernel semaphore locked, but we", 
      "-\t * clear ->lock_depth so that schedule() doesnt", 
      "-\t * auto-release the semaphore:", 
      "-\t */", 
      "+\tdo {", 
      "+\t\tadd_preempt_count(PREEMPT_ACTIVE);", 
      "+", 
      "+\t\t/*", 
      "+\t\t * We keep the big kernel semaphore locked, but we", 
      "+\t\t * clear ->lock_depth so that schedule() doesnt", 
      "+\t\t * auto-release the semaphore:", 
      "+\t\t */", 
      "#ifdef CONFIG_PREEMPT_BKL", 
      "-\tsaved_lock_depth = task->lock_depth;", 
      "-\ttask->lock_depth = -1;", 
      "+\t\tsaved_lock_depth = task->lock_depth;", 
      "+\t\ttask->lock_depth = -1;", 
      "#endif", 
      "-\tlocal_irq_enable();", 
      "-\tschedule();", 
      "-\tlocal_irq_disable();", 
      "+\t\tlocal_irq_enable();", 
      "+\t\tschedule();", 
      "+\t\tlocal_irq_disable();", 
      "#ifdef CONFIG_PREEMPT_BKL", 
      "-\ttask->lock_depth = saved_lock_depth;", 
      "+\t\ttask->lock_depth = saved_lock_depth;", 
      "#endif", 
      "-\tsub_preempt_count(PREEMPT_ACTIVE);", 
      "+\t\tsub_preempt_count(PREEMPT_ACTIVE);", 
      "", 
      "-\t/* we could miss a preemption opportunity between schedule and now */", 
      "-\tbarrier();", 
      "-\tif (unlikely(test_thread_flag(TIF_NEED_RESCHED)))", 
      "-\t\tgoto need_resched;", 
      "+\t\t/*", 
      "+\t\t * Check again in case we missed a preemption opportunity", 
      "+\t\t * between schedule and now.", 
      "+\t\t */", 
      "+\t\tbarrier();", 
      "+\t} while (unlikely(test_thread_flag(TIF_NEED_RESCHED)));", 
      "}", 
      "", 
      "#endif /* CONFIG_PREEMPT */", 
      "@@ -3634,21 +3793,20 @@ int default_wake_function(wait_queue_t *", 
      "EXPORT_SYMBOL(default_wake_function);", 
      "", 
      "/*", 
      "- * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just", 
      "- * wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve", 
      "+ * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just", 
      "+ * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve", 
      "* number) then we wake all the non-exclusive tasks and one exclusive task.", 
      "*", 
      "* There are circumstances in which we can try to wake a task which has already", 
      "- * started to run but is not in state TASK_RUNNING.  try_to_wake_up() returns", 
      "+ * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns", 
      "* zero in this (rare) case, and we handle it by continuing to scan the queue.", 
      "*/", 
      "static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,", 
      "int nr_exclusive, int sync, void *key)", 
      "{", 
      "-\tstruct list_head *tmp, *next;", 
      "+\twait_queue_t *curr, *next;", 
      "", 
      "-\tlist_for_each_safe(tmp, next, &q->task_list) {", 
      "-\t\twait_queue_t *curr = list_entry(tmp, wait_queue_t, task_list);", 
      "+\tlist_for_each_entry_safe(curr, next, &q->task_list, task_list) {", 
      "unsigned flags = curr->flags;", 
      "", 
      "if (curr->func(curr, mode, sync, key) &&", 
      "@@ -3714,7 +3872,7 @@ __wake_up_sync(wait_queue_head_t *q, uns", 
      "}", 
      "EXPORT_SYMBOL_GPL(__wake_up_sync);\t/* For internal use only */", 
      "", 
      "-void fastcall complete(struct completion *x)", 
      "+void complete(struct completion *x)", 
      "{", 
      "unsigned long flags;", 
      "", 
      "@@ -3726,7 +3884,7 @@ void fastcall complete(struct completion", 
      "}", 
      "EXPORT_SYMBOL(complete);", 
      "", 
      "-void fastcall complete_all(struct completion *x)", 
      "+void complete_all(struct completion *x)", 
      "{", 
      "unsigned long flags;", 
      "", 
      "@@ -3738,206 +3896,119 @@ void fastcall complete_all(struct comple", 
      "}", 
      "EXPORT_SYMBOL(complete_all);", 
      "", 
      "-void fastcall __sched wait_for_completion(struct completion *x)", 
      "-{", 
      "-\tmight_sleep();", 
      "-", 
      "-\tspin_lock_irq(&x->wait.lock);", 
      "-\tif (!x->done) {", 
      "-\t\tDECLARE_WAITQUEUE(wait, current);", 
      "-", 
      "-\t\twait.flags |= WQ_FLAG_EXCLUSIVE;", 
      "-\t\t__add_wait_queue_tail(&x->wait, &wait);", 
      "-\t\tdo {", 
      "-\t\t\t__set_current_state(TASK_UNINTERRUPTIBLE);", 
      "-\t\t\tspin_unlock_irq(&x->wait.lock);", 
      "-\t\t\tschedule();", 
      "-\t\t\tspin_lock_irq(&x->wait.lock);", 
      "-\t\t} while (!x->done);", 
      "-\t\t__remove_wait_queue(&x->wait, &wait);", 
      "-\t}", 
      "-\tx->done--;", 
      "-\tspin_unlock_irq(&x->wait.lock);", 
      "-}", 
      "-EXPORT_SYMBOL(wait_for_completion);", 
      "-", 
      "-unsigned long fastcall __sched", 
      "-wait_for_completion_timeout(struct completion *x, unsigned long timeout)", 
      "+static inline long __sched", 
      "+do_wait_for_common(struct completion *x, long timeout, int state)", 
      "{", 
      "-\tmight_sleep();", 
      "-", 
      "-\tspin_lock_irq(&x->wait.lock);", 
      "if (!x->done) {", 
      "DECLARE_WAITQUEUE(wait, current);", 
      "", 
      "wait.flags |= WQ_FLAG_EXCLUSIVE;", 
      "__add_wait_queue_tail(&x->wait, &wait);", 
      "do {", 
      "-\t\t\t__set_current_state(TASK_UNINTERRUPTIBLE);", 
      "+\t\t\tif (state == TASK_INTERRUPTIBLE &&", 
      "+\t\t\t    signal_pending(current)) {", 
      "+\t\t\t\t__remove_wait_queue(&x->wait, &wait);", 
      "+\t\t\t\treturn -ERESTARTSYS;", 
      "+\t\t\t}", 
      "+\t\t\t__set_current_state(state);", 
      "spin_unlock_irq(&x->wait.lock);", 
      "timeout = schedule_timeout(timeout);", 
      "spin_lock_irq(&x->wait.lock);", 
      "if (!timeout) {", 
      "__remove_wait_queue(&x->wait, &wait);", 
      "-\t\t\t\tgoto out;", 
      "+\t\t\t\treturn timeout;", 
      "}", 
      "} while (!x->done);", 
      "__remove_wait_queue(&x->wait, &wait);", 
      "}", 
      "x->done--;", 
      "-out:", 
      "-\tspin_unlock_irq(&x->wait.lock);", 
      "return timeout;", 
      "}", 
      "-EXPORT_SYMBOL(wait_for_completion_timeout);", 
      "", 
      "-int fastcall __sched wait_for_completion_interruptible(struct completion *x)", 
      "+static long __sched", 
      "+wait_for_common(struct completion *x, long timeout, int state)", 
      "{", 
      "-\tint ret = 0;", 
      "-", 
      "might_sleep();", 
      "", 
      "spin_lock_irq(&x->wait.lock);", 
      "-\tif (!x->done) {", 
      "-\t\tDECLARE_WAITQUEUE(wait, current);", 
      "-", 
      "-\t\twait.flags |= WQ_FLAG_EXCLUSIVE;", 
      "-\t\t__add_wait_queue_tail(&x->wait, &wait);", 
      "-\t\tdo {", 
      "-\t\t\tif (signal_pending(current)) {", 
      "-\t\t\t\tret = -ERESTARTSYS;", 
      "-\t\t\t\t__remove_wait_queue(&x->wait, &wait);", 
      "-\t\t\t\tgoto out;", 
      "-\t\t\t}", 
      "-\t\t\t__set_current_state(TASK_INTERRUPTIBLE);", 
      "-\t\t\tspin_unlock_irq(&x->wait.lock);", 
      "-\t\t\tschedule();", 
      "-\t\t\tspin_lock_irq(&x->wait.lock);", 
      "-\t\t} while (!x->done);", 
      "-\t\t__remove_wait_queue(&x->wait, &wait);", 
      "-\t}", 
      "-\tx->done--;", 
      "-out:", 
      "+\ttimeout = do_wait_for_common(x, timeout, state);", 
      "spin_unlock_irq(&x->wait.lock);", 
      "-", 
      "-\treturn ret;", 
      "+\treturn timeout;", 
      "}", 
      "-EXPORT_SYMBOL(wait_for_completion_interruptible);", 
      "", 
      "-unsigned long fastcall __sched", 
      "-wait_for_completion_interruptible_timeout(struct completion *x,", 
      "-\t\t\t\t\t  unsigned long timeout)", 
      "+void __sched wait_for_completion(struct completion *x)", 
      "{", 
      "-\tmight_sleep();", 
      "-", 
      "-\tspin_lock_irq(&x->wait.lock);", 
      "-\tif (!x->done) {", 
      "-\t\tDECLARE_WAITQUEUE(wait, current);", 
      "+\twait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);", 
      "+}", 
      "+EXPORT_SYMBOL(wait_for_completion);", 
      "", 
      "-\t\twait.flags |= WQ_FLAG_EXCLUSIVE;", 
      "-\t\t__add_wait_queue_tail(&x->wait, &wait);", 
      "-\t\tdo {", 
      "-\t\t\tif (signal_pending(current)) {", 
      "-\t\t\t\ttimeout = -ERESTARTSYS;", 
      "-\t\t\t\t__remove_wait_queue(&x->wait, &wait);", 
      "-\t\t\t\tgoto out;", 
      "-\t\t\t}", 
      "-\t\t\t__set_current_state(TASK_INTERRUPTIBLE);", 
      "-\t\t\tspin_unlock_irq(&x->wait.lock);", 
      "-\t\t\ttimeout = schedule_timeout(timeout);", 
      "-\t\t\tspin_lock_irq(&x->wait.lock);", 
      "-\t\t\tif (!timeout) {", 
      "-\t\t\t\t__remove_wait_queue(&x->wait, &wait);", 
      "-\t\t\t\tgoto out;", 
      "-\t\t\t}", 
      "-\t\t} while (!x->done);", 
      "-\t\t__remove_wait_queue(&x->wait, &wait);", 
      "-\t}", 
      "-\tx->done--;", 
      "-out:", 
      "-\tspin_unlock_irq(&x->wait.lock);", 
      "-\treturn timeout;", 
      "+unsigned long __sched", 
      "+wait_for_completion_timeout(struct completion *x, unsigned long timeout)", 
      "+{", 
      "+\treturn wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);", 
      "}", 
      "-EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);", 
      "+EXPORT_SYMBOL(wait_for_completion_timeout);", 
      "", 
      "-static inline void", 
      "-sleep_on_head(wait_queue_head_t *q, wait_queue_t *wait, unsigned long *flags)", 
      "+int __sched wait_for_completion_interruptible(struct completion *x)", 
      "{", 
      "-\tspin_lock_irqsave(&q->lock, *flags);", 
      "-\t__add_wait_queue(q, wait);", 
      "-\tspin_unlock(&q->lock);", 
      "+\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);", 
      "+\tif (t == -ERESTARTSYS)", 
      "+\t\treturn t;", 
      "+\treturn 0;", 
      "}", 
      "+EXPORT_SYMBOL(wait_for_completion_interruptible);", 
      "", 
      "-static inline void", 
      "-sleep_on_tail(wait_queue_head_t *q, wait_queue_t *wait, unsigned long *flags)", 
      "+unsigned long __sched", 
      "+wait_for_completion_interruptible_timeout(struct completion *x,", 
      "+\t\t\t\t\t  unsigned long timeout)", 
      "{", 
      "-\tspin_lock_irq(&q->lock);", 
      "-\t__remove_wait_queue(q, wait);", 
      "-\tspin_unlock_irqrestore(&q->lock, *flags);", 
      "+\treturn wait_for_common(x, timeout, TASK_INTERRUPTIBLE);", 
      "}", 
      "+EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);", 
      "", 
      "-void __sched interruptible_sleep_on(wait_queue_head_t *q)", 
      "+static long __sched", 
      "+sleep_on_common(wait_queue_head_t *q, int state, long timeout)", 
      "{", 
      "unsigned long flags;", 
      "wait_queue_t wait;", 
      "", 
      "init_waitqueue_entry(&wait, current);", 
      "", 
      "-\tcurrent->state = TASK_INTERRUPTIBLE;", 
      "+\t__set_current_state(state);", 
      "", 
      "-\tsleep_on_head(q, &wait, &flags);", 
      "-\tschedule();", 
      "-\tsleep_on_tail(q, &wait, &flags);", 
      "+\tspin_lock_irqsave(&q->lock, flags);", 
      "+\t__add_wait_queue(q, &wait);", 
      "+\tspin_unlock(&q->lock);", 
      "+\ttimeout = schedule_timeout(timeout);", 
      "+\tspin_lock_irq(&q->lock);", 
      "+\t__remove_wait_queue(q, &wait);", 
      "+\tspin_unlock_irqrestore(&q->lock, flags);", 
      "+", 
      "+\treturn timeout;", 
      "+}", 
      "+", 
      "+void __sched interruptible_sleep_on(wait_queue_head_t *q)", 
      "+{", 
      "+\tsleep_on_common(q, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);", 
      "}", 
      "EXPORT_SYMBOL(interruptible_sleep_on);", 
      "", 
      "long __sched", 
      "interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)", 
      "{", 
      "-\tunsigned long flags;", 
      "-\twait_queue_t wait;", 
      "-", 
      "-\tinit_waitqueue_entry(&wait, current);", 
      "-", 
      "-\tcurrent->state = TASK_INTERRUPTIBLE;", 
      "-", 
      "-\tsleep_on_head(q, &wait, &flags);", 
      "-\ttimeout = schedule_timeout(timeout);", 
      "-\tsleep_on_tail(q, &wait, &flags);", 
      "-", 
      "-\treturn timeout;", 
      "+\treturn sleep_on_common(q, TASK_INTERRUPTIBLE, timeout);", 
      "}", 
      "EXPORT_SYMBOL(interruptible_sleep_on_timeout);", 
      "", 
      "void __sched sleep_on(wait_queue_head_t *q)", 
      "{", 
      "-\tunsigned long flags;", 
      "-\twait_queue_t wait;", 
      "-", 
      "-\tinit_waitqueue_entry(&wait, current);", 
      "-", 
      "-\tcurrent->state = TASK_UNINTERRUPTIBLE;", 
      "-", 
      "-\tsleep_on_head(q, &wait, &flags);", 
      "-\tschedule();", 
      "-\tsleep_on_tail(q, &wait, &flags);", 
      "+\tsleep_on_common(q, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);", 
      "}", 
      "EXPORT_SYMBOL(sleep_on);", 
      "", 
      "long __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)", 
      "{", 
      "-\tunsigned long flags;", 
      "-\twait_queue_t wait;", 
      "-", 
      "-\tinit_waitqueue_entry(&wait, current);", 
      "-", 
      "-\tcurrent->state = TASK_UNINTERRUPTIBLE;", 
      "-", 
      "-\tsleep_on_head(q, &wait, &flags);", 
      "-\ttimeout = schedule_timeout(timeout);", 
      "-\tsleep_on_tail(q, &wait, &flags);", 
      "-", 
      "-\treturn timeout;", 
      "+\treturn sleep_on_common(q, TASK_UNINTERRUPTIBLE, timeout);", 
      "}", 
      "EXPORT_SYMBOL(sleep_on_timeout);", 
      "", 
      "@@ -3956,7 +4027,7 @@ EXPORT_SYMBOL(sleep_on_timeout);", 
      "void rt_mutex_setprio(struct task_struct *p, int prio)", 
      "{", 
      "unsigned long flags;", 
      "-\tint oldprio, on_rq;", 
      "+\tint oldprio, on_rq, running;", 
      "struct rq *rq;", 
      "", 
      "BUG_ON(prio < 0 || prio > MAX_PRIO);", 
      "@@ -3966,8 +4037,12 @@ void rt_mutex_setprio(struct task_struct", 
      "", 
      "oldprio = p->prio;", 
      "on_rq = p->se.on_rq;", 
      "-\tif (on_rq)", 
      "+\trunning = task_current(rq, p);", 
      "+\tif (on_rq) {", 
      "dequeue_task(rq, p, 0);", 
      "+\t\tif (running)", 
      "+\t\t\tp->sched_class->put_prev_task(rq, p);", 
      "+\t}", 
      "", 
      "if (rt_prio(prio))", 
      "p->sched_class = &rt_sched_class;", 
      "@@ -3977,13 +4052,15 @@ void rt_mutex_setprio(struct task_struct", 
      "p->prio = prio;", 
      "", 
      "if (on_rq) {", 
      "+\t\tif (running)", 
      "+\t\t\tp->sched_class->set_curr_task(rq);", 
      "enqueue_task(rq, p, 0);", 
      "/*", 
      "* Reschedule if we are currently running on this runqueue and", 
      "* our priority decreased, or if we are not currently running on", 
      "* this runqueue and our priority is higher than the current's", 
      "*/", 
      "-\t\tif (task_running(rq, p)) {", 
      "+\t\tif (running) {", 
      "if (p->prio > oldprio)", 
      "resched_task(rq->curr);", 
      "} else {", 
      "@@ -4147,9 +4224,9 @@ struct task_struct *idle_task(int cpu)", 
      "* find_process_by_pid - find a process with a matching PID value.", 
      "* @pid: the pid in question.", 
      "*/", 
      "-static inline struct task_struct *find_process_by_pid(pid_t pid)", 
      "+static struct task_struct *find_process_by_pid(pid_t pid)", 
      "{", 
      "-\treturn pid ? find_task_by_pid(pid) : current;", 
      "+\treturn pid ? find_task_by_vpid(pid) : current;", 
      "}", 
      "", 
      "/* Actually do priority change: must hold rq lock. */", 
      "@@ -4189,7 +4266,7 @@ __setscheduler(struct rq *rq, struct tas", 
      "int sched_setscheduler(struct task_struct *p, int policy,", 
      "struct sched_param *param)", 
      "{", 
      "-\tint retval, oldprio, oldpolicy = -1, on_rq;", 
      "+\tint retval, oldprio, oldpolicy = -1, on_rq, running;", 
      "unsigned long flags;", 
      "struct rq *rq;", 
      "", 
      "@@ -4271,18 +4348,26 @@ recheck:", 
      "}", 
      "update_rq_clock(rq);", 
      "on_rq = p->se.on_rq;", 
      "-\tif (on_rq)", 
      "+\trunning = task_current(rq, p);", 
      "+\tif (on_rq) {", 
      "deactivate_task(rq, p, 0);", 
      "+\t\tif (running)", 
      "+\t\t\tp->sched_class->put_prev_task(rq, p);", 
      "+\t}", 
      "+", 
      "oldprio = p->prio;", 
      "__setscheduler(rq, p, policy, param->sched_priority);", 
      "+", 
      "if (on_rq) {", 
      "+\t\tif (running)", 
      "+\t\t\tp->sched_class->set_curr_task(rq);", 
      "activate_task(rq, p, 0);", 
      "/*", 
      "* Reschedule if we are currently running on this runqueue and", 
      "* our priority decreased, or if we are not currently running on", 
      "* this runqueue and our priority is higher than the current's", 
      "*/", 
      "-\t\tif (task_running(rq, p)) {", 
      "+\t\tif (running) {", 
      "if (p->prio > oldprio)", 
      "resched_task(rq->curr);", 
      "} else {", 
      "@@ -4326,8 +4411,8 @@ do_sched_setscheduler(pid_t pid, int pol", 
      "* @policy: new policy.", 
      "* @param: structure containing the new RT priority.", 
      "*/", 
      "-asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,", 
      "-\t\t\t\t       struct sched_param __user *param)", 
      "+asmlinkage long", 
      "+sys_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)", 
      "{", 
      "/* negative values for policy are not valid */", 
      "if (policy < 0)", 
      "@@ -4353,10 +4438,10 @@ asmlinkage long sys_sched_setparam(pid_t", 
      "asmlinkage long sys_sched_getscheduler(pid_t pid)", 
      "{", 
      "struct task_struct *p;", 
      "-\tint retval = -EINVAL;", 
      "+\tint retval;", 
      "", 
      "if (pid < 0)", 
      "-\t\tgoto out_nounlock;", 
      "+\t\treturn -EINVAL;", 
      "", 
      "retval = -ESRCH;", 
      "read_lock(&tasklist_lock);", 
      "@@ -4367,8 +4452,6 @@ asmlinkage long sys_sched_getscheduler(p", 
      "retval = p->policy;", 
      "}", 
      "read_unlock(&tasklist_lock);", 
      "-", 
      "-out_nounlock:", 
      "return retval;", 
      "}", 
      "", 
      "@@ -4381,10 +4464,10 @@ asmlinkage long sys_sched_getparam(pid_t", 
      "{", 
      "struct sched_param lp;", 
      "struct task_struct *p;", 
      "-\tint retval = -EINVAL;", 
      "+\tint retval;", 
      "", 
      "if (!param || pid < 0)", 
      "-\t\tgoto out_nounlock;", 
      "+\t\treturn -EINVAL;", 
      "", 
      "read_lock(&tasklist_lock);", 
      "p = find_process_by_pid(pid);", 
      "@@ -4404,7 +4487,6 @@ asmlinkage long sys_sched_getparam(pid_t", 
      "*/", 
      "retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;", 
      "", 
      "-out_nounlock:", 
      "return retval;", 
      "", 
      "out_unlock:", 
      "@@ -4430,7 +4512,7 @@ long sched_setaffinity(pid_t pid, cpumas", 
      "", 
      "/*", 
      "* It is not safe to call set_cpus_allowed with the", 
      "-\t * tasklist_lock held.  We will bump the task_struct's", 
      "+\t * tasklist_lock held. We will bump the task_struct's", 
      "* usage count and then drop tasklist_lock.", 
      "*/", 
      "get_task_struct(p);", 
      "@@ -4447,8 +4529,21 @@ long sched_setaffinity(pid_t pid, cpumas", 
      "", 
      "cpus_allowed = cpuset_cpus_allowed(p);", 
      "cpus_and(new_mask, new_mask, cpus_allowed);", 
      "+ again:", 
      "retval = set_cpus_allowed(p, new_mask);", 
      "", 
      "+\tif (!retval) {", 
      "+\t\tcpus_allowed = cpuset_cpus_allowed(p);", 
      "+\t\tif (!cpus_subset(new_mask, cpus_allowed)) {", 
      "+\t\t\t/*", 
      "+\t\t\t * We must have raced with a concurrent cpuset", 
      "+\t\t\t * update. Just reset the cpus_allowed to the", 
      "+\t\t\t * cpuset's cpus_allowed", 
      "+\t\t\t */", 
      "+\t\t\tnew_mask = cpus_allowed;", 
      "+\t\t\tgoto again;", 
      "+\t\t}", 
      "+\t}", 
      "out_unlock:", 
      "put_task_struct(p);", 
      "mutex_unlock(&sched_hotcpu_mutex);", 
      "@@ -4564,8 +4659,8 @@ asmlinkage long sys_sched_yield(void)", 
      "{", 
      "struct rq *rq = this_rq_lock();", 
      "", 
      "-\tschedstat_inc(rq, yld_cnt);", 
      "-\tcurrent->sched_class->yield_task(rq, current);", 
      "+\tschedstat_inc(rq, yld_count);", 
      "+\tcurrent->sched_class->yield_task(rq);", 
      "", 
      "/*", 
      "* Since we are going to call schedule() anyway, there's", 
      "@@ -4613,7 +4708,7 @@ EXPORT_SYMBOL(cond_resched);", 
      "* cond_resched_lock() - if a reschedule is pending, drop the given lock,", 
      "* call schedule, and on return reacquire the lock.", 
      "*", 
      "- * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level", 
      "+ * This works OK both with and without CONFIG_PREEMPT. We do strange low-level", 
      "* operations here to prevent schedule() from being called twice (once via", 
      "* spin_unlock(), once by hand).", 
      "*/", 
      "@@ -4667,7 +4762,7 @@ void __sched yield(void)", 
      "EXPORT_SYMBOL(yield);", 
      "", 
      "/*", 
      "- * This task is about to go to sleep on IO.  Increment rq->nr_iowait so", 
      "+ * This task is about to go to sleep on IO. Increment rq->nr_iowait so", 
      "* that process accounting knows that this is a task in IO wait state.", 
      "*", 
      "* But don't do that if it is a deliberate, throttling IO wait (this task", 
      "@@ -4759,11 +4854,12 @@ asmlinkage", 
      "long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)", 
      "{", 
      "struct task_struct *p;", 
      "-\tint retval = -EINVAL;", 
      "+\tunsigned int time_slice;", 
      "+\tint retval;", 
      "struct timespec t;", 
      "", 
      "if (pid < 0)", 
      "-\t\tgoto out_nounlock;", 
      "+\t\treturn -EINVAL;", 
      "", 
      "retval = -ESRCH;", 
      "read_lock(&tasklist_lock);", 
      "@@ -4775,12 +4871,28 @@ long sys_sched_rr_get_interval(pid_t pid", 
      "if (retval)", 
      "goto out_unlock;", 
      "", 
      "-\tjiffies_to_timespec(p->policy == SCHED_FIFO ?", 
      "-\t\t\t\t0 : static_prio_timeslice(p->static_prio), &t);", 
      "+\t/*", 
      "+\t * Time slice is 0 for SCHED_FIFO tasks and for SCHED_OTHER", 
      "+\t * tasks that are on an otherwise idle runqueue:", 
      "+\t */", 
      "+\ttime_slice = 0;", 
      "+\tif (p->policy == SCHED_RR) {", 
      "+\t\ttime_slice = DEF_TIMESLICE;", 
      "+\t} else {", 
      "+\t\tstruct sched_entity *se = &p->se;", 
      "+\t\tunsigned long flags;", 
      "+\t\tstruct rq *rq;", 
      "+", 
      "+\t\trq = task_rq_lock(p, &flags);", 
      "+\t\tif (rq->cfs.load.weight)", 
      "+\t\t\ttime_slice = NS_TO_JIFFIES(sched_slice(&rq->cfs, se));", 
      "+\t\ttask_rq_unlock(rq, &flags);", 
      "+\t}", 
      "read_unlock(&tasklist_lock);", 
      "+\tjiffies_to_timespec(time_slice, &t);", 
      "retval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;", 
      "-out_nounlock:", 
      "return retval;", 
      "+", 
      "out_unlock:", 
      "read_unlock(&tasklist_lock);", 
      "return retval;", 
      "@@ -4794,18 +4906,18 @@ static void show_task(struct task_struct", 
      "unsigned state;", 
      "", 
      "state = p->state ? __ffs(p->state) + 1 : 0;", 
      "-\tprintk(\"%-13.13s %c\", p->comm,", 
      "+\tprintk(KERN_INFO \"%-13.13s %c\", p->comm,", 
      "state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');", 
      "#if BITS_PER_LONG == 32", 
      "if (state == TASK_RUNNING)", 
      "-\t\tprintk(\" running  \");", 
      "+\t\tprintk(KERN_CONT \" running  \");", 
      "else", 
      "-\t\tprintk(\" %08lx \", thread_saved_pc(p));", 
      "+\t\tprintk(KERN_CONT \" %08lx \", thread_saved_pc(p));", 
      "#else", 
      "if (state == TASK_RUNNING)", 
      "-\t\tprintk(\"  running task    \");", 
      "+\t\tprintk(KERN_CONT \"  running task    \");", 
      "else", 
      "-\t\tprintk(\" %016lx \", thread_saved_pc(p));", 
      "+\t\tprintk(KERN_CONT \" %016lx \", thread_saved_pc(p));", 
      "#endif", 
      "#ifdef CONFIG_DEBUG_STACK_USAGE", 
      "{", 
      "@@ -4815,7 +4927,8 @@ static void show_task(struct task_struct", 
      "free = (unsigned long)n - (unsigned long)end_of_stack(p);", 
      "}", 
      "#endif", 
      "-\tprintk(\"%5lu %5d %6d\\n\", free, p->pid, p->parent->pid);", 
      "+\tprintk(KERN_CONT \"%5lu %5d %6d\\n\", free,", 
      "+\t\ttask_pid_nr(p), task_pid_nr(p->parent));", 
      "", 
      "if (state != TASK_RUNNING)", 
      "show_stack(p, NULL);", 
      "@@ -4921,7 +5034,7 @@ cpumask_t nohz_cpu_mask = CPU_MASK_NONE;", 
      "static inline void sched_init_granularity(void)", 
      "{", 
      "unsigned int factor = 1 + ilog2(num_online_cpus());", 
      "-\tconst unsigned long limit = 100000000;", 
      "+\tconst unsigned long limit = 200000000;", 
      "", 
      "sysctl_sched_min_granularity *= factor;", 
      "if (sysctl_sched_min_granularity > limit)", 
      "@@ -4931,8 +5044,8 @@ static inline void sched_init_granularit", 
      "if (sysctl_sched_latency > limit)", 
      "sysctl_sched_latency = limit;", 
      "", 
      "-\tsysctl_sched_runtime_limit = sysctl_sched_latency;", 
      "-\tsysctl_sched_wakeup_granularity = sysctl_sched_min_granularity / 2;", 
      "+\tsysctl_sched_wakeup_granularity *= factor;", 
      "+\tsysctl_sched_batch_wakeup_granularity *= factor;", 
      "}", 
      "", 
      "#ifdef CONFIG_SMP", 
      "@@ -4958,7 +5071,7 @@ static inline void sched_init_granularit", 
      "* is removed from the allowed bitmask.", 
      "*", 
      "* NOTE: the caller must have a valid reference to the task, the", 
      "- * task must not exit() & deallocate itself prematurely.  The", 
      "+ * task must not exit() & deallocate itself prematurely. The", 
      "* call is not atomic; no spinlocks may be held.", 
      "*/", 
      "int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)", 
      "@@ -4995,7 +5108,7 @@ out:", 
      "EXPORT_SYMBOL_GPL(set_cpus_allowed);", 
      "", 
      "/*", 
      "- * Move (not current) task off this cpu, onto dest cpu.  We're doing", 
      "+ * Move (not current) task off this cpu, onto dest cpu. We're doing", 
      "* this because either it can't run here any more (set_cpus_allowed()", 
      "* away from this CPU, or CPU going down), or because we're", 
      "* attempting to rebalance this task on exec (sched_exec).", 
      "@@ -5057,6 +5170,8 @@ static int migration_thread(void *data)", 
      "struct migration_req *req;", 
      "struct list_head *head;", 
      "", 
      "+\t\ttry_to_freeze();", 
      "+", 
      "spin_lock_irq(&rq->lock);", 
      "", 
      "if (cpu_is_offline(cpu)) {", 
      "@@ -5101,8 +5216,19 @@ wait_to_die:", 
      "}", 
      "", 
      "#ifdef CONFIG_HOTPLUG_CPU", 
      "+", 
      "+static int __migrate_task_irq(struct task_struct *p, int src_cpu, int dest_cpu)", 
      "+{", 
      "+\tint ret;", 
      "+", 
      "+\tlocal_irq_disable();", 
      "+\tret = __migrate_task(p, src_cpu, dest_cpu);", 
      "+\tlocal_irq_enable();", 
      "+\treturn ret;", 
      "+}", 
      "+", 
      "/*", 
      "- * Figure out where task on dead CPU should go, use force if neccessary.", 
      "+ * Figure out where task on dead CPU should go, use force if necessary.", 
      "* NOTE: interrupts should be disabled by the caller", 
      "*/", 
      "static void move_task_off_dead_cpu(int dead_cpu, struct task_struct *p)", 
      "@@ -5112,35 +5238,43 @@ static void move_task_off_dead_cpu(int d", 
      "struct rq *rq;", 
      "int dest_cpu;", 
      "", 
      "-restart:", 
      "-\t/* On same node? */", 
      "-\tmask = node_to_cpumask(cpu_to_node(dead_cpu));", 
      "-\tcpus_and(mask, mask, p->cpus_allowed);", 
      "-\tdest_cpu = any_online_cpu(mask);", 
      "-", 
      "-\t/* On any allowed CPU? */", 
      "-\tif (dest_cpu == NR_CPUS)", 
      "-\t\tdest_cpu = any_online_cpu(p->cpus_allowed);", 
      "+\tdo {", 
      "+\t\t/* On same node? */", 
      "+\t\tmask = node_to_cpumask(cpu_to_node(dead_cpu));", 
      "+\t\tcpus_and(mask, mask, p->cpus_allowed);", 
      "+\t\tdest_cpu = any_online_cpu(mask);", 
      "+", 
      "+\t\t/* On any allowed CPU? */", 
      "+\t\tif (dest_cpu == NR_CPUS)", 
      "+\t\t\tdest_cpu = any_online_cpu(p->cpus_allowed);", 
      "+", 
      "+\t\t/* No more Mr. Nice Guy. */", 
      "+\t\tif (dest_cpu == NR_CPUS) {", 
      "+\t\t\tcpumask_t cpus_allowed = cpuset_cpus_allowed_locked(p);", 
      "+\t\t\t/*", 
      "+\t\t\t * Try to stay on the same cpuset, where the", 
      "+\t\t\t * current cpuset may be a subset of all cpus.", 
      "+\t\t\t * The cpuset_cpus_allowed_locked() variant of", 
      "+\t\t\t * cpuset_cpus_allowed() will not block. It must be", 
      "+\t\t\t * called within calls to cpuset_lock/cpuset_unlock.", 
      "+\t\t\t */", 
      "+\t\t\trq = task_rq_lock(p, &flags);", 
      "+\t\t\tp->cpus_allowed = cpus_allowed;", 
      "+\t\t\tdest_cpu = any_online_cpu(p->cpus_allowed);", 
      "+\t\t\ttask_rq_unlock(rq, &flags);", 
      "", 
      "-\t/* No more Mr. Nice Guy. */", 
      "-\tif (dest_cpu == NR_CPUS) {", 
      "-\t\trq = task_rq_lock(p, &flags);", 
      "-\t\tcpus_setall(p->cpus_allowed);", 
      "-\t\tdest_cpu = any_online_cpu(p->cpus_allowed);", 
      "-\t\ttask_rq_unlock(rq, &flags);", 
      "-", 
      "-\t\t/*", 
      "-\t\t * Don't tell them about moving exiting tasks or", 
      "-\t\t * kernel threads (both mm NULL), since they never", 
      "-\t\t * leave kernel.", 
      "-\t\t */", 
      "-\t\tif (p->mm && printk_ratelimit())", 
      "-\t\t\tprintk(KERN_INFO \"process %d (%s) no \"", 
      "-\t\t\t       \"longer affine to cpu%d\\n\",", 
      "-\t\t\t       p->pid, p->comm, dead_cpu);", 
      "-\t}", 
      "-\tif (!__migrate_task(p, dead_cpu, dest_cpu))", 
      "-\t\tgoto restart;", 
      "+\t\t\t/*", 
      "+\t\t\t * Don't tell them about moving exiting tasks or", 
      "+\t\t\t * kernel threads (both mm NULL), since they never", 
      "+\t\t\t * leave kernel.", 
      "+\t\t\t */", 
      "+\t\t\tif (p->mm && printk_ratelimit()) {", 
      "+\t\t\t\tprintk(KERN_INFO \"process %d (%s) no \"", 
      "+\t\t\t\t       \"longer affine to cpu%d\\n\",", 
      "+\t\t\t\t\ttask_pid_nr(p), p->comm, dead_cpu);", 
      "+\t\t\t}", 
      "+\t\t}", 
      "+\t} while (!__migrate_task_irq(p, dead_cpu, dest_cpu));", 
      "}", 
      "", 
      "/*", 
      "@@ -5168,7 +5302,7 @@ static void migrate_live_tasks(int src_c", 
      "{", 
      "struct task_struct *p, *t;", 
      "", 
      "-\twrite_lock_irq(&tasklist_lock);", 
      "+\tread_lock(&tasklist_lock);", 
      "", 
      "do_each_thread(t, p) {", 
      "if (p == current)", 
      "@@ -5178,13 +5312,13 @@ static void migrate_live_tasks(int src_c", 
      "move_task_off_dead_cpu(src_cpu, p);", 
      "} while_each_thread(t, p);", 
      "", 
      "-\twrite_unlock_irq(&tasklist_lock);", 
      "+\tread_unlock(&tasklist_lock);", 
      "}", 
      "", 
      "/*", 
      "* Schedules idle task to be the next runnable task on current CPU.", 
      "- * It does so by boosting its priority to highest possible and adding it to", 
      "- * the _front_ of the runqueue. Used by CPU offline code.", 
      "+ * It does so by boosting its priority to highest possible.", 
      "+ * Used by CPU offline code.", 
      "*/", 
      "void sched_idle_next(void)", 
      "{", 
      "@@ -5204,8 +5338,8 @@ void sched_idle_next(void)", 
      "", 
      "__setscheduler(rq, p, SCHED_FIFO, MAX_RT_PRIO-1);", 
      "", 
      "-\t/* Add idle task to the _front_ of its priority queue: */", 
      "-\tactivate_idle_task(p, rq);", 
      "+\tupdate_rq_clock(rq);", 
      "+\tactivate_task(rq, p, 0);", 
      "", 
      "spin_unlock_irqrestore(&rq->lock, flags);", 
      "}", 
      "@@ -5231,7 +5365,7 @@ static void migrate_dead(unsigned int de", 
      "struct rq *rq = cpu_rq(dead_cpu);", 
      "", 
      "/* Must be exiting, otherwise would be on tasklist. */", 
      "-\tBUG_ON(p->exit_state != EXIT_ZOMBIE && p->exit_state != EXIT_DEAD);", 
      "+\tBUG_ON(!p->exit_state);", 
      "", 
      "/* Cannot have done final schedule yet: would have vanished. */", 
      "BUG_ON(p->state == TASK_DEAD);", 
      "@@ -5240,13 +5374,12 @@ static void migrate_dead(unsigned int de", 
      "", 
      "/*", 
      "* Drop lock around migration; if someone else moves it,", 
      "-\t * that's OK.  No task can be added to this CPU, so iteration is", 
      "+\t * that's OK. No task can be added to this CPU, so iteration is", 
      "* fine.", 
      "-\t * NOTE: interrupts should be left disabled  --dev@", 
      "*/", 
      "-\tspin_unlock(&rq->lock);", 
      "+\tspin_unlock_irq(&rq->lock);", 
      "move_task_off_dead_cpu(dead_cpu, p);", 
      "-\tspin_lock(&rq->lock);", 
      "+\tspin_lock_irq(&rq->lock);", 
      "", 
      "put_task_struct(p);", 
      "}", 
      "@@ -5277,7 +5410,7 @@ static struct ctl_table sd_ctl_dir[] = {", 
      ".procname\t= \"sched_domain\",", 
      ".mode\t\t= 0555,", 
      "},", 
      "-\t{0,},", 
      "+\t{0, },", 
      "};", 
      "", 
      "static struct ctl_table sd_ctl_root[] = {", 
      "@@ -5287,20 +5420,38 @@ static struct ctl_table sd_ctl_root[] =", 
      ".mode\t\t= 0555,", 
      ".child\t\t= sd_ctl_dir,", 
      "},", 
      "-\t{0,},", 
      "+\t{0, },", 
      "};", 
      "", 
      "static struct ctl_table *sd_alloc_ctl_entry(int n)", 
      "{", 
      "struct ctl_table *entry =", 
      "-\t\tkmalloc(n * sizeof(struct ctl_table), GFP_KERNEL);", 
      "-", 
      "-\tBUG_ON(!entry);", 
      "-\tmemset(entry, 0, n * sizeof(struct ctl_table));", 
      "+\t\tkcalloc(n, sizeof(struct ctl_table), GFP_KERNEL);", 
      "", 
      "return entry;", 
      "}", 
      "", 
      "+static void sd_free_ctl_entry(struct ctl_table **tablep)", 
      "+{", 
      "+\tstruct ctl_table *entry;", 
      "+", 
      "+\t/*", 
      "+\t * In the intermediate directories, both the child directory and", 
      "+\t * procname are dynamically allocated and could fail but the mode", 
      "+\t * will always be set. In the lowest directory the names are", 
      "+\t * static strings and all have proc handlers.", 
      "+\t */", 
      "+\tfor (entry = *tablep; entry->mode; entry++) {", 
      "+\t\tif (entry->child)", 
      "+\t\t\tsd_free_ctl_entry(&entry->child);", 
      "+\t\tif (entry->proc_handler == NULL)", 
      "+\t\t\tkfree(entry->procname);", 
      "+\t}", 
      "+", 
      "+\tkfree(*tablep);", 
      "+\t*tablep = NULL;", 
      "+}", 
      "+", 
      "static void", 
      "set_table_entry(struct ctl_table *entry,", 
      "const char *procname, void *data, int maxlen,", 
      "@@ -5318,6 +5469,9 @@ sd_alloc_ctl_domain_table(struct sched_d", 
      "{", 
      "struct ctl_table *table = sd_alloc_ctl_entry(12);", 
      "", 
      "+\tif (table == NULL)", 
      "+\t\treturn NULL;", 
      "+", 
      "set_table_entry(&table[0], \"min_interval\", &sd->min_interval,", 
      "sizeof(long), 0644, proc_doulongvec_minmax);", 
      "set_table_entry(&table[1], \"max_interval\", &sd->max_interval,", 
      "@@ -5341,6 +5495,7 @@ sd_alloc_ctl_domain_table(struct sched_d", 
      "sizeof(int), 0644, proc_dointvec_minmax);", 
      "set_table_entry(&table[10], \"flags\", &sd->flags,", 
      "sizeof(int), 0644, proc_dointvec_minmax);", 
      "+\t/* &table[11] is terminator */", 
      "", 
      "return table;", 
      "}", 
      "@@ -5355,6 +5510,8 @@ static ctl_table *sd_alloc_ctl_cpu_table", 
      "for_each_domain(cpu, sd)", 
      "domain_num++;", 
      "entry = table = sd_alloc_ctl_entry(domain_num + 1);", 
      "+\tif (table == NULL)", 
      "+\t\treturn NULL;", 
      "", 
      "i = 0;", 
      "for_each_domain(cpu, sd) {", 
      "@@ -5369,24 +5526,44 @@ static ctl_table *sd_alloc_ctl_cpu_table", 
      "}", 
      "", 
      "static struct ctl_table_header *sd_sysctl_header;", 
      "-static void init_sched_domain_sysctl(void)", 
      "+static void register_sched_domain_sysctl(void)", 
      "{", 
      "int i, cpu_num = num_online_cpus();", 
      "struct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);", 
      "char buf[32];", 
      "", 
      "+\tWARN_ON(sd_ctl_dir[0].child);", 
      "sd_ctl_dir[0].child = entry;", 
      "", 
      "-\tfor (i = 0; i < cpu_num; i++, entry++) {", 
      "+\tif (entry == NULL)", 
      "+\t\treturn;", 
      "+", 
      "+\tfor_each_online_cpu(i) {", 
      "snprintf(buf, 32, \"cpu%d\", i);", 
      "entry->procname = kstrdup(buf, GFP_KERNEL);", 
      "entry->mode = 0555;", 
      "entry->child = sd_alloc_ctl_cpu_table(i);", 
      "+\t\tentry++;", 
      "}", 
      "+", 
      "+\tWARN_ON(sd_sysctl_header);", 
      "sd_sysctl_header = register_sysctl_table(sd_ctl_root);", 
      "}", 
      "+", 
      "+/* may be called multiple times per register */", 
      "+static void unregister_sched_domain_sysctl(void)", 
      "+{", 
      "+\tif (sd_sysctl_header)", 
      "+\t\tunregister_sysctl_table(sd_sysctl_header);", 
      "+\tsd_sysctl_header = NULL;", 
      "+\tif (sd_ctl_dir[0].child)", 
      "+\t\tsd_free_ctl_entry(&sd_ctl_dir[0].child);", 
      "+}", 
      "#else", 
      "-static void init_sched_domain_sysctl(void)", 
      "+static void register_sched_domain_sysctl(void)", 
      "+{", 
      "+}", 
      "+static void unregister_sched_domain_sysctl(void)", 
      "{", 
      "}", 
      "#endif", 
      "@@ -5413,6 +5590,7 @@ migration_call(struct notifier_block *nf", 
      "p = kthread_create(migration_thread, hcpu, \"migration/%d\", cpu);", 
      "if (IS_ERR(p))", 
      "return NOTIFY_BAD;", 
      "+\t\tp->flags |= PF_NOFREEZE;", 
      "kthread_bind(p, cpu);", 
      "/* Must be high prio: stop_machine expects to yield to it. */", 
      "rq = task_rq_lock(p, &flags);", 
      "@@ -5423,7 +5601,7 @@ migration_call(struct notifier_block *nf", 
      "", 
      "case CPU_ONLINE:", 
      "case CPU_ONLINE_FROZEN:", 
      "-\t\t/* Strictly unneccessary, as first user will wake it. */", 
      "+\t\t/* Strictly unnecessary, as first user will wake it. */", 
      "wake_up_process(cpu_rq(cpu)->migration_thread);", 
      "break;", 
      "", 
      "@@ -5432,7 +5610,7 @@ migration_call(struct notifier_block *nf", 
      "case CPU_UP_CANCELED_FROZEN:", 
      "if (!cpu_rq(cpu)->migration_thread)", 
      "break;", 
      "-\t\t/* Unbind it from offline cpu so it can run.  Fall thru. */", 
      "+\t\t/* Unbind it from offline cpu so it can run. Fall thru. */", 
      "kthread_bind(cpu_rq(cpu)->migration_thread,", 
      "any_online_cpu(cpu_online_map));", 
      "kthread_stop(cpu_rq(cpu)->migration_thread);", 
      "@@ -5441,25 +5619,29 @@ migration_call(struct notifier_block *nf", 
      "", 
      "case CPU_DEAD:", 
      "case CPU_DEAD_FROZEN:", 
      "+\t\tcpuset_lock(); /* around calls to cpuset_cpus_allowed_lock() */", 
      "migrate_live_tasks(cpu);", 
      "rq = cpu_rq(cpu);", 
      "kthread_stop(rq->migration_thread);", 
      "rq->migration_thread = NULL;", 
      "/* Idle task back to normal (off runqueue, low prio) */", 
      "-\t\trq = task_rq_lock(rq->idle, &flags);", 
      "+\t\tspin_lock_irq(&rq->lock);", 
      "update_rq_clock(rq);", 
      "deactivate_task(rq, rq->idle, 0);", 
      "rq->idle->static_prio = MAX_PRIO;", 
      "__setscheduler(rq, rq->idle, SCHED_NORMAL, 0);", 
      "rq->idle->sched_class = &idle_sched_class;", 
      "migrate_dead_tasks(cpu);", 
      "-\t\ttask_rq_unlock(rq, &flags);", 
      "+\t\tspin_unlock_irq(&rq->lock);", 
      "+\t\tcpuset_unlock();", 
      "migrate_nr_uninterruptible(rq);", 
      "BUG_ON(rq->nr_running != 0);", 
      "", 
      "-\t\t/* No need to migrate the tasks: it was best-effort if", 
      "-\t\t * they didn't take sched_hotcpu_mutex.  Just wake up", 
      "-\t\t * the requestors. */", 
      "+\t\t/*", 
      "+\t\t * No need to migrate the tasks: it was best-effort if", 
      "+\t\t * they didn't take sched_hotcpu_mutex. Just wake up", 
      "+\t\t * the requestors.", 
      "+\t\t */", 
      "spin_lock_irq(&rq->lock);", 
      "while (!list_empty(&rq->migration_queue)) {", 
      "struct migration_req *req;", 
      "@@ -5487,7 +5669,7 @@ static struct notifier_block __cpuinitda", 
      ".priority = 10", 
      "};", 
      "", 
      "-int __init migration_init(void)", 
      "+void __init migration_init(void)", 
      "{", 
      "void *cpu = (void *)(long)smp_processor_id();", 
      "int err;", 
      "@@ -5497,8 +5679,6 @@ int __init migration_init(void)", 
      "BUG_ON(err == NOTIFY_BAD);", 
      "migration_call(&migration_notifier, CPU_ONLINE, cpu);", 
      "register_cpu_notifier(&migration_notifier);", 
      "-", 
      "-\treturn 0;", 
      "}", 
      "#endif", 
      "", 
      "@@ -5508,100 +5688,102 @@ int __init migration_init(void)", 
      "int nr_cpu_ids __read_mostly = NR_CPUS;", 
      "EXPORT_SYMBOL(nr_cpu_ids);", 
      "", 
      "-#undef SCHED_DOMAIN_DEBUG", 
      "-#ifdef SCHED_DOMAIN_DEBUG", 
      "-static void sched_domain_debug(struct sched_domain *sd, int cpu)", 
      "-{", 
      "-\tint level = 0;", 
      "+#ifdef CONFIG_SCHED_DEBUG", 
      "", 
      "-\tif (!sd) {", 
      "-\t\tprintk(KERN_DEBUG \"CPU%d attaching NULL sched-domain.\\n\", cpu);", 
      "-\t\treturn;", 
      "+static int sched_domain_debug_one(struct sched_domain *sd, int cpu, int level)", 
      "+{", 
      "+\tstruct sched_group *group = sd->groups;", 
      "+\tcpumask_t groupmask;", 
      "+\tchar str[NR_CPUS];", 
      "+", 
      "+\tcpumask_scnprintf(str, NR_CPUS, sd->span);", 
      "+\tcpus_clear(groupmask);", 
      "+", 
      "+\tprintk(KERN_DEBUG \"%*s domain %d: \", level, \"\", level);", 
      "+", 
      "+\tif (!(sd->flags & SD_LOAD_BALANCE)) {", 
      "+\t\tprintk(\"does not load-balance\\n\");", 
      "+\t\tif (sd->parent)", 
      "+\t\t\tprintk(KERN_ERR \"ERROR: !SD_LOAD_BALANCE domain\"", 
      "+\t\t\t\t\t\" has parent\");", 
      "+\t\treturn -1;", 
      "}", 
      "", 
      "-\tprintk(KERN_DEBUG \"CPU%d attaching sched-domain:\\n\", cpu);", 
      "+\tprintk(KERN_CONT \"span %s\\n\", str);", 
      "", 
      "+\tif (!cpu_isset(cpu, sd->span)) {", 
      "+\t\tprintk(KERN_ERR \"ERROR: domain->span does not contain \"", 
      "+\t\t\t\t\"CPU%d\\n\", cpu);", 
      "+\t}", 
      "+\tif (!cpu_isset(cpu, group->cpumask)) {", 
      "+\t\tprintk(KERN_ERR \"ERROR: domain->groups does not contain\"", 
      "+\t\t\t\t\" CPU%d\\n\", cpu);", 
      "+\t}", 
      "+", 
      "+\tprintk(KERN_DEBUG \"%*s groups:\", level + 1, \"\");", 
      "do {", 
      "-\t\tint i;", 
      "-\t\tchar str[NR_CPUS];", 
      "-\t\tstruct sched_group *group = sd->groups;", 
      "-\t\tcpumask_t groupmask;", 
      "-", 
      "-\t\tcpumask_scnprintf(str, NR_CPUS, sd->span);", 
      "-\t\tcpus_clear(groupmask);", 
      "-", 
      "-\t\tprintk(KERN_DEBUG);", 
      "-\t\tfor (i = 0; i < level + 1; i++)", 
      "-\t\t\tprintk(\" \");", 
      "-\t\tprintk(\"domain %d: \", level);", 
      "-", 
      "-\t\tif (!(sd->flags & SD_LOAD_BALANCE)) {", 
      "-\t\t\tprintk(\"does not load-balance\\n\");", 
      "-\t\t\tif (sd->parent)", 
      "-\t\t\t\tprintk(KERN_ERR \"ERROR: !SD_LOAD_BALANCE domain\"", 
      "-\t\t\t\t\t\t\" has parent\");", 
      "+\t\tif (!group) {", 
      "+\t\t\tprintk(\"\\n\");", 
      "+\t\t\tprintk(KERN_ERR \"ERROR: group is NULL\\n\");", 
      "break;", 
      "}", 
      "", 
      "-\t\tprintk(\"span %s\\n\", str);", 
      "+\t\tif (!group->__cpu_power) {", 
      "+\t\t\tprintk(KERN_CONT \"\\n\");", 
      "+\t\t\tprintk(KERN_ERR \"ERROR: domain->cpu_power not \"", 
      "+\t\t\t\t\t\"set\\n\");", 
      "+\t\t\tbreak;", 
      "+\t\t}", 
      "", 
      "-\t\tif (!cpu_isset(cpu, sd->span))", 
      "-\t\t\tprintk(KERN_ERR \"ERROR: domain->span does not contain \"", 
      "-\t\t\t\t\t\"CPU%d\\n\", cpu);", 
      "-\t\tif (!cpu_isset(cpu, group->cpumask))", 
      "-\t\t\tprintk(KERN_ERR \"ERROR: domain->groups does not contain\"", 
      "-\t\t\t\t\t\" CPU%d\\n\", cpu);", 
      "-", 
      "-\t\tprintk(KERN_DEBUG);", 
      "-\t\tfor (i = 0; i < level + 2; i++)", 
      "-\t\t\tprintk(\" \");", 
      "-\t\tprintk(\"groups:\");", 
      "-\t\tdo {", 
      "-\t\t\tif (!group) {", 
      "-\t\t\t\tprintk(\"\\n\");", 
      "-\t\t\t\tprintk(KERN_ERR \"ERROR: group is NULL\\n\");", 
      "-\t\t\t\tbreak;", 
      "-\t\t\t}", 
      "+\t\tif (!cpus_weight(group->cpumask)) {", 
      "+\t\t\tprintk(KERN_CONT \"\\n\");", 
      "+\t\t\tprintk(KERN_ERR \"ERROR: empty group\\n\");", 
      "+\t\t\tbreak;", 
      "+\t\t}", 
      "", 
      "-\t\t\tif (!group->__cpu_power) {", 
      "-\t\t\t\tprintk(\"\\n\");", 
      "-\t\t\t\tprintk(KERN_ERR \"ERROR: domain->cpu_power not \"", 
      "-\t\t\t\t\t\t\"set\\n\");", 
      "-\t\t\t}", 
      "+\t\tif (cpus_intersects(groupmask, group->cpumask)) {", 
      "+\t\t\tprintk(KERN_CONT \"\\n\");", 
      "+\t\t\tprintk(KERN_ERR \"ERROR: repeated CPUs\\n\");", 
      "+\t\t\tbreak;", 
      "+\t\t}", 
      "", 
      "-\t\t\tif (!cpus_weight(group->cpumask)) {", 
      "-\t\t\t\tprintk(\"\\n\");", 
      "-\t\t\t\tprintk(KERN_ERR \"ERROR: empty group\\n\");", 
      "-\t\t\t}", 
      "+\t\tcpus_or(groupmask, groupmask, group->cpumask);", 
      "", 
      "-\t\t\tif (cpus_intersects(groupmask, group->cpumask)) {", 
      "-\t\t\t\tprintk(\"\\n\");", 
      "-\t\t\t\tprintk(KERN_ERR \"ERROR: repeated CPUs\\n\");", 
      "-\t\t\t}", 
      "+\t\tcpumask_scnprintf(str, NR_CPUS, group->cpumask);", 
      "+\t\tprintk(KERN_CONT \" %s\", str);", 
      "+", 
      "+\t\tgroup = group->next;", 
      "+\t} while (group != sd->groups);", 
      "+\tprintk(KERN_CONT \"\\n\");", 
      "+", 
      "+\tif (!cpus_equal(sd->span, groupmask))", 
      "+\t\tprintk(KERN_ERR \"ERROR: groups don't span domain->span\\n\");", 
      "+", 
      "+\tif (sd->parent && !cpus_subset(groupmask, sd->parent->span))", 
      "+\t\tprintk(KERN_ERR \"ERROR: parent span is not a superset \"", 
      "+\t\t\t\"of domain->span\\n\");", 
      "+\treturn 0;", 
      "+}", 
      "", 
      "-\t\t\tcpus_or(groupmask, groupmask, group->cpumask);", 
      "+static void sched_domain_debug(struct sched_domain *sd, int cpu)", 
      "+{", 
      "+\tint level = 0;", 
      "", 
      "-\t\t\tcpumask_scnprintf(str, NR_CPUS, group->cpumask);", 
      "-\t\t\tprintk(\" %s\", str);", 
      "+\tif (!sd) {", 
      "+\t\tprintk(KERN_DEBUG \"CPU%d attaching NULL sched-domain.\\n\", cpu);", 
      "+\t\treturn;", 
      "+\t}", 
      "", 
      "-\t\t\tgroup = group->next;", 
      "-\t\t} while (group != sd->groups);", 
      "-\t\tprintk(\"\\n\");", 
      "-", 
      "-\t\tif (!cpus_equal(sd->span, groupmask))", 
      "-\t\t\tprintk(KERN_ERR \"ERROR: groups don't span \"", 
      "-\t\t\t\t\t\"domain->span\\n\");", 
      "+\tprintk(KERN_DEBUG \"CPU%d attaching sched-domain:\\n\", cpu);", 
      "", 
      "+\tfor (;;) {", 
      "+\t\tif (sched_domain_debug_one(sd, cpu, level))", 
      "+\t\t\tbreak;", 
      "level++;", 
      "sd = sd->parent;", 
      "if (!sd)", 
      "-\t\t\tcontinue;", 
      "-", 
      "-\t\tif (!cpus_subset(groupmask, sd->span))", 
      "-\t\t\tprintk(KERN_ERR \"ERROR: parent span is not a superset \"", 
      "-\t\t\t\t\"of domain->span\\n\");", 
      "-", 
      "-\t} while (sd);", 
      "+\t\t\tbreak;", 
      "+\t}", 
      "}", 
      "#else", 
      "# define sched_domain_debug(sd, cpu) do { } while (0)", 
      "@@ -5710,7 +5892,7 @@ static int __init isolated_cpu_setup(cha", 
      "return 1;", 
      "}", 
      "", 
      "-__setup (\"isolcpus=\", isolated_cpu_setup);", 
      "+__setup(\"isolcpus=\", isolated_cpu_setup);", 
      "", 
      "/*", 
      "* init_sched_build_groups takes the cpumask we wish to span, and a pointer", 
      "@@ -5767,7 +5949,7 @@ init_sched_build_groups(cpumask_t span,", 
      "* @node: node whose sched_domain we're building", 
      "* @used_nodes: nodes already in the sched_domain", 
      "*", 
      "- * Find the next node to include in a given scheduling domain.  Simply", 
      "+ * Find the next node to include in a given scheduling domain. Simply", 
      "* finds the closest node not already in the @used_nodes map.", 
      "*", 
      "* Should use nodemask_t.", 
      "@@ -5807,7 +5989,7 @@ static int find_next_best_node(int node,", 
      "* @node: node whose cpumask we're constructing", 
      "* @size: number of nodes to include in this span", 
      "*", 
      "- * Given a node, construct a good cpumask for its sched_domain to span.  It", 
      "+ * Given a node, construct a good cpumask for its sched_domain to span. It", 
      "* should be one that prevents unnecessary balancing, but also spreads tasks", 
      "* out optimally.", 
      "*/", 
      "@@ -5844,8 +6026,8 @@ int sched_smt_power_savings = 0, sched_m", 
      "static DEFINE_PER_CPU(struct sched_domain, cpu_domains);", 
      "static DEFINE_PER_CPU(struct sched_group, sched_group_cpus);", 
      "", 
      "-static int cpu_to_cpu_group(int cpu, const cpumask_t *cpu_map,", 
      "-\t\t\t    struct sched_group **sg)", 
      "+static int", 
      "+cpu_to_cpu_group(int cpu, const cpumask_t *cpu_map, struct sched_group **sg)", 
      "{", 
      "if (sg)", 
      "*sg = &per_cpu(sched_group_cpus, cpu);", 
      "@@ -5862,11 +6044,11 @@ static DEFINE_PER_CPU(struct sched_group", 
      "#endif", 
      "", 
      "#if defined(CONFIG_SCHED_MC) && defined(CONFIG_SCHED_SMT)", 
      "-static int cpu_to_core_group(int cpu, const cpumask_t *cpu_map,", 
      "-\t\t\t     struct sched_group **sg)", 
      "+static int", 
      "+cpu_to_core_group(int cpu, const cpumask_t *cpu_map, struct sched_group **sg)", 
      "{", 
      "int group;", 
      "-\tcpumask_t mask = cpu_sibling_map[cpu];", 
      "+\tcpumask_t mask = cpu_sibling_map(cpu);", 
      "cpus_and(mask, mask, *cpu_map);", 
      "group = first_cpu(mask);", 
      "if (sg)", 
      "@@ -5874,8 +6056,8 @@ static int cpu_to_core_group(int cpu, co", 
      "return group;", 
      "}", 
      "#elif defined(CONFIG_SCHED_MC)", 
      "-static int cpu_to_core_group(int cpu, const cpumask_t *cpu_map,", 
      "-\t\t\t     struct sched_group **sg)", 
      "+static int", 
      "+cpu_to_core_group(int cpu, const cpumask_t *cpu_map, struct sched_group **sg)", 
      "{", 
      "if (sg)", 
      "*sg = &per_cpu(sched_group_core, cpu);", 
      "@@ -5886,8 +6068,8 @@ static int cpu_to_core_group(int cpu, co", 
      "static DEFINE_PER_CPU(struct sched_domain, phys_domains);", 
      "static DEFINE_PER_CPU(struct sched_group, sched_group_phys);", 
      "", 
      "-static int cpu_to_phys_group(int cpu, const cpumask_t *cpu_map,", 
      "-\t\t\t     struct sched_group **sg)", 
      "+static int", 
      "+cpu_to_phys_group(int cpu, const cpumask_t *cpu_map, struct sched_group **sg)", 
      "{", 
      "int group;", 
      "#ifdef CONFIG_SCHED_MC", 
      "@@ -5895,7 +6077,7 @@ static int cpu_to_phys_group(int cpu, co", 
      "cpus_and(mask, mask, *cpu_map);", 
      "group = first_cpu(mask);", 
      "#elif defined(CONFIG_SCHED_SMT)", 
      "-\tcpumask_t mask = cpu_sibling_map[cpu];", 
      "+\tcpumask_t mask = cpu_sibling_map(cpu);", 
      "cpus_and(mask, mask, *cpu_map);", 
      "group = first_cpu(mask);", 
      "#else", 
      "@@ -5939,24 +6121,23 @@ static void init_numa_sched_groups_power", 
      "", 
      "if (!sg)", 
      "return;", 
      "-next_sg:", 
      "-\tfor_each_cpu_mask(j, sg->cpumask) {", 
      "-\t\tstruct sched_domain *sd;", 
      "+\tdo {", 
      "+\t\tfor_each_cpu_mask(j, sg->cpumask) {", 
      "+\t\t\tstruct sched_domain *sd;", 
      "", 
      "-\t\tsd = &per_cpu(phys_domains, j);", 
      "-\t\tif (j != first_cpu(sd->groups->cpumask)) {", 
      "-\t\t\t/*", 
      "-\t\t\t * Only add \"power\" once for each", 
      "-\t\t\t * physical package.", 
      "-\t\t\t */", 
      "-\t\t\tcontinue;", 
      "-\t\t}", 
      "+\t\t\tsd = &per_cpu(phys_domains, j);", 
      "+\t\t\tif (j != first_cpu(sd->groups->cpumask)) {", 
      "+\t\t\t\t/*", 
      "+\t\t\t\t * Only add \"power\" once for each", 
      "+\t\t\t\t * physical package.", 
      "+\t\t\t\t */", 
      "+\t\t\t\tcontinue;", 
      "+\t\t\t}", 
      "", 
      "-\t\tsg_inc_cpu_power(sg, sd->groups->__cpu_power);", 
      "-\t}", 
      "-\tsg = sg->next;", 
      "-\tif (sg != group_head)", 
      "-\t\tgoto next_sg;", 
      "+\t\t\tsg_inc_cpu_power(sg, sd->groups->__cpu_power);", 
      "+\t\t}", 
      "+\t\tsg = sg->next;", 
      "+\t} while (sg != group_head);", 
      "}", 
      "#endif", 
      "", 
      "@@ -6067,8 +6248,8 @@ static int build_sched_domains(const cpu", 
      "/*", 
      "* Allocate the per-node list of sched groups", 
      "*/", 
      "-\tsched_group_nodes = kzalloc(sizeof(struct sched_group *)*MAX_NUMNODES,", 
      "-\t\t\t\t\t   GFP_KERNEL);", 
      "+\tsched_group_nodes = kcalloc(MAX_NUMNODES, sizeof(struct sched_group *),", 
      "+\t\t\t\t    GFP_KERNEL);", 
      "if (!sched_group_nodes) {", 
      "printk(KERN_WARNING \"Can not alloc sched group node list\\n\");", 
      "return -ENOMEM;", 
      "@@ -6130,7 +6311,7 @@ static int build_sched_domains(const cpu", 
      "p = sd;", 
      "sd = &per_cpu(cpu_domains, i);", 
      "*sd = SD_SIBLING_INIT;", 
      "-\t\tsd->span = cpu_sibling_map[i];", 
      "+\t\tsd->span = cpu_sibling_map(i);", 
      "cpus_and(sd->span, sd->span, *cpu_map);", 
      "sd->parent = p;", 
      "p->child = sd;", 
      "@@ -6141,7 +6322,7 @@ static int build_sched_domains(const cpu", 
      "#ifdef CONFIG_SCHED_SMT", 
      "/* Set up CPU (sibling) groups */", 
      "for_each_cpu_mask(i, *cpu_map) {", 
      "-\t\tcpumask_t this_sibling_map = cpu_sibling_map[i];", 
      "+\t\tcpumask_t this_sibling_map = cpu_sibling_map(i);", 
      "cpus_and(this_sibling_map, this_sibling_map, *cpu_map);", 
      "if (i != first_cpu(this_sibling_map))", 
      "continue;", 
      "@@ -6303,22 +6484,33 @@ error:", 
      "return -ENOMEM;", 
      "#endif", 
      "}", 
      "+", 
      "+static cpumask_t *doms_cur;\t/* current sched domains */", 
      "+static int ndoms_cur;\t\t/* number of sched domains in 'doms_cur' */", 
      "+", 
      "+/*", 
      "+ * Special case: If a kmalloc of a doms_cur partition (array of", 
      "+ * cpumask_t) fails, then fallback to a single sched domain,", 
      "+ * as determined by the single cpumask_t fallback_doms.", 
      "+ */", 
      "+static cpumask_t fallback_doms;", 
      "+", 
      "/*", 
      "- * Set up scheduler domains and groups.  Callers must hold the hotplug lock.", 
      "+ * Set up scheduler domains and groups. Callers must hold the hotplug lock.", 
      "+ * For now this just excludes isolated cpus, but could be used to", 
      "+ * exclude other special cases in the future.", 
      "*/", 
      "static int arch_init_sched_domains(const cpumask_t *cpu_map)", 
      "{", 
      "-\tcpumask_t cpu_default_map;", 
      "int err;", 
      "", 
      "-\t/*", 
      "-\t * Setup mask for cpus without special case scheduling requirements.", 
      "-\t * For now this just excludes isolated cpus, but could be used to", 
      "-\t * exclude other special cases in the future.", 
      "-\t */", 
      "-\tcpus_andnot(cpu_default_map, *cpu_map, cpu_isolated_map);", 
      "-", 
      "-\terr = build_sched_domains(&cpu_default_map);", 
      "+\tndoms_cur = 1;", 
      "+\tdoms_cur = kmalloc(sizeof(cpumask_t), GFP_KERNEL);", 
      "+\tif (!doms_cur)", 
      "+\t\tdoms_cur = &fallback_doms;", 
      "+\tcpus_andnot(*doms_cur, *cpu_map, cpu_isolated_map);", 
      "+\terr = build_sched_domains(doms_cur);", 
      "+\tregister_sched_domain_sysctl();", 
      "", 
      "return err;", 
      "}", 
      "@@ -6336,6 +6528,8 @@ static void detach_destroy_domains(const", 
      "{", 
      "int i;", 
      "", 
      "+\tunregister_sched_domain_sysctl();", 
      "+", 
      "for_each_cpu_mask(i, *cpu_map)", 
      "cpu_attach_domain(NULL, i);", 
      "synchronize_sched();", 
      "@@ -6343,30 +6537,70 @@ static void detach_destroy_domains(const", 
      "}", 
      "", 
      "/*", 
      "- * Partition sched domains as specified by the cpumasks below.", 
      "- * This attaches all cpus from the cpumasks to the NULL domain,", 
      "- * waits for a RCU quiescent period, recalculates sched", 
      "- * domain information and then attaches them back to the", 
      "- * correct sched domains", 
      "+ * Partition sched domains as specified by the 'ndoms_new'", 
      "+ * cpumasks in the array doms_new[] of cpumasks. This compares", 
      "+ * doms_new[] to the current sched domain partitioning, doms_cur[].", 
      "+ * It destroys each deleted domain and builds each new domain.", 
      "+ *", 
      "+ * 'doms_new' is an array of cpumask_t's of length 'ndoms_new'.", 
      "+ * The masks don't intersect (don't overlap.) We should setup one", 
      "+ * sched domain for each mask. CPUs not in any of the cpumasks will", 
      "+ * not be load balanced. If the same cpumask appears both in the", 
      "+ * current 'doms_cur' domains and in the new 'doms_new', we can leave", 
      "+ * it as it is.", 
      "+ *", 
      "+ * The passed in 'doms_new' should be kmalloc'd. This routine takes", 
      "+ * ownership of it and will kfree it when done with it. If the caller", 
      "+ * failed the kmalloc call, then it can pass in doms_new == NULL,", 
      "+ * and partition_sched_domains() will fallback to the single partition", 
      "+ * 'fallback_doms'.", 
      "+ *", 
      "* Call with hotplug lock held", 
      "*/", 
      "-int partition_sched_domains(cpumask_t *partition1, cpumask_t *partition2)", 
      "+void partition_sched_domains(int ndoms_new, cpumask_t *doms_new)", 
      "{", 
      "-\tcpumask_t change_map;", 
      "-\tint err = 0;", 
      "+\tint i, j;", 
      "", 
      "-\tcpus_and(*partition1, *partition1, cpu_online_map);", 
      "-\tcpus_and(*partition2, *partition2, cpu_online_map);", 
      "-\tcpus_or(change_map, *partition1, *partition2);", 
      "-", 
      "-\t/* Detach sched domains from all of the affected cpus */", 
      "-\tdetach_destroy_domains(&change_map);", 
      "-\tif (!cpus_empty(*partition1))", 
      "-\t\terr = build_sched_domains(partition1);", 
      "-\tif (!err && !cpus_empty(*partition2))", 
      "-\t\terr = build_sched_domains(partition2);", 
      "+\t/* always unregister in case we don't destroy any domains */", 
      "+\tunregister_sched_domain_sysctl();", 
      "", 
      "-\treturn err;", 
      "+\tif (doms_new == NULL) {", 
      "+\t\tndoms_new = 1;", 
      "+\t\tdoms_new = &fallback_doms;", 
      "+\t\tcpus_andnot(doms_new[0], cpu_online_map, cpu_isolated_map);", 
      "+\t}", 
      "+", 
      "+\t/* Destroy deleted domains */", 
      "+\tfor (i = 0; i < ndoms_cur; i++) {", 
      "+\t\tfor (j = 0; j < ndoms_new; j++) {", 
      "+\t\t\tif (cpus_equal(doms_cur[i], doms_new[j]))", 
      "+\t\t\t\tgoto match1;", 
      "+\t\t}", 
      "+\t\t/* no match - a current sched domain not in new doms_new[] */", 
      "+\t\tdetach_destroy_domains(doms_cur + i);", 
      "+match1:", 
      "+\t\t;", 
      "+\t}", 
      "+", 
      "+\t/* Build new domains */", 
      "+\tfor (i = 0; i < ndoms_new; i++) {", 
      "+\t\tfor (j = 0; j < ndoms_cur; j++) {", 
      "+\t\t\tif (cpus_equal(doms_new[i], doms_cur[j]))", 
      "+\t\t\t\tgoto match2;", 
      "+\t\t}", 
      "+\t\t/* no match - add a new doms_new */", 
      "+\t\tbuild_sched_domains(doms_new + i);", 
      "+match2:", 
      "+\t\t;", 
      "+\t}", 
      "+", 
      "+\t/* Remember the new sched domains */", 
      "+\tif (doms_cur != &fallback_doms)", 
      "+\t\tkfree(doms_cur);", 
      "+\tdoms_cur = doms_new;", 
      "+\tndoms_cur = ndoms_new;", 
      "+", 
      "+\tregister_sched_domain_sysctl();", 
      "}", 
      "", 
      "#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)", 
      "@@ -6446,7 +6680,7 @@ int sched_create_sysfs_power_savings_ent", 
      "#endif", 
      "", 
      "/*", 
      "- * Force a reinitialization of the sched domains hierarchy.  The domains", 
      "+ * Force a reinitialization of the sched domains hierarchy. The domains", 
      "* and groups cannot be updated in place without racing with the balancing", 
      "* code, so we temporarily attach all running cpus to the NULL domain", 
      "* which will prevent rebalancing while the sched domains are recalculated.", 
      "@@ -6497,8 +6731,6 @@ void __init sched_init_smp(void)", 
      "/* XXX: Theoretical race here - CPU may be hotplugged now */", 
      "hotcpu_notifier(update_sched_domains, 0);", 
      "", 
      "-\tinit_sched_domain_sysctl();", 
      "-", 
      "/* Move init over to a non-isolated CPU */", 
      "if (set_cpus_allowed(current, non_isolated_cpus) < 0)", 
      "BUG();", 
      "@@ -6513,36 +6745,25 @@ void __init sched_init_smp(void)", 
      "", 
      "int in_sched_functions(unsigned long addr)", 
      "{", 
      "-\t/* Linker adds these: start and end of __sched functions */", 
      "-\textern char __sched_text_start[], __sched_text_end[];", 
      "-", 
      "return in_lock_functions(addr) ||", 
      "(addr >= (unsigned long)__sched_text_start", 
      "&& addr < (unsigned long)__sched_text_end);", 
      "}", 
      "", 
      "-static inline void init_cfs_rq(struct cfs_rq *cfs_rq, struct rq *rq)", 
      "+static void init_cfs_rq(struct cfs_rq *cfs_rq, struct rq *rq)", 
      "{", 
      "cfs_rq->tasks_timeline = RB_ROOT;", 
      "-\tcfs_rq->fair_clock = 1;", 
      "#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "cfs_rq->rq = rq;", 
      "#endif", 
      "+\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));", 
      "}", 
      "", 
      "void __init sched_init(void)", 
      "{", 
      "-\tu64 now = sched_clock();", 
      "int highest_cpu = 0;", 
      "int i, j;", 
      "", 
      "-\t/*", 
      "-\t * Link up the scheduling class hierarchy:", 
      "-\t */", 
      "-\trt_sched_class.next = &fair_sched_class;", 
      "-\tfair_sched_class.next = &idle_sched_class;", 
      "-\tidle_sched_class.next = NULL;", 
      "-", 
      "for_each_possible_cpu(i) {", 
      "struct rt_prio_array *array;", 
      "struct rq *rq;", 
      "@@ -6555,10 +6776,28 @@ void __init sched_init(void)", 
      "init_cfs_rq(&rq->cfs, rq);", 
      "#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);", 
      "-\t\tlist_add(&rq->cfs.leaf_cfs_rq_list, &rq->leaf_cfs_rq_list);", 
      "+\t\t{", 
      "+\t\t\tstruct cfs_rq *cfs_rq = &per_cpu(init_cfs_rq, i);", 
      "+\t\t\tstruct sched_entity *se =", 
      "+\t\t\t\t\t &per_cpu(init_sched_entity, i);", 
      "+", 
      "+\t\t\tinit_cfs_rq_p[i] = cfs_rq;", 
      "+\t\t\tinit_cfs_rq(cfs_rq, rq);", 
      "+\t\t\tcfs_rq->tg = &init_task_group;", 
      "+\t\t\tlist_add(&cfs_rq->leaf_cfs_rq_list,", 
      "+\t\t\t\t\t\t\t &rq->leaf_cfs_rq_list);", 
      "+", 
      "+\t\t\tinit_sched_entity_p[i] = se;", 
      "+\t\t\tse->cfs_rq = &rq->cfs;", 
      "+\t\t\tse->my_q = cfs_rq;", 
      "+\t\t\tse->load.weight = init_task_group_load;", 
      "+\t\t\tse->load.inv_weight =", 
      "+\t\t\t\t div64_64(1ULL<<32, init_task_group_load);", 
      "+\t\t\tse->parent = NULL;", 
      "+\t\t}", 
      "+\t\tinit_task_group.shares = init_task_group_load;", 
      "+\t\tspin_lock_init(&init_task_group.lock);", 
      "#endif", 
      "-\t\trq->ls.load_update_last = now;", 
      "-\t\trq->ls.load_update_start = now;", 
      "", 
      "for (j = 0; j < CPU_LOAD_IDX_MAX; j++)", 
      "rq->cpu_load[j] = 0;", 
      "@@ -6646,26 +6885,40 @@ EXPORT_SYMBOL(__might_sleep);", 
      "#endif", 
      "", 
      "#ifdef CONFIG_MAGIC_SYSRQ", 
      "+static void normalize_task(struct rq *rq, struct task_struct *p)", 
      "+{", 
      "+\tint on_rq;", 
      "+\tupdate_rq_clock(rq);", 
      "+\ton_rq = p->se.on_rq;", 
      "+\tif (on_rq)", 
      "+\t\tdeactivate_task(rq, p, 0);", 
      "+\t__setscheduler(rq, p, SCHED_NORMAL, 0);", 
      "+\tif (on_rq) {", 
      "+\t\tactivate_task(rq, p, 0);", 
      "+\t\tresched_task(rq->curr);", 
      "+\t}", 
      "+}", 
      "+", 
      "void normalize_rt_tasks(void)", 
      "{", 
      "struct task_struct *g, *p;", 
      "unsigned long flags;", 
      "struct rq *rq;", 
      "-\tint on_rq;", 
      "", 
      "read_lock_irq(&tasklist_lock);", 
      "do_each_thread(g, p) {", 
      "-\t\tp->se.fair_key\t\t\t= 0;", 
      "-\t\tp->se.wait_runtime\t\t= 0;", 
      "+\t\t/*", 
      "+\t\t * Only normalize user tasks:", 
      "+\t\t */", 
      "+\t\tif (!p->mm)", 
      "+\t\t\tcontinue;", 
      "+", 
      "p->se.exec_start\t\t= 0;", 
      "-\t\tp->se.wait_start_fair\t\t= 0;", 
      "-\t\tp->se.sleep_start_fair\t\t= 0;", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "p->se.wait_start\t\t= 0;", 
      "p->se.sleep_start\t\t= 0;", 
      "p->se.block_start\t\t= 0;", 
      "#endif", 
      "-\t\ttask_rq(p)->cfs.fair_clock\t= 0;", 
      "task_rq(p)->clock\t\t= 0;", 
      "", 
      "if (!rt_task(p)) {", 
      "@@ -6680,26 +6933,9 @@ void normalize_rt_tasks(void)", 
      "", 
      "spin_lock_irqsave(&p->pi_lock, flags);", 
      "rq = __task_rq_lock(p);", 
      "-#ifdef CONFIG_SMP", 
      "-\t\t/*", 
      "-\t\t * Do not touch the migration thread:", 
      "-\t\t */", 
      "-\t\tif (p == rq->migration_thread)", 
      "-\t\t\tgoto out_unlock;", 
      "-#endif", 
      "", 
      "-\t\tupdate_rq_clock(rq);", 
      "-\t\ton_rq = p->se.on_rq;", 
      "-\t\tif (on_rq)", 
      "-\t\t\tdeactivate_task(rq, p, 0);", 
      "-\t\t__setscheduler(rq, p, SCHED_NORMAL, 0);", 
      "-\t\tif (on_rq) {", 
      "-\t\t\tactivate_task(rq, p, 0);", 
      "-\t\t\tresched_task(rq->curr);", 
      "-\t\t}", 
      "-#ifdef CONFIG_SMP", 
      "- out_unlock:", 
      "-#endif", 
      "+\t\tnormalize_task(rq, p);", 
      "+", 
      "__task_rq_unlock(rq);", 
      "spin_unlock_irqrestore(&p->pi_lock, flags);", 
      "} while_each_thread(g, p);", 
      "@@ -6737,8 +6973,8 @@ struct task_struct *curr_task(int cpu)", 
      "* @p: the task pointer to set.", 
      "*", 
      "* Description: This function must only be used when non-maskable interrupts", 
      "- * are serviced on a separate stack.  It allows the architecture to switch the", 
      "- * notion of the current task on a cpu in a non-blocking manner.  This function", 
      "+ * are serviced on a separate stack. It allows the architecture to switch the", 
      "+ * notion of the current task on a cpu in a non-blocking manner. This function", 
      "* must be called with all CPU's synchronized, and interrupts disabled, the", 
      "* and caller must save the original value of the current task (see", 
      "* curr_task() above) and restore that value before reenabling interrupts and", 
      "@@ -6794,3 +7030,425 @@ void set_kernel_trace_flag_all_tasks(voi", 
      "read_unlock(&tasklist_lock);", 
      "}", 
      "EXPORT_SYMBOL_GPL(set_kernel_trace_flag_all_tasks);", 
      "+", 
      "+#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "+", 
      "+/* allocate runqueue etc for a new task group */", 
      "+struct task_group *sched_create_group(void)", 
      "+{", 
      "+\tstruct task_group *tg;", 
      "+\tstruct cfs_rq *cfs_rq;", 
      "+\tstruct sched_entity *se;", 
      "+\tstruct rq *rq;", 
      "+\tint i;", 
      "+", 
      "+\ttg = kzalloc(sizeof(*tg), GFP_KERNEL);", 
      "+\tif (!tg)", 
      "+\t\treturn ERR_PTR(-ENOMEM);", 
      "+", 
      "+\ttg->cfs_rq = kzalloc(sizeof(cfs_rq) * NR_CPUS, GFP_KERNEL);", 
      "+\tif (!tg->cfs_rq)", 
      "+\t\tgoto err;", 
      "+\ttg->se = kzalloc(sizeof(se) * NR_CPUS, GFP_KERNEL);", 
      "+\tif (!tg->se)", 
      "+\t\tgoto err;", 
      "+", 
      "+\tfor_each_possible_cpu(i) {", 
      "+\t\trq = cpu_rq(i);", 
      "+", 
      "+\t\tcfs_rq = kmalloc_node(sizeof(struct cfs_rq), GFP_KERNEL,", 
      "+\t\t\t\t\t\t\t cpu_to_node(i));", 
      "+\t\tif (!cfs_rq)", 
      "+\t\t\tgoto err;", 
      "+", 
      "+\t\tse = kmalloc_node(sizeof(struct sched_entity), GFP_KERNEL,", 
      "+\t\t\t\t\t\t\tcpu_to_node(i));", 
      "+\t\tif (!se)", 
      "+\t\t\tgoto err;", 
      "+", 
      "+\t\tmemset(cfs_rq, 0, sizeof(struct cfs_rq));", 
      "+\t\tmemset(se, 0, sizeof(struct sched_entity));", 
      "+", 
      "+\t\ttg->cfs_rq[i] = cfs_rq;", 
      "+\t\tinit_cfs_rq(cfs_rq, rq);", 
      "+\t\tcfs_rq->tg = tg;", 
      "+", 
      "+\t\ttg->se[i] = se;", 
      "+\t\tse->cfs_rq = &rq->cfs;", 
      "+\t\tse->my_q = cfs_rq;", 
      "+\t\tse->load.weight = NICE_0_LOAD;", 
      "+\t\tse->load.inv_weight = div64_64(1ULL<<32, NICE_0_LOAD);", 
      "+\t\tse->parent = NULL;", 
      "+\t}", 
      "+", 
      "+\tfor_each_possible_cpu(i) {", 
      "+\t\trq = cpu_rq(i);", 
      "+\t\tcfs_rq = tg->cfs_rq[i];", 
      "+\t\tlist_add_rcu(&cfs_rq->leaf_cfs_rq_list, &rq->leaf_cfs_rq_list);", 
      "+\t}", 
      "+", 
      "+\ttg->shares = NICE_0_LOAD;", 
      "+\tspin_lock_init(&tg->lock);", 
      "+", 
      "+\treturn tg;", 
      "+", 
      "+err:", 
      "+\tfor_each_possible_cpu(i) {", 
      "+\t\tif (tg->cfs_rq)", 
      "+\t\t\tkfree(tg->cfs_rq[i]);", 
      "+\t\tif (tg->se)", 
      "+\t\t\tkfree(tg->se[i]);", 
      "+\t}", 
      "+\tkfree(tg->cfs_rq);", 
      "+\tkfree(tg->se);", 
      "+\tkfree(tg);", 
      "+", 
      "+\treturn ERR_PTR(-ENOMEM);", 
      "+}", 
      "+", 
      "+/* rcu callback to free various structures associated with a task group */", 
      "+static void free_sched_group(struct rcu_head *rhp)", 
      "+{", 
      "+\tstruct task_group *tg = container_of(rhp, struct task_group, rcu);", 
      "+\tstruct cfs_rq *cfs_rq;", 
      "+\tstruct sched_entity *se;", 
      "+\tint i;", 
      "+", 
      "+\t/* now it should be safe to free those cfs_rqs */", 
      "+\tfor_each_possible_cpu(i) {", 
      "+\t\tcfs_rq = tg->cfs_rq[i];", 
      "+\t\tkfree(cfs_rq);", 
      "+", 
      "+\t\tse = tg->se[i];", 
      "+\t\tkfree(se);", 
      "+\t}", 
      "+", 
      "+\tkfree(tg->cfs_rq);", 
      "+\tkfree(tg->se);", 
      "+\tkfree(tg);", 
      "+}", 
      "+", 
      "+/* Destroy runqueue etc associated with a task group */", 
      "+void sched_destroy_group(struct task_group *tg)", 
      "+{", 
      "+\tstruct cfs_rq *cfs_rq = NULL;", 
      "+\tint i;", 
      "+", 
      "+\tfor_each_possible_cpu(i) {", 
      "+\t\tcfs_rq = tg->cfs_rq[i];", 
      "+\t\tlist_del_rcu(&cfs_rq->leaf_cfs_rq_list);", 
      "+\t}", 
      "+", 
      "+\tBUG_ON(!cfs_rq);", 
      "+", 
      "+\t/* wait for possible concurrent references to cfs_rqs complete */", 
      "+\tcall_rcu(&tg->rcu, free_sched_group);", 
      "+}", 
      "+", 
      "+/* change task's runqueue when it moves between groups.", 
      "+ *\tThe caller of this function should have put the task in its new group", 
      "+ *\tby now. This function just updates tsk->se.cfs_rq and tsk->se.parent to", 
      "+ *\treflect its new group.", 
      "+ */", 
      "+void sched_move_task(struct task_struct *tsk)", 
      "+{", 
      "+\tint on_rq, running;", 
      "+\tunsigned long flags;", 
      "+\tstruct rq *rq;", 
      "+", 
      "+\trq = task_rq_lock(tsk, &flags);", 
      "+", 
      "+\tif (tsk->sched_class != &fair_sched_class) {", 
      "+\t\tset_task_cfs_rq(tsk, task_cpu(tsk));", 
      "+\t\tgoto done;", 
      "+\t}", 
      "+", 
      "+\tupdate_rq_clock(rq);", 
      "+", 
      "+\trunning = task_current(rq, tsk);", 
      "+\ton_rq = tsk->se.on_rq;", 
      "+", 
      "+\tif (on_rq) {", 
      "+\t\tdequeue_task(rq, tsk, 0);", 
      "+\t\tif (unlikely(running))", 
      "+\t\t\ttsk->sched_class->put_prev_task(rq, tsk);", 
      "+\t}", 
      "+", 
      "+\tset_task_cfs_rq(tsk, task_cpu(tsk));", 
      "+", 
      "+\tif (on_rq) {", 
      "+\t\tif (unlikely(running))", 
      "+\t\t\ttsk->sched_class->set_curr_task(rq);", 
      "+\t\tenqueue_task(rq, tsk, 0);", 
      "+\t}", 
      "+", 
      "+done:", 
      "+\ttask_rq_unlock(rq, &flags);", 
      "+}", 
      "+", 
      "+static void set_se_shares(struct sched_entity *se, unsigned long shares)", 
      "+{", 
      "+\tstruct cfs_rq *cfs_rq = se->cfs_rq;", 
      "+\tstruct rq *rq = cfs_rq->rq;", 
      "+\tint on_rq;", 
      "+", 
      "+\tspin_lock_irq(&rq->lock);", 
      "+", 
      "+\ton_rq = se->on_rq;", 
      "+\tif (on_rq)", 
      "+\t\tdequeue_entity(cfs_rq, se, 0);", 
      "+", 
      "+\tse->load.weight = shares;", 
      "+\tse->load.inv_weight = div64_64((1ULL<<32), shares);", 
      "+", 
      "+\tif (on_rq)", 
      "+\t\tenqueue_entity(cfs_rq, se, 0);", 
      "+", 
      "+\tspin_unlock_irq(&rq->lock);", 
      "+}", 
      "+", 
      "+int sched_group_set_shares(struct task_group *tg, unsigned long shares)", 
      "+{", 
      "+\tint i;", 
      "+", 
      "+\tspin_lock(&tg->lock);", 
      "+\tif (tg->shares == shares)", 
      "+\t\tgoto done;", 
      "+", 
      "+\ttg->shares = shares;", 
      "+\tfor_each_possible_cpu(i)", 
      "+\t\tset_se_shares(tg->se[i], shares);", 
      "+", 
      "+done:", 
      "+\tspin_unlock(&tg->lock);", 
      "+\treturn 0;", 
      "+}", 
      "+", 
      "+unsigned long sched_group_shares(struct task_group *tg)", 
      "+{", 
      "+\treturn tg->shares;", 
      "+}", 
      "+", 
      "+#endif\t/* CONFIG_FAIR_GROUP_SCHED */", 
      "+", 
      "+#ifdef CONFIG_FAIR_CGROUP_SCHED", 
      "+", 
      "+/* return corresponding task_group object of a cgroup */", 
      "+static inline struct task_group *cgroup_tg(struct cgroup *cgrp)", 
      "+{", 
      "+\treturn container_of(cgroup_subsys_state(cgrp, cpu_cgroup_subsys_id),", 
      "+\t\t\t    struct task_group, css);", 
      "+}", 
      "+", 
      "+static struct cgroup_subsys_state *", 
      "+cpu_cgroup_create(struct cgroup_subsys *ss, struct cgroup *cgrp)", 
      "+{", 
      "+\tstruct task_group *tg;", 
      "+", 
      "+\tif (!cgrp->parent) {", 
      "+\t\t/* This is early initialization for the top cgroup */", 
      "+\t\tinit_task_group.css.cgroup = cgrp;", 
      "+\t\treturn &init_task_group.css;", 
      "+\t}", 
      "+", 
      "+\t/* we support only 1-level deep hierarchical scheduler atm */", 
      "+\tif (cgrp->parent->parent)", 
      "+\t\treturn ERR_PTR(-EINVAL);", 
      "+", 
      "+\ttg = sched_create_group();", 
      "+\tif (IS_ERR(tg))", 
      "+\t\treturn ERR_PTR(-ENOMEM);", 
      "+", 
      "+\t/* Bind the cgroup to task_group object we just created */", 
      "+\ttg->css.cgroup = cgrp;", 
      "+", 
      "+\treturn &tg->css;", 
      "+}", 
      "+", 
      "+static void", 
      "+cpu_cgroup_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)", 
      "+{", 
      "+\tstruct task_group *tg = cgroup_tg(cgrp);", 
      "+", 
      "+\tsched_destroy_group(tg);", 
      "+}", 
      "+", 
      "+static int", 
      "+cpu_cgroup_can_attach(struct cgroup_subsys *ss, struct cgroup *cgrp,", 
      "+\t\t      struct task_struct *tsk)", 
      "+{", 
      "+\t/* We don't support RT-tasks being in separate groups */", 
      "+\tif (tsk->sched_class != &fair_sched_class)", 
      "+\t\treturn -EINVAL;", 
      "+", 
      "+\treturn 0;", 
      "+}", 
      "+", 
      "+static void", 
      "+cpu_cgroup_attach(struct cgroup_subsys *ss, struct cgroup *cgrp,", 
      "+\t\t\tstruct cgroup *old_cont, struct task_struct *tsk)", 
      "+{", 
      "+\tsched_move_task(tsk);", 
      "+}", 
      "+", 
      "+static int cpu_shares_write_uint(struct cgroup *cgrp, struct cftype *cftype,", 
      "+\t\t\t\tu64 shareval)", 
      "+{", 
      "+\treturn sched_group_set_shares(cgroup_tg(cgrp), shareval);", 
      "+}", 
      "+", 
      "+static u64 cpu_shares_read_uint(struct cgroup *cgrp, struct cftype *cft)", 
      "+{", 
      "+\tstruct task_group *tg = cgroup_tg(cgrp);", 
      "+", 
      "+\treturn (u64) tg->shares;", 
      "+}", 
      "+", 
      "+static struct cftype cpu_files[] = {", 
      "+\t{", 
      "+\t\t.name = \"shares\",", 
      "+\t\t.read_uint = cpu_shares_read_uint,", 
      "+\t\t.write_uint = cpu_shares_write_uint,", 
      "+\t},", 
      "+};", 
      "+", 
      "+static int cpu_cgroup_populate(struct cgroup_subsys *ss, struct cgroup *cont)", 
      "+{", 
      "+\treturn cgroup_add_files(cont, ss, cpu_files, ARRAY_SIZE(cpu_files));", 
      "+}", 
      "+", 
      "+struct cgroup_subsys cpu_cgroup_subsys = {", 
      "+\t.name\t\t= \"cpu\",", 
      "+\t.create\t\t= cpu_cgroup_create,", 
      "+\t.destroy\t= cpu_cgroup_destroy,", 
      "+\t.can_attach\t= cpu_cgroup_can_attach,", 
      "+\t.attach\t\t= cpu_cgroup_attach,", 
      "+\t.populate\t= cpu_cgroup_populate,", 
      "+\t.subsys_id\t= cpu_cgroup_subsys_id,", 
      "+\t.early_init\t= 1,", 
      "+};", 
      "+", 
      "+#endif\t/* CONFIG_FAIR_CGROUP_SCHED */", 
      "+", 
      "+#ifdef CONFIG_CGROUP_CPUACCT", 
      "+", 
      "+/*", 
      "+ * CPU accounting code for task groups.", 
      "+ *", 
      "+ * Based on the work by Paul Menage (menage@google.com) and Balbir Singh", 
      "+ * (balbir@in.ibm.com).", 
      "+ */", 
      "+", 
      "+/* track cpu usage of a group of tasks */", 
      "+struct cpuacct {", 
      "+\tstruct cgroup_subsys_state css;", 
      "+\t/* cpuusage holds pointer to a u64-type object on every cpu */", 
      "+\tu64 *cpuusage;", 
      "+};", 
      "+", 
      "+struct cgroup_subsys cpuacct_subsys;", 
      "+", 
      "+/* return cpu accounting group corresponding to this container */", 
      "+static inline struct cpuacct *cgroup_ca(struct cgroup *cont)", 
      "+{", 
      "+\treturn container_of(cgroup_subsys_state(cont, cpuacct_subsys_id),", 
      "+\t\t\t    struct cpuacct, css);", 
      "+}", 
      "+", 
      "+/* return cpu accounting group to which this task belongs */", 
      "+static inline struct cpuacct *task_ca(struct task_struct *tsk)", 
      "+{", 
      "+\treturn container_of(task_subsys_state(tsk, cpuacct_subsys_id),", 
      "+\t\t\t    struct cpuacct, css);", 
      "+}", 
      "+", 
      "+/* create a new cpu accounting group */", 
      "+static struct cgroup_subsys_state *cpuacct_create(", 
      "+\tstruct cgroup_subsys *ss, struct cgroup *cont)", 
      "+{", 
      "+\tstruct cpuacct *ca = kzalloc(sizeof(*ca), GFP_KERNEL);", 
      "+", 
      "+\tif (!ca)", 
      "+\t\treturn ERR_PTR(-ENOMEM);", 
      "+", 
      "+\tca->cpuusage = alloc_percpu(u64);", 
      "+\tif (!ca->cpuusage) {", 
      "+\t\tkfree(ca);", 
      "+\t\treturn ERR_PTR(-ENOMEM);", 
      "+\t}", 
      "+", 
      "+\treturn &ca->css;", 
      "+}", 
      "+", 
      "+/* destroy an existing cpu accounting group */", 
      "+static void", 
      "+cpuacct_destroy(struct cgroup_subsys *ss, struct cgroup *cont)", 
      "+{", 
      "+\tstruct cpuacct *ca = cgroup_ca(cont);", 
      "+", 
      "+\tfree_percpu(ca->cpuusage);", 
      "+\tkfree(ca);", 
      "+}", 
      "+", 
      "+/* return total cpu usage (in nanoseconds) of a group */", 
      "+static u64 cpuusage_read(struct cgroup *cont, struct cftype *cft)", 
      "+{", 
      "+\tstruct cpuacct *ca = cgroup_ca(cont);", 
      "+\tu64 totalcpuusage = 0;", 
      "+\tint i;", 
      "+", 
      "+\tfor_each_possible_cpu(i) {", 
      "+\t\tu64 *cpuusage = percpu_ptr(ca->cpuusage, i);", 
      "+", 
      "+\t\t/*", 
      "+\t\t * Take rq->lock to make 64-bit addition safe on 32-bit", 
      "+\t\t * platforms.", 
      "+\t\t */", 
      "+\t\tspin_lock_irq(&cpu_rq(i)->lock);", 
      "+\t\ttotalcpuusage += *cpuusage;", 
      "+\t\tspin_unlock_irq(&cpu_rq(i)->lock);", 
      "+\t}", 
      "+", 
      "+\treturn totalcpuusage;", 
      "+}", 
      "+", 
      "+static struct cftype files[] = {", 
      "+\t{", 
      "+\t\t.name = \"usage\",", 
      "+\t\t.read_uint = cpuusage_read,", 
      "+\t},", 
      "+};", 
      "+", 
      "+static int cpuacct_populate(struct cgroup_subsys *ss, struct cgroup *cont)", 
      "+{", 
      "+\treturn cgroup_add_files(cont, ss, files, ARRAY_SIZE(files));", 
      "+}", 
      "+", 
      "+/*", 
      "+ * charge this task's execution time to its accounting group.", 
      "+ *", 
      "+ * called with rq->lock held.", 
      "+ */", 
      "+static void cpuacct_charge(struct task_struct *tsk, u64 cputime)", 
      "+{", 
      "+\tstruct cpuacct *ca;", 
      "+", 
      "+\tif (!cpuacct_subsys.active)", 
      "+\t\treturn;", 
      "+", 
      "+\tca = task_ca(tsk);", 
      "+\tif (ca) {", 
      "+\t\tu64 *cpuusage = percpu_ptr(ca->cpuusage, task_cpu(tsk));", 
      "+", 
      "+\t\t*cpuusage += cputime;", 
      "+\t}", 
      "+}", 
      "+", 
      "+struct cgroup_subsys cpuacct_subsys = {", 
      "+\t.name = \"cpuacct\",", 
      "+\t.create = cpuacct_create,", 
      "+\t.destroy = cpuacct_destroy,", 
      "+\t.populate = cpuacct_populate,", 
      "+\t.subsys_id = cpuacct_subsys_id,", 
      "+};", 
      "+#endif\t/* CONFIG_CGROUP_CPUACCT */"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/sched_debug.c", 
    "linux-2.6.23/kernel/sched_debug.c", 
    [
      "Index: linux-2.6.23/kernel/sched_debug.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/sched_debug.c", 
      "+++ linux-2.6.23/kernel/sched_debug.c", 
      "@@ -28,6 +28,31 @@", 
      "printk(x);\t\t\t\\", 
      "} while (0)", 
      "", 
      "+/*", 
      "+ * Ease the printing of nsec fields:", 
      "+ */", 
      "+static long long nsec_high(long long nsec)", 
      "+{", 
      "+\tif (nsec < 0) {", 
      "+\t\tnsec = -nsec;", 
      "+\t\tdo_div(nsec, 1000000);", 
      "+\t\treturn -nsec;", 
      "+\t}", 
      "+\tdo_div(nsec, 1000000);", 
      "+", 
      "+\treturn nsec;", 
      "+}", 
      "+", 
      "+static unsigned long nsec_low(long long nsec)", 
      "+{", 
      "+\tif (nsec < 0)", 
      "+\t\tnsec = -nsec;", 
      "+", 
      "+\treturn do_div(nsec, 1000000);", 
      "+}", 
      "+", 
      "+#define SPLIT_NS(x) nsec_high(x), nsec_low(x)", 
      "+", 
      "static void", 
      "print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)", 
      "{", 
      "@@ -36,42 +61,35 @@ print_task(struct seq_file *m, struct rq", 
      "else", 
      "SEQ_printf(m, \" \");", 
      "", 
      "-\tSEQ_printf(m, \"%15s %5d %15Ld %13Ld %13Ld %9Ld %5d \",", 
      "+\tSEQ_printf(m, \"%15s %5d %9Ld.%06ld %9Ld %5d \",", 
      "p->comm, p->pid,", 
      "-\t\t(long long)p->se.fair_key,", 
      "-\t\t(long long)(p->se.fair_key - rq->cfs.fair_clock),", 
      "-\t\t(long long)p->se.wait_runtime,", 
      "+\t\tSPLIT_NS(p->se.vruntime),", 
      "(long long)(p->nvcsw + p->nivcsw),", 
      "p->prio);", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "-\tSEQ_printf(m, \"%15Ld %15Ld %15Ld %15Ld %15Ld\\n\",", 
      "-\t\t(long long)p->se.sum_exec_runtime,", 
      "-\t\t(long long)p->se.sum_wait_runtime,", 
      "-\t\t(long long)p->se.sum_sleep_runtime,", 
      "-\t\t(long long)p->se.wait_runtime_overruns,", 
      "-\t\t(long long)p->se.wait_runtime_underruns);", 
      "+\tSEQ_printf(m, \"%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld\\n\",", 
      "+\t\tSPLIT_NS(p->se.vruntime),", 
      "+\t\tSPLIT_NS(p->se.sum_exec_runtime),", 
      "+\t\tSPLIT_NS(p->se.sum_sleep_runtime));", 
      "#else", 
      "-\tSEQ_printf(m, \"%15Ld %15Ld %15Ld %15Ld %15Ld\\n\",", 
      "-\t\t0LL, 0LL, 0LL, 0LL, 0LL);", 
      "+\tSEQ_printf(m, \"%15Ld %15Ld %15Ld.%06ld %15Ld.%06ld %15Ld.%06ld\\n\",", 
      "+\t\t0LL, 0LL, 0LL, 0L, 0LL, 0L, 0LL, 0L);", 
      "#endif", 
      "}", 
      "", 
      "static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)", 
      "{", 
      "struct task_struct *g, *p;", 
      "+\tunsigned long flags;", 
      "", 
      "SEQ_printf(m,", 
      "\"\\nrunnable tasks:\\n\"", 
      "-\t\"            task   PID        tree-key         delta       waiting\"", 
      "-\t\"  switches  prio\"", 
      "-\t\"        sum-exec        sum-wait       sum-sleep\"", 
      "-\t\"    wait-overrun   wait-underrun\\n\"", 
      "-\t\"------------------------------------------------------------------\"", 
      "-\t\"----------------\"", 
      "-\t\"------------------------------------------------\"", 
      "-\t\"--------------------------------\\n\");", 
      "+\t\"            task   PID         tree-key  switches  prio\"", 
      "+\t\"     exec-runtime         sum-exec        sum-sleep\\n\"", 
      "+\t\"------------------------------------------------------\"", 
      "+\t\"----------------------------------------------------\\n\");", 
      "", 
      "-\tread_lock_irq(&tasklist_lock);", 
      "+\tread_lock_irqsave(&tasklist_lock, flags);", 
      "", 
      "do_each_thread(g, p) {", 
      "if (!p->se.on_rq || task_cpu(p) != rq_cpu)", 
      "@@ -80,48 +98,51 @@ static void print_rq(struct seq_file *m,", 
      "print_task(m, rq, p);", 
      "} while_each_thread(g, p);", 
      "", 
      "-\tread_unlock_irq(&tasklist_lock);", 
      "+\tread_unlock_irqrestore(&tasklist_lock, flags);", 
      "}", 
      "", 
      "-static void", 
      "-print_cfs_rq_runtime_sum(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)", 
      "+void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)", 
      "{", 
      "-\ts64 wait_runtime_rq_sum = 0;", 
      "-\tstruct task_struct *p;", 
      "-\tstruct rb_node *curr;", 
      "-\tunsigned long flags;", 
      "+\ts64 MIN_vruntime = -1, min_vruntime, max_vruntime = -1,", 
      "+\t\tspread, rq0_min_vruntime, spread0;", 
      "struct rq *rq = &per_cpu(runqueues, cpu);", 
      "+\tstruct sched_entity *last;", 
      "+\tunsigned long flags;", 
      "", 
      "-\tspin_lock_irqsave(&rq->lock, flags);", 
      "-\tcurr = first_fair(cfs_rq);", 
      "-\twhile (curr) {", 
      "-\t\tp = rb_entry(curr, struct task_struct, se.run_node);", 
      "-\t\twait_runtime_rq_sum += p->se.wait_runtime;", 
      "-", 
      "-\t\tcurr = rb_next(curr);", 
      "-\t}", 
      "-\tspin_unlock_irqrestore(&rq->lock, flags);", 
      "-", 
      "-\tSEQ_printf(m, \"  .%-30s: %Ld\\n\", \"wait_runtime_rq_sum\",", 
      "-\t\t(long long)wait_runtime_rq_sum);", 
      "-}", 
      "-", 
      "-void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)", 
      "-{", 
      "SEQ_printf(m, \"\\ncfs_rq\\n\");", 
      "", 
      "-#define P(x) \\", 
      "-\tSEQ_printf(m, \"  .%-30s: %Ld\\n\", #x, (long long)(cfs_rq->x))", 
      "-", 
      "-\tP(fair_clock);", 
      "-\tP(exec_clock);", 
      "-\tP(wait_runtime);", 
      "-\tP(wait_runtime_overruns);", 
      "-\tP(wait_runtime_underruns);", 
      "-\tP(sleeper_bonus);", 
      "-#undef P", 
      "+\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"exec_clock\",", 
      "+\t\t\tSPLIT_NS(cfs_rq->exec_clock));", 
      "", 
      "-\tprint_cfs_rq_runtime_sum(m, cpu, cfs_rq);", 
      "+\tspin_lock_irqsave(&rq->lock, flags);", 
      "+\tif (cfs_rq->rb_leftmost)", 
      "+\t\tMIN_vruntime = (__pick_next_entity(cfs_rq))->vruntime;", 
      "+\tlast = __pick_last_entity(cfs_rq);", 
      "+\tif (last)", 
      "+\t\tmax_vruntime = last->vruntime;", 
      "+\tmin_vruntime = rq->cfs.min_vruntime;", 
      "+\trq0_min_vruntime = per_cpu(runqueues, 0).cfs.min_vruntime;", 
      "+\tspin_unlock_irqrestore(&rq->lock, flags);", 
      "+\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"MIN_vruntime\",", 
      "+\t\t\tSPLIT_NS(MIN_vruntime));", 
      "+\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"min_vruntime\",", 
      "+\t\t\tSPLIT_NS(min_vruntime));", 
      "+\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"max_vruntime\",", 
      "+\t\t\tSPLIT_NS(max_vruntime));", 
      "+\tspread = max_vruntime - MIN_vruntime;", 
      "+\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"spread\",", 
      "+\t\t\tSPLIT_NS(spread));", 
      "+\tspread0 = min_vruntime - rq0_min_vruntime;", 
      "+\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"spread0\",", 
      "+\t\t\tSPLIT_NS(spread0));", 
      "+\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"nr_running\", cfs_rq->nr_running);", 
      "+\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"load\", cfs_rq->load.weight);", 
      "+#ifdef CONFIG_SCHEDSTATS", 
      "+\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"bkl_count\",", 
      "+\t\t\trq->bkl_count);", 
      "+#endif", 
      "+\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"nr_spread_over\",", 
      "+\t\t\tcfs_rq->nr_spread_over);", 
      "}", 
      "", 
      "static void print_cpu(struct seq_file *m, int cpu)", 
      "@@ -141,31 +162,32 @@ static void print_cpu(struct seq_file *m", 
      "", 
      "#define P(x) \\", 
      "SEQ_printf(m, \"  .%-30s: %Ld\\n\", #x, (long long)(rq->x))", 
      "+#define PN(x) \\", 
      "+\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", #x, SPLIT_NS(rq->x))", 
      "", 
      "P(nr_running);", 
      "SEQ_printf(m, \"  .%-30s: %lu\\n\", \"load\",", 
      "-\t\t   rq->ls.load.weight);", 
      "-\tP(ls.delta_fair);", 
      "-\tP(ls.delta_exec);", 
      "+\t\t   rq->load.weight);", 
      "P(nr_switches);", 
      "P(nr_load_updates);", 
      "P(nr_uninterruptible);", 
      "SEQ_printf(m, \"  .%-30s: %lu\\n\", \"jiffies\", jiffies);", 
      "-\tP(next_balance);", 
      "+\tPN(next_balance);", 
      "P(curr->pid);", 
      "-\tP(clock);", 
      "-\tP(idle_clock);", 
      "-\tP(prev_clock_raw);", 
      "+\tPN(clock);", 
      "+\tPN(idle_clock);", 
      "+\tPN(prev_clock_raw);", 
      "P(clock_warps);", 
      "P(clock_overflows);", 
      "P(clock_deep_idle_events);", 
      "-\tP(clock_max_delta);", 
      "+\tPN(clock_max_delta);", 
      "P(cpu_load[0]);", 
      "P(cpu_load[1]);", 
      "P(cpu_load[2]);", 
      "P(cpu_load[3]);", 
      "P(cpu_load[4]);", 
      "#undef P", 
      "+#undef PN", 
      "", 
      "print_cfs_stats(m, cpu);", 
      "", 
      "@@ -177,12 +199,25 @@ static int sched_debug_show(struct seq_f", 
      "u64 now = ktime_to_ns(ktime_get());", 
      "int cpu;", 
      "", 
      "-\tSEQ_printf(m, \"Sched Debug Version: v0.05-v20, %s %.*s\\n\",", 
      "+\tSEQ_printf(m, \"Sched Debug Version: v0.07, %s %.*s\\n\",", 
      "init_utsname()->release,", 
      "(int)strcspn(init_utsname()->version, \" \"),", 
      "init_utsname()->version);", 
      "", 
      "-\tSEQ_printf(m, \"now at %Lu nsecs\\n\", (unsigned long long)now);", 
      "+\tSEQ_printf(m, \"now at %Lu.%06ld msecs\\n\", SPLIT_NS(now));", 
      "+", 
      "+#define P(x) \\", 
      "+\tSEQ_printf(m, \"  .%-40s: %Ld\\n\", #x, (long long)(x))", 
      "+#define PN(x) \\", 
      "+\tSEQ_printf(m, \"  .%-40s: %Ld.%06ld\\n\", #x, SPLIT_NS(x))", 
      "+\tPN(sysctl_sched_latency);", 
      "+\tPN(sysctl_sched_min_granularity);", 
      "+\tPN(sysctl_sched_wakeup_granularity);", 
      "+\tPN(sysctl_sched_batch_wakeup_granularity);", 
      "+\tPN(sysctl_sched_child_runs_first);", 
      "+\tP(sysctl_sched_features);", 
      "+#undef PN", 
      "+#undef P", 
      "", 
      "for_each_online_cpu(cpu)", 
      "print_cpu(m, cpu);", 
      "@@ -202,7 +237,7 @@ static int sched_debug_open(struct inode", 
      "return single_open(filp, sched_debug_show, NULL);", 
      "}", 
      "", 
      "-static struct file_operations sched_debug_fops = {", 
      "+static const struct file_operations sched_debug_fops = {", 
      ".open\t\t= sched_debug_open,", 
      ".read\t\t= seq_read,", 
      ".llseek\t\t= seq_lseek,", 
      "@@ -226,6 +261,7 @@ __initcall(init_sched_debug_procfs);", 
      "", 
      "void proc_sched_show_task(struct task_struct *p, struct seq_file *m)", 
      "{", 
      "+\tunsigned long nr_switches;", 
      "unsigned long flags;", 
      "int num_threads = 1;", 
      "", 
      "@@ -237,41 +273,91 @@ void proc_sched_show_task(struct task_st", 
      "rcu_read_unlock();", 
      "", 
      "SEQ_printf(m, \"%s (%d, #threads: %d)\\n\", p->comm, p->pid, num_threads);", 
      "-\tSEQ_printf(m, \"----------------------------------------------\\n\");", 
      "+\tSEQ_printf(m,", 
      "+\t\t\"---------------------------------------------------------\\n\");", 
      "+#define __P(F) \\", 
      "+\tSEQ_printf(m, \"%-35s:%21Ld\\n\", #F, (long long)F)", 
      "#define P(F) \\", 
      "-\tSEQ_printf(m, \"%-25s:%20Ld\\n\", #F, (long long)p->F)", 
      "+\tSEQ_printf(m, \"%-35s:%21Ld\\n\", #F, (long long)p->F)", 
      "+#define __PN(F) \\", 
      "+\tSEQ_printf(m, \"%-35s:%14Ld.%06ld\\n\", #F, SPLIT_NS((long long)F))", 
      "+#define PN(F) \\", 
      "+\tSEQ_printf(m, \"%-35s:%14Ld.%06ld\\n\", #F, SPLIT_NS((long long)p->F))", 
      "+", 
      "+\tPN(se.exec_start);", 
      "+\tPN(se.vruntime);", 
      "+\tPN(se.sum_exec_runtime);", 
      "", 
      "-\tP(se.wait_runtime);", 
      "-\tP(se.wait_start_fair);", 
      "-\tP(se.exec_start);", 
      "-\tP(se.sleep_start_fair);", 
      "-\tP(se.sum_exec_runtime);", 
      "+\tnr_switches = p->nvcsw + p->nivcsw;", 
      "", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "-\tP(se.wait_start);", 
      "-\tP(se.sleep_start);", 
      "-\tP(se.block_start);", 
      "-\tP(se.sleep_max);", 
      "-\tP(se.block_max);", 
      "-\tP(se.exec_max);", 
      "-\tP(se.wait_max);", 
      "-\tP(se.wait_runtime_overruns);", 
      "-\tP(se.wait_runtime_underruns);", 
      "-\tP(se.sum_wait_runtime);", 
      "+\tPN(se.wait_start);", 
      "+\tPN(se.sleep_start);", 
      "+\tPN(se.block_start);", 
      "+\tPN(se.sleep_max);", 
      "+\tPN(se.block_max);", 
      "+\tPN(se.exec_max);", 
      "+\tPN(se.slice_max);", 
      "+\tPN(se.wait_max);", 
      "+\tP(sched_info.bkl_count);", 
      "+\tP(se.nr_migrations);", 
      "+\tP(se.nr_migrations_cold);", 
      "+\tP(se.nr_failed_migrations_affine);", 
      "+\tP(se.nr_failed_migrations_running);", 
      "+\tP(se.nr_failed_migrations_hot);", 
      "+\tP(se.nr_forced_migrations);", 
      "+\tP(se.nr_forced2_migrations);", 
      "+\tP(se.nr_wakeups);", 
      "+\tP(se.nr_wakeups_sync);", 
      "+\tP(se.nr_wakeups_migrate);", 
      "+\tP(se.nr_wakeups_local);", 
      "+\tP(se.nr_wakeups_remote);", 
      "+\tP(se.nr_wakeups_affine);", 
      "+\tP(se.nr_wakeups_affine_attempts);", 
      "+\tP(se.nr_wakeups_passive);", 
      "+\tP(se.nr_wakeups_idle);", 
      "+", 
      "+\t{", 
      "+\t\tu64 avg_atom, avg_per_cpu;", 
      "+", 
      "+\t\tavg_atom = p->se.sum_exec_runtime;", 
      "+\t\tif (nr_switches)", 
      "+\t\t\tdo_div(avg_atom, nr_switches);", 
      "+\t\telse", 
      "+\t\t\tavg_atom = -1LL;", 
      "+", 
      "+\t\tavg_per_cpu = p->se.sum_exec_runtime;", 
      "+\t\tif (p->se.nr_migrations) {", 
      "+\t\t\tavg_per_cpu = div64_64(avg_per_cpu,", 
      "+\t\t\t\t\t       p->se.nr_migrations);", 
      "+\t\t} else {", 
      "+\t\t\tavg_per_cpu = -1LL;", 
      "+\t\t}", 
      "+", 
      "+\t\t__PN(avg_atom);", 
      "+\t\t__PN(avg_per_cpu);", 
      "+\t}", 
      "#endif", 
      "-\tSEQ_printf(m, \"%-25s:%20Ld\\n\",", 
      "-\t\t   \"nr_switches\", (long long)(p->nvcsw + p->nivcsw));", 
      "+\t__P(nr_switches);", 
      "+\tSEQ_printf(m, \"%-35s:%21Ld\\n\",", 
      "+\t\t   \"nr_voluntary_switches\", (long long)p->nvcsw);", 
      "+\tSEQ_printf(m, \"%-35s:%21Ld\\n\",", 
      "+\t\t   \"nr_involuntary_switches\", (long long)p->nivcsw);", 
      "+", 
      "P(se.load.weight);", 
      "P(policy);", 
      "P(prio);", 
      "+#undef PN", 
      "+#undef __PN", 
      "#undef P", 
      "+#undef __P", 
      "", 
      "{", 
      "u64 t0, t1;", 
      "", 
      "t0 = sched_clock();", 
      "t1 = sched_clock();", 
      "-\t\tSEQ_printf(m, \"%-25s:%20Ld\\n\",", 
      "+\t\tSEQ_printf(m, \"%-35s:%21Ld\\n\",", 
      "\"clock-delta\", (long long)(t1-t0));", 
      "}", 
      "}", 
      "@@ -279,9 +365,32 @@ void proc_sched_show_task(struct task_st", 
      "void proc_sched_set_task(struct task_struct *p)", 
      "{", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "-\tp->se.sleep_max = p->se.block_max = p->se.exec_max = p->se.wait_max = 0;", 
      "-\tp->se.wait_runtime_overruns = p->se.wait_runtime_underruns = 0;", 
      "+\tp->se.wait_max\t\t\t\t= 0;", 
      "+\tp->se.sleep_max\t\t\t\t= 0;", 
      "+\tp->se.sum_sleep_runtime\t\t\t= 0;", 
      "+\tp->se.block_max\t\t\t\t= 0;", 
      "+\tp->se.exec_max\t\t\t\t= 0;", 
      "+\tp->se.slice_max\t\t\t\t= 0;", 
      "+\tp->se.nr_migrations\t\t\t= 0;", 
      "+\tp->se.nr_migrations_cold\t\t= 0;", 
      "+\tp->se.nr_failed_migrations_affine\t= 0;", 
      "+\tp->se.nr_failed_migrations_running\t= 0;", 
      "+\tp->se.nr_failed_migrations_hot\t\t= 0;", 
      "+\tp->se.nr_forced_migrations\t\t= 0;", 
      "+\tp->se.nr_forced2_migrations\t\t= 0;", 
      "+\tp->se.nr_wakeups\t\t\t= 0;", 
      "+\tp->se.nr_wakeups_sync\t\t\t= 0;", 
      "+\tp->se.nr_wakeups_migrate\t\t= 0;", 
      "+\tp->se.nr_wakeups_local\t\t\t= 0;", 
      "+\tp->se.nr_wakeups_remote\t\t\t= 0;", 
      "+\tp->se.nr_wakeups_affine\t\t\t= 0;", 
      "+\tp->se.nr_wakeups_affine_attempts\t= 0;", 
      "+\tp->se.nr_wakeups_passive\t\t= 0;", 
      "+\tp->se.nr_wakeups_idle\t\t\t= 0;", 
      "+\tp->sched_info.bkl_count\t\t\t= 0;", 
      "#endif", 
      "-\tp->se.sum_exec_runtime = 0;", 
      "-\tp->se.prev_sum_exec_runtime\t= 0;", 
      "+\tp->se.sum_exec_runtime\t\t\t= 0;", 
      "+\tp->se.prev_sum_exec_runtime\t\t= 0;", 
      "+\tp->nvcsw\t\t\t\t= 0;", 
      "+\tp->nivcsw\t\t\t\t= 0;", 
      "}"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/sched_fair.c", 
    "linux-2.6.23/kernel/sched_fair.c", 
    [
      "Index: linux-2.6.23/kernel/sched_fair.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/sched_fair.c", 
      "+++ linux-2.6.23/kernel/sched_fair.c", 
      "@@ -22,25 +22,34 @@", 
      "", 
      "/*", 
      "* Targeted preemption latency for CPU-bound tasks:", 
      "- * (default: 20ms, units: nanoseconds)", 
      "+ * (default: 20ms * (1 + ilog(ncpus)), units: nanoseconds)", 
      "*", 
      "* NOTE: this latency value is not the same as the concept of", 
      "- * 'timeslice length' - timeslices in CFS are of variable length.", 
      "- * (to see the precise effective timeslice length of your workload,", 
      "- *  run vmstat and monitor the context-switches field)", 
      "+ * 'timeslice length' - timeslices in CFS are of variable length", 
      "+ * and have no persistent notion like in traditional, time-slice", 
      "+ * based scheduling concepts.", 
      "*", 
      "- * On SMP systems the value of this is multiplied by the log2 of the", 
      "- * number of CPUs. (i.e. factor 2x on 2-way systems, 3x on 4-way", 
      "- * systems, 4x on 8-way systems, 5x on 16-way systems, etc.)", 
      "- * Targeted preemption latency for CPU-bound tasks:", 
      "+ * (to see the precise effective timeslice length of your workload,", 
      "+ *  run vmstat and monitor the context-switches (cs) field)", 
      "*/", 
      "-unsigned int sysctl_sched_latency __read_mostly = 20000000ULL;", 
      "+unsigned int sysctl_sched_latency = 20000000ULL;", 
      "", 
      "/*", 
      "* Minimal preemption granularity for CPU-bound tasks:", 
      "- * (default: 2 msec, units: nanoseconds)", 
      "+ * (default: 4 msec * (1 + ilog(ncpus)), units: nanoseconds)", 
      "+ */", 
      "+unsigned int sysctl_sched_min_granularity = 4000000ULL;", 
      "+", 
      "+/*", 
      "+ * is kept at sysctl_sched_latency / sysctl_sched_min_granularity", 
      "*/", 
      "-unsigned int sysctl_sched_min_granularity __read_mostly = 2000000ULL;", 
      "+static unsigned int sched_nr_latency = 5;", 
      "+", 
      "+/*", 
      "+ * After fork, child runs first. (default) If set to 0 then", 
      "+ * parent will (try to) run first.", 
      "+ */", 
      "+const_debug unsigned int sysctl_sched_child_runs_first = 1;", 
      "", 
      "/*", 
      "* sys_sched_yield() compat mode", 
      "@@ -52,52 +61,25 @@ unsigned int __read_mostly sysctl_sched_", 
      "", 
      "/*", 
      "* SCHED_BATCH wake-up granularity.", 
      "- * (default: 25 msec, units: nanoseconds)", 
      "+ * (default: 10 msec * (1 + ilog(ncpus)), units: nanoseconds)", 
      "*", 
      "* This option delays the preemption effects of decoupled workloads", 
      "* and reduces their over-scheduling. Synchronous workloads will still", 
      "* have immediate wakeup/sleep latencies.", 
      "*/", 
      "-unsigned int sysctl_sched_batch_wakeup_granularity __read_mostly = 25000000UL;", 
      "+unsigned int sysctl_sched_batch_wakeup_granularity = 10000000UL;", 
      "", 
      "/*", 
      "* SCHED_OTHER wake-up granularity.", 
      "- * (default: 1 msec, units: nanoseconds)", 
      "+ * (default: 10 msec * (1 + ilog(ncpus)), units: nanoseconds)", 
      "*", 
      "* This option delays the preemption effects of decoupled workloads", 
      "* and reduces their over-scheduling. Synchronous workloads will still", 
      "* have immediate wakeup/sleep latencies.", 
      "*/", 
      "-unsigned int sysctl_sched_wakeup_granularity __read_mostly = 1000000UL;", 
      "-", 
      "-unsigned int sysctl_sched_stat_granularity __read_mostly;", 
      "-", 
      "-/*", 
      "- * Initialized in sched_init_granularity() [to 5 times the base granularity]:", 
      "- */", 
      "-unsigned int sysctl_sched_runtime_limit __read_mostly;", 
      "-", 
      "-/*", 
      "- * Debugging: various feature bits", 
      "- */", 
      "-enum {", 
      "-\tSCHED_FEAT_FAIR_SLEEPERS\t= 1,", 
      "-\tSCHED_FEAT_SLEEPER_AVG\t\t= 2,", 
      "-\tSCHED_FEAT_SLEEPER_LOAD_AVG\t= 4,", 
      "-\tSCHED_FEAT_PRECISE_CPU_LOAD\t= 8,", 
      "-\tSCHED_FEAT_START_DEBIT\t\t= 16,", 
      "-\tSCHED_FEAT_SKIP_INITIAL\t\t= 32,", 
      "-};", 
      "-", 
      "-unsigned int sysctl_sched_features __read_mostly =", 
      "-\t\tSCHED_FEAT_FAIR_SLEEPERS\t*1 |", 
      "-\t\tSCHED_FEAT_SLEEPER_AVG\t\t*0 |", 
      "-\t\tSCHED_FEAT_SLEEPER_LOAD_AVG\t*1 |", 
      "-\t\tSCHED_FEAT_PRECISE_CPU_LOAD\t*0 |", 
      "-\t\tSCHED_FEAT_START_DEBIT\t\t*1 |", 
      "-\t\tSCHED_FEAT_SKIP_INITIAL\t\t*0;", 
      "+unsigned int sysctl_sched_wakeup_granularity = 10000000UL;", 
      "", 
      "-extern struct sched_class fair_sched_class;", 
      "+const_debug unsigned int sysctl_sched_migration_cost = 500000UL;", 
      "", 
      "/**************************************************************", 
      "* CFS operations on generic schedulable entities:", 
      "@@ -111,21 +93,9 @@ static inline struct rq *rq_of(struct cf", 
      "return cfs_rq->rq;", 
      "}", 
      "", 
      "-/* currently running entity (if any) on this cfs_rq */", 
      "-static inline struct sched_entity *cfs_rq_curr(struct cfs_rq *cfs_rq)", 
      "-{", 
      "-\treturn cfs_rq->curr;", 
      "-}", 
      "-", 
      "/* An entity is a task if it doesn't \"own\" a runqueue */", 
      "#define entity_is_task(se)\t(!se->my_q)", 
      "", 
      "-static inline void", 
      "-set_cfs_rq_curr(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "-{", 
      "-\tcfs_rq->curr = se;", 
      "-}", 
      "-", 
      "#else\t/* CONFIG_FAIR_GROUP_SCHED */", 
      "", 
      "static inline struct rq *rq_of(struct cfs_rq *cfs_rq)", 
      "@@ -133,21 +103,8 @@ static inline struct rq *rq_of(struct cf", 
      "return container_of(cfs_rq, struct rq, cfs);", 
      "}", 
      "", 
      "-static inline struct sched_entity *cfs_rq_curr(struct cfs_rq *cfs_rq)", 
      "-{", 
      "-\tstruct rq *rq = rq_of(cfs_rq);", 
      "-", 
      "-\tif (unlikely(rq->curr->sched_class != &fair_sched_class))", 
      "-\t\treturn NULL;", 
      "-", 
      "-\treturn &rq->curr->se;", 
      "-}", 
      "-", 
      "#define entity_is_task(se)\t1", 
      "", 
      "-static inline void", 
      "-set_cfs_rq_curr(struct cfs_rq *cfs_rq, struct sched_entity *se) { }", 
      "-", 
      "#endif\t/* CONFIG_FAIR_GROUP_SCHED */", 
      "", 
      "static inline struct task_struct *task_of(struct sched_entity *se)", 
      "@@ -160,16 +117,38 @@ static inline struct task_struct *task_o", 
      "* Scheduling class tree data structure manipulation methods:", 
      "*/", 
      "", 
      "+static inline u64 max_vruntime(u64 min_vruntime, u64 vruntime)", 
      "+{", 
      "+\ts64 delta = (s64)(vruntime - min_vruntime);", 
      "+\tif (delta > 0)", 
      "+\t\tmin_vruntime = vruntime;", 
      "+", 
      "+\treturn min_vruntime;", 
      "+}", 
      "+", 
      "+static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)", 
      "+{", 
      "+\ts64 delta = (s64)(vruntime - min_vruntime);", 
      "+\tif (delta < 0)", 
      "+\t\tmin_vruntime = vruntime;", 
      "+", 
      "+\treturn min_vruntime;", 
      "+}", 
      "+", 
      "+static inline s64 entity_key(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "+{", 
      "+\treturn se->vruntime - cfs_rq->min_vruntime;", 
      "+}", 
      "+", 
      "/*", 
      "* Enqueue an entity into the rb-tree:", 
      "*/", 
      "-static inline void", 
      "-__enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "+static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "struct rb_node **link = &cfs_rq->tasks_timeline.rb_node;", 
      "struct rb_node *parent = NULL;", 
      "struct sched_entity *entry;", 
      "-\ts64 key = se->fair_key;", 
      "+\ts64 key = entity_key(cfs_rq, se);", 
      "int leftmost = 1;", 
      "", 
      "/*", 
      "@@ -182,7 +161,7 @@ __enqueue_entity(struct cfs_rq *cfs_rq,", 
      "* We dont care about collisions. Nodes with", 
      "* the same key stay together.", 
      "*/", 
      "-\t\tif (key - entry->fair_key < 0) {", 
      "+\t\tif (key < entity_key(cfs_rq, entry)) {", 
      "link = &parent->rb_left;", 
      "} else {", 
      "link = &parent->rb_right;", 
      "@@ -199,24 +178,14 @@ __enqueue_entity(struct cfs_rq *cfs_rq,", 
      "", 
      "rb_link_node(&se->run_node, parent, link);", 
      "rb_insert_color(&se->run_node, &cfs_rq->tasks_timeline);", 
      "-\tupdate_load_add(&cfs_rq->load, se->load.weight);", 
      "-\tcfs_rq->nr_running++;", 
      "-\tse->on_rq = 1;", 
      "-", 
      "-\tschedstat_add(cfs_rq, wait_runtime, se->wait_runtime);", 
      "}", 
      "", 
      "-static inline void", 
      "-__dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "+static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "if (cfs_rq->rb_leftmost == &se->run_node)", 
      "cfs_rq->rb_leftmost = rb_next(&se->run_node);", 
      "-\trb_erase(&se->run_node, &cfs_rq->tasks_timeline);", 
      "-\tupdate_load_sub(&cfs_rq->load, se->load.weight);", 
      "-\tcfs_rq->nr_running--;", 
      "-\tse->on_rq = 0;", 
      "", 
      "-\tschedstat_add(cfs_rq, wait_runtime, -se->wait_runtime);", 
      "+\trb_erase(&se->run_node, &cfs_rq->tasks_timeline);", 
      "}", 
      "", 
      "static inline struct rb_node *first_fair(struct cfs_rq *cfs_rq)", 
      "@@ -229,118 +198,103 @@ static struct sched_entity *__pick_next_", 
      "return rb_entry(first_fair(cfs_rq), struct sched_entity, run_node);", 
      "}", 
      "", 
      "+static inline struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)", 
      "+{", 
      "+\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_node;", 
      "+\tstruct sched_entity *se = NULL;", 
      "+\tstruct rb_node *parent;", 
      "+", 
      "+\twhile (*link) {", 
      "+\t\tparent = *link;", 
      "+\t\tse = rb_entry(parent, struct sched_entity, run_node);", 
      "+\t\tlink = &parent->rb_right;", 
      "+\t}", 
      "+", 
      "+\treturn se;", 
      "+}", 
      "+", 
      "/**************************************************************", 
      "* Scheduling class statistics methods:", 
      "*/", 
      "", 
      "+#ifdef CONFIG_SCHED_DEBUG", 
      "+int sched_nr_latency_handler(struct ctl_table *table, int write,", 
      "+\t\tstruct file *filp, void __user *buffer, size_t *lenp,", 
      "+\t\tloff_t *ppos)", 
      "+{", 
      "+\tint ret = proc_dointvec_minmax(table, write, filp, buffer, lenp, ppos);", 
      "+", 
      "+\tif (ret || !write)", 
      "+\t\treturn ret;", 
      "+", 
      "+\tsched_nr_latency = DIV_ROUND_UP(sysctl_sched_latency,", 
      "+\t\t\t\t\tsysctl_sched_min_granularity);", 
      "+", 
      "+\treturn 0;", 
      "+}", 
      "+#endif", 
      "+", 
      "/*", 
      "- * Calculate the preemption granularity needed to schedule every", 
      "- * runnable task once per sysctl_sched_latency amount of time.", 
      "- * (down to a sensible low limit on granularity)", 
      "- *", 
      "- * For example, if there are 2 tasks running and latency is 10 msecs,", 
      "- * we switch tasks every 5 msecs. If we have 3 tasks running, we have", 
      "- * to switch tasks every 3.33 msecs to get a 10 msecs observed latency", 
      "- * for each task. We do finer and finer scheduling up to until we", 
      "- * reach the minimum granularity value.", 
      "- *", 
      "- * To achieve this we use the following dynamic-granularity rule:", 
      "- *", 
      "- *    gran = lat/nr - lat/nr/nr", 
      "+ * The idea is to set a period in which each task runs once.", 
      "*", 
      "- * This comes out of the following equations:", 
      "+ * When there are too many tasks (sysctl_sched_nr_latency) we have to stretch", 
      "+ * this period because otherwise the slices get too small.", 
      "*", 
      "- *    kA1 + gran = kB1", 
      "- *    kB2 + gran = kA2", 
      "- *    kA2 = kA1", 
      "- *    kB2 = kB1 - d + d/nr", 
      "- *    lat = d * nr", 
      "- *", 
      "- * Where 'k' is key, 'A' is task A (waiting), 'B' is task B (running),", 
      "- * '1' is start of time, '2' is end of time, 'd' is delay between", 
      "- * 1 and 2 (during which task B was running), 'nr' is number of tasks", 
      "- * running, 'lat' is the the period of each task. ('lat' is the", 
      "- * sched_latency that we aim for.)", 
      "+ * p = (nr <= nl) ? l : l*nr/nl", 
      "*/", 
      "-static long", 
      "-sched_granularity(struct cfs_rq *cfs_rq)", 
      "+static u64 __sched_period(unsigned long nr_running)", 
      "{", 
      "-\tunsigned int gran = sysctl_sched_latency;", 
      "-\tunsigned int nr = cfs_rq->nr_running;", 
      "+\tu64 period = sysctl_sched_latency;", 
      "+\tunsigned long nr_latency = sched_nr_latency;", 
      "", 
      "-\tif (nr > 1) {", 
      "-\t\tgran = gran/nr - gran/nr/nr;", 
      "-\t\tgran = max(gran, sysctl_sched_min_granularity);", 
      "+\tif (unlikely(nr_running > nr_latency)) {", 
      "+\t\tperiod *= nr_running;", 
      "+\t\tdo_div(period, nr_latency);", 
      "}", 
      "", 
      "-\treturn gran;", 
      "+\treturn period;", 
      "}", 
      "", 
      "/*", 
      "- * We rescale the rescheduling granularity of tasks according to their", 
      "- * nice level, but only linearly, not exponentially:", 
      "+ * We calculate the wall-time slice from the period by taking a part", 
      "+ * proportional to the weight.", 
      "+ *", 
      "+ * s = p*w/rw", 
      "*/", 
      "-static long", 
      "-niced_granularity(struct sched_entity *curr, unsigned long granularity)", 
      "+static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\tu64 tmp;", 
      "+\tu64 slice = __sched_period(cfs_rq->nr_running);", 
      "", 
      "-\tif (likely(curr->load.weight == NICE_0_LOAD))", 
      "-\t\treturn granularity;", 
      "-\t/*", 
      "-\t * Positive nice levels get the same granularity as nice-0:", 
      "-\t */", 
      "-\tif (likely(curr->load.weight < NICE_0_LOAD)) {", 
      "-\t\ttmp = curr->load.weight * (u64)granularity;", 
      "-\t\treturn (long) (tmp >> NICE_0_SHIFT);", 
      "-\t}", 
      "-\t/*", 
      "-\t * Negative nice level tasks get linearly finer", 
      "-\t * granularity:", 
      "-\t */", 
      "-\ttmp = curr->load.inv_weight * (u64)granularity;", 
      "+\tslice *= se->load.weight;", 
      "+\tdo_div(slice, cfs_rq->load.weight);", 
      "", 
      "-\t/*", 
      "-\t * It will always fit into 'long':", 
      "-\t */", 
      "-\treturn (long) (tmp >> (WMULT_SHIFT-NICE_0_SHIFT));", 
      "+\treturn slice;", 
      "}", 
      "", 
      "-static inline void", 
      "-limit_wait_runtime(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "+/*", 
      "+ * We calculate the vruntime slice.", 
      "+ *", 
      "+ * vs = s/w = p/rw", 
      "+ */", 
      "+static u64 __sched_vslice(unsigned long rq_weight, unsigned long nr_running)", 
      "{", 
      "-\tlong limit = sysctl_sched_runtime_limit;", 
      "+\tu64 vslice = __sched_period(nr_running);", 
      "", 
      "-\t/*", 
      "-\t * Niced tasks have the same history dynamic range as", 
      "-\t * non-niced tasks:", 
      "-\t */", 
      "-\tif (unlikely(se->wait_runtime > limit)) {", 
      "-\t\tse->wait_runtime = limit;", 
      "-\t\tschedstat_inc(se, wait_runtime_overruns);", 
      "-\t\tschedstat_inc(cfs_rq, wait_runtime_overruns);", 
      "-\t}", 
      "-\tif (unlikely(se->wait_runtime < -limit)) {", 
      "-\t\tse->wait_runtime = -limit;", 
      "-\t\tschedstat_inc(se, wait_runtime_underruns);", 
      "-\t\tschedstat_inc(cfs_rq, wait_runtime_underruns);", 
      "-\t}", 
      "+\tvslice *= NICE_0_LOAD;", 
      "+\tdo_div(vslice, rq_weight);", 
      "+", 
      "+\treturn vslice;", 
      "}", 
      "", 
      "-static inline void", 
      "-__add_wait_runtime(struct cfs_rq *cfs_rq, struct sched_entity *se, long delta)", 
      "+static u64 sched_vslice(struct cfs_rq *cfs_rq)", 
      "{", 
      "-\tse->wait_runtime += delta;", 
      "-\tschedstat_add(se, sum_wait_runtime, delta);", 
      "-\tlimit_wait_runtime(cfs_rq, se);", 
      "+\treturn __sched_vslice(cfs_rq->load.weight, cfs_rq->nr_running);", 
      "}", 
      "", 
      "-static void", 
      "-add_wait_runtime(struct cfs_rq *cfs_rq, struct sched_entity *se, long delta)", 
      "+static u64 sched_vslice_add(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\tschedstat_add(cfs_rq, wait_runtime, -se->wait_runtime);", 
      "-\t__add_wait_runtime(cfs_rq, se, delta);", 
      "-\tschedstat_add(cfs_rq, wait_runtime, se->wait_runtime);", 
      "+\treturn __sched_vslice(cfs_rq->load.weight + se->load.weight,", 
      "+\t\t\tcfs_rq->nr_running + 1);", 
      "}", 
      "", 
      "/*", 
      "@@ -348,46 +302,41 @@ add_wait_runtime(struct cfs_rq *cfs_rq,", 
      "* are not in our scheduling class.", 
      "*/", 
      "static inline void", 
      "-__update_curr(struct cfs_rq *cfs_rq, struct sched_entity *curr)", 
      "+__update_curr(struct cfs_rq *cfs_rq, struct sched_entity *curr,", 
      "+\t      unsigned long delta_exec)", 
      "{", 
      "-\tunsigned long delta, delta_exec, delta_fair, delta_mine;", 
      "-\tstruct load_weight *lw = &cfs_rq->load;", 
      "-\tunsigned long load = lw->weight;", 
      "+\tunsigned long delta_exec_weighted;", 
      "+\tu64 vruntime;", 
      "", 
      "-\tdelta_exec = curr->delta_exec;", 
      "schedstat_set(curr->exec_max, max((u64)delta_exec, curr->exec_max));", 
      "", 
      "curr->sum_exec_runtime += delta_exec;", 
      "-\tcfs_rq->exec_clock += delta_exec;", 
      "-", 
      "-\tif (unlikely(!load))", 
      "-\t\treturn;", 
      "-", 
      "-\tdelta_fair = calc_delta_fair(delta_exec, lw);", 
      "-\tdelta_mine = calc_delta_mine(delta_exec, curr->load.weight, lw);", 
      "-", 
      "-\tif (cfs_rq->sleeper_bonus > sysctl_sched_min_granularity) {", 
      "-\t\tdelta = min((u64)delta_mine, cfs_rq->sleeper_bonus);", 
      "-\t\tdelta = min(delta, (unsigned long)(", 
      "-\t\t\t(long)sysctl_sched_runtime_limit - curr->wait_runtime));", 
      "-\t\tcfs_rq->sleeper_bonus -= delta;", 
      "-\t\tdelta_mine -= delta;", 
      "+\tschedstat_add(cfs_rq, exec_clock, delta_exec);", 
      "+\tdelta_exec_weighted = delta_exec;", 
      "+\tif (unlikely(curr->load.weight != NICE_0_LOAD)) {", 
      "+\t\tdelta_exec_weighted = calc_delta_fair(delta_exec_weighted,", 
      "+\t\t\t\t\t\t\t&curr->load);", 
      "}", 
      "+\tcurr->vruntime += delta_exec_weighted;", 
      "", 
      "-\tcfs_rq->fair_clock += delta_fair;", 
      "/*", 
      "-\t * We executed delta_exec amount of time on the CPU,", 
      "-\t * but we were only entitled to delta_mine amount of", 
      "-\t * time during that period (if nr_running == 1 then", 
      "-\t * the two values are equal)", 
      "-\t * [Note: delta_mine - delta_exec is negative]:", 
      "+\t * maintain cfs_rq->min_vruntime to be a monotonic increasing", 
      "+\t * value tracking the leftmost vruntime in the tree.", 
      "*/", 
      "-\tadd_wait_runtime(cfs_rq, curr, delta_mine - delta_exec);", 
      "+\tif (first_fair(cfs_rq)) {", 
      "+\t\tvruntime = min_vruntime(curr->vruntime,", 
      "+\t\t\t\t__pick_next_entity(cfs_rq)->vruntime);", 
      "+\t} else", 
      "+\t\tvruntime = curr->vruntime;", 
      "+", 
      "+\tcfs_rq->min_vruntime =", 
      "+\t\tmax_vruntime(cfs_rq->min_vruntime, vruntime);", 
      "}", 
      "", 
      "static void update_curr(struct cfs_rq *cfs_rq)", 
      "{", 
      "-\tstruct sched_entity *curr = cfs_rq_curr(cfs_rq);", 
      "+\tstruct sched_entity *curr = cfs_rq->curr;", 
      "+\tu64 now = rq_of(cfs_rq)->clock;", 
      "unsigned long delta_exec;", 
      "", 
      "if (unlikely(!curr))", 
      "@@ -398,135 +347,53 @@ static void update_curr(struct cfs_rq *c", 
      "* since the last time we changed load (this cannot", 
      "* overflow on 32 bits):", 
      "*/", 
      "-\tdelta_exec = (unsigned long)(rq_of(cfs_rq)->clock - curr->exec_start);", 
      "+\tdelta_exec = (unsigned long)(now - curr->exec_start);", 
      "+", 
      "+\t__update_curr(cfs_rq, curr, delta_exec);", 
      "+\tcurr->exec_start = now;", 
      "", 
      "-\tcurr->delta_exec += delta_exec;", 
      "+\tif (entity_is_task(curr)) {", 
      "+\t\tstruct task_struct *curtask = task_of(curr);", 
      "", 
      "-\tif (unlikely(curr->delta_exec > sysctl_sched_stat_granularity)) {", 
      "-\t\t__update_curr(cfs_rq, curr);", 
      "-\t\tcurr->delta_exec = 0;", 
      "+\t\tcpuacct_charge(curtask, delta_exec);", 
      "}", 
      "-\tcurr->exec_start = rq_of(cfs_rq)->clock;", 
      "}", 
      "", 
      "static inline void", 
      "update_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\tse->wait_start_fair = cfs_rq->fair_clock;", 
      "schedstat_set(se->wait_start, rq_of(cfs_rq)->clock);", 
      "}", 
      "", 
      "/*", 
      "- * We calculate fair deltas here, so protect against the random effects", 
      "- * of a multiplication overflow by capping it to the runtime limit:", 
      "- */", 
      "-#if BITS_PER_LONG == 32", 
      "-static inline unsigned long", 
      "-calc_weighted(unsigned long delta, unsigned long weight, int shift)", 
      "-{", 
      "-\tu64 tmp = (u64)delta * weight >> shift;", 
      "-", 
      "-\tif (unlikely(tmp > sysctl_sched_runtime_limit*2))", 
      "-\t\treturn sysctl_sched_runtime_limit*2;", 
      "-\treturn tmp;", 
      "-}", 
      "-#else", 
      "-static inline unsigned long", 
      "-calc_weighted(unsigned long delta, unsigned long weight, int shift)", 
      "-{", 
      "-\treturn delta * weight >> shift;", 
      "-}", 
      "-#endif", 
      "-", 
      "-/*", 
      "* Task is being enqueued - update stats:", 
      "*/", 
      "static void update_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\ts64 key;", 
      "-", 
      "/*", 
      "* Are we enqueueing a waiting task? (for current tasks", 
      "* a dequeue/enqueue event is a NOP)", 
      "*/", 
      "-\tif (se != cfs_rq_curr(cfs_rq))", 
      "+\tif (se != cfs_rq->curr)", 
      "update_stats_wait_start(cfs_rq, se);", 
      "-\t/*", 
      "-\t * Update the key:", 
      "-\t */", 
      "-\tkey = cfs_rq->fair_clock;", 
      "-", 
      "-\t/*", 
      "-\t * Optimize the common nice 0 case:", 
      "-\t */", 
      "-\tif (likely(se->load.weight == NICE_0_LOAD)) {", 
      "-\t\tkey -= se->wait_runtime;", 
      "-\t} else {", 
      "-\t\tu64 tmp;", 
      "-", 
      "-\t\tif (se->wait_runtime < 0) {", 
      "-\t\t\ttmp = -se->wait_runtime;", 
      "-\t\t\tkey += (tmp * se->load.inv_weight) >>", 
      "-\t\t\t\t\t(WMULT_SHIFT - NICE_0_SHIFT);", 
      "-\t\t} else {", 
      "-\t\t\ttmp = se->wait_runtime;", 
      "-\t\t\tkey -= (tmp * se->load.inv_weight) >>", 
      "-\t\t\t\t\t(WMULT_SHIFT - NICE_0_SHIFT);", 
      "-\t\t}", 
      "-\t}", 
      "-", 
      "-\tse->fair_key = key;", 
      "-}", 
      "-", 
      "-/*", 
      "- * Note: must be called with a freshly updated rq->fair_clock.", 
      "- */", 
      "-static inline void", 
      "-__update_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "-{", 
      "-\tunsigned long delta_fair = se->delta_fair_run;", 
      "-", 
      "-\tschedstat_set(se->wait_max, max(se->wait_max,", 
      "-\t\t\trq_of(cfs_rq)->clock - se->wait_start));", 
      "-", 
      "-\tif (unlikely(se->load.weight != NICE_0_LOAD))", 
      "-\t\tdelta_fair = calc_weighted(delta_fair, se->load.weight,", 
      "-\t\t\t\t\t\t\tNICE_0_SHIFT);", 
      "-", 
      "-\tadd_wait_runtime(cfs_rq, se, delta_fair);", 
      "}", 
      "", 
      "static void", 
      "update_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\tunsigned long delta_fair;", 
      "-", 
      "-\tif (unlikely(!se->wait_start_fair))", 
      "-\t\treturn;", 
      "-", 
      "-\tdelta_fair = (unsigned long)min((u64)(2*sysctl_sched_runtime_limit),", 
      "-\t\t\t(u64)(cfs_rq->fair_clock - se->wait_start_fair));", 
      "-", 
      "-\tse->delta_fair_run += delta_fair;", 
      "-\tif (unlikely(abs(se->delta_fair_run) >=", 
      "-\t\t\t\tsysctl_sched_stat_granularity)) {", 
      "-\t\t__update_stats_wait_end(cfs_rq, se);", 
      "-\t\tse->delta_fair_run = 0;", 
      "-\t}", 
      "-", 
      "-\tse->wait_start_fair = 0;", 
      "+\tschedstat_set(se->wait_max, max(se->wait_max,", 
      "+\t\t\trq_of(cfs_rq)->clock - se->wait_start));", 
      "schedstat_set(se->wait_start, 0);", 
      "}", 
      "", 
      "static inline void", 
      "update_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\tupdate_curr(cfs_rq);", 
      "/*", 
      "* Mark the end of the wait period if dequeueing a", 
      "* waiting task:", 
      "*/", 
      "-\tif (se != cfs_rq_curr(cfs_rq))", 
      "+\tif (se != cfs_rq->curr)", 
      "update_stats_wait_end(cfs_rq, se);", 
      "}", 
      "", 
      "@@ -542,79 +409,28 @@ update_stats_curr_start(struct cfs_rq *c", 
      "se->exec_start = rq_of(cfs_rq)->clock;", 
      "}", 
      "", 
      "-/*", 
      "- * We are descheduling a task - update its stats:", 
      "- */", 
      "-static inline void", 
      "-update_stats_curr_end(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "-{", 
      "-\tse->exec_start = 0;", 
      "-}", 
      "-", 
      "/**************************************************", 
      "* Scheduling class queueing methods:", 
      "*/", 
      "", 
      "-static void __enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "+static void", 
      "+account_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\tunsigned long load = cfs_rq->load.weight, delta_fair;", 
      "-\tlong prev_runtime;", 
      "-", 
      "-\t/*", 
      "-\t * Do not boost sleepers if there's too much bonus 'in flight'", 
      "-\t * already:", 
      "-\t */", 
      "-\tif (unlikely(cfs_rq->sleeper_bonus > sysctl_sched_runtime_limit))", 
      "-\t\treturn;", 
      "-", 
      "-\tif (sysctl_sched_features & SCHED_FEAT_SLEEPER_LOAD_AVG)", 
      "-\t\tload = rq_of(cfs_rq)->cpu_load[2];", 
      "-", 
      "-\tdelta_fair = se->delta_fair_sleep;", 
      "-", 
      "-\t/*", 
      "-\t * Fix up delta_fair with the effect of us running", 
      "-\t * during the whole sleep period:", 
      "-\t */", 
      "-\tif (sysctl_sched_features & SCHED_FEAT_SLEEPER_AVG)", 
      "-\t\tdelta_fair = div64_likely32((u64)delta_fair * load,", 
      "-\t\t\t\t\t\tload + se->load.weight);", 
      "-", 
      "-\tif (unlikely(se->load.weight != NICE_0_LOAD))", 
      "-\t\tdelta_fair = calc_weighted(delta_fair, se->load.weight,", 
      "-\t\t\t\t\t\t\tNICE_0_SHIFT);", 
      "-", 
      "-\tprev_runtime = se->wait_runtime;", 
      "-\t__add_wait_runtime(cfs_rq, se, delta_fair);", 
      "-\tdelta_fair = se->wait_runtime - prev_runtime;", 
      "+\tupdate_load_add(&cfs_rq->load, se->load.weight);", 
      "+\tcfs_rq->nr_running++;", 
      "+\tse->on_rq = 1;", 
      "+}", 
      "", 
      "-\t/*", 
      "-\t * Track the amount of bonus we've given to sleepers:", 
      "-\t */", 
      "-\tcfs_rq->sleeper_bonus += delta_fair;", 
      "+static void", 
      "+account_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "+{", 
      "+\tupdate_load_sub(&cfs_rq->load, se->load.weight);", 
      "+\tcfs_rq->nr_running--;", 
      "+\tse->on_rq = 0;", 
      "}", 
      "", 
      "static void enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\tstruct task_struct *tsk = task_of(se);", 
      "-\tunsigned long delta_fair;", 
      "-", 
      "-\tif ((entity_is_task(se) && tsk->policy == SCHED_BATCH) ||", 
      "-\t\t\t !(sysctl_sched_features & SCHED_FEAT_FAIR_SLEEPERS))", 
      "-\t\treturn;", 
      "-", 
      "-\tdelta_fair = (unsigned long)min((u64)(2*sysctl_sched_runtime_limit),", 
      "-\t\t(u64)(cfs_rq->fair_clock - se->sleep_start_fair));", 
      "-", 
      "-\tse->delta_fair_sleep += delta_fair;", 
      "-\tif (unlikely(abs(se->delta_fair_sleep) >=", 
      "-\t\t\t\tsysctl_sched_stat_granularity)) {", 
      "-\t\t__enqueue_sleeper(cfs_rq, se);", 
      "-\t\tse->delta_fair_sleep = 0;", 
      "-\t}", 
      "-", 
      "-\tse->sleep_start_fair = 0;", 
      "-", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "if (se->sleep_start) {", 
      "u64 delta = rq_of(cfs_rq)->clock - se->sleep_start;", 
      "@@ -646,6 +462,8 @@ static void enqueue_sleeper(struct cfs_r", 
      "* time that the task spent sleeping:", 
      "*/", 
      "if (unlikely(immediate_read(prof_on) == SLEEP_PROFILING)) {", 
      "+\t\t\tstruct task_struct *tsk = task_of(se);", 
      "+", 
      "profile_hits(SLEEP_PROFILING, (void *)get_wchan(tsk),", 
      "delta >> 20);", 
      "}", 
      "@@ -653,27 +471,86 @@ static void enqueue_sleeper(struct cfs_r", 
      "#endif", 
      "}", 
      "", 
      "+static void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "+{", 
      "+#ifdef CONFIG_SCHED_DEBUG", 
      "+\ts64 d = se->vruntime - cfs_rq->min_vruntime;", 
      "+", 
      "+\tif (d < 0)", 
      "+\t\td = -d;", 
      "+", 
      "+\tif (d > 3*sysctl_sched_latency)", 
      "+\t\tschedstat_inc(cfs_rq, nr_spread_over);", 
      "+#endif", 
      "+}", 
      "+", 
      "+static void", 
      "+place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial)", 
      "+{", 
      "+\tu64 vruntime;", 
      "+", 
      "+\tvruntime = cfs_rq->min_vruntime;", 
      "+", 
      "+\tif (sched_feat(TREE_AVG)) {", 
      "+\t\tstruct sched_entity *last = __pick_last_entity(cfs_rq);", 
      "+\t\tif (last) {", 
      "+\t\t\tvruntime += last->vruntime;", 
      "+\t\t\tvruntime >>= 1;", 
      "+\t\t}", 
      "+\t} else if (sched_feat(APPROX_AVG) && cfs_rq->nr_running)", 
      "+\t\tvruntime += sched_vslice(cfs_rq)/2;", 
      "+", 
      "+\t/*", 
      "+\t * The 'current' period is already promised to the current tasks,", 
      "+\t * however the extra weight of the new task will slow them down a", 
      "+\t * little, place the new task so that it fits in the slot that", 
      "+\t * stays open at the end.", 
      "+\t */", 
      "+\tif (initial && sched_feat(START_DEBIT))", 
      "+\t\tvruntime += sched_vslice_add(cfs_rq, se);", 
      "+", 
      "+\tif (!initial) {", 
      "+\t\t/* sleeps upto a single latency don't count. */", 
      "+\t\tif (sched_feat(NEW_FAIR_SLEEPERS) && entity_is_task(se))", 
      "+\t\t\tvruntime -= sysctl_sched_latency;", 
      "+", 
      "+\t\t/* ensure we never gain time by being placed backwards. */", 
      "+\t\tvruntime = max_vruntime(se->vruntime, vruntime);", 
      "+\t}", 
      "+", 
      "+\tse->vruntime = vruntime;", 
      "+}", 
      "+", 
      "static void", 
      "enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int wakeup)", 
      "{", 
      "/*", 
      "-\t * Update the fair clock.", 
      "+\t * Update run-time statistics of the 'current'.", 
      "*/", 
      "update_curr(cfs_rq);", 
      "", 
      "-\tif (wakeup)", 
      "+\tif (wakeup) {", 
      "+\t\tplace_entity(cfs_rq, se, 0);", 
      "enqueue_sleeper(cfs_rq, se);", 
      "+\t}", 
      "", 
      "update_stats_enqueue(cfs_rq, se);", 
      "-\t__enqueue_entity(cfs_rq, se);", 
      "+\tcheck_spread(cfs_rq, se);", 
      "+\tif (se != cfs_rq->curr)", 
      "+\t\t__enqueue_entity(cfs_rq, se);", 
      "+\taccount_entity_enqueue(cfs_rq, se);", 
      "}", 
      "", 
      "static void", 
      "dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int sleep)", 
      "{", 
      "+\t/*", 
      "+\t * Update run-time statistics of the 'current'.", 
      "+\t */", 
      "+\tupdate_curr(cfs_rq);", 
      "+", 
      "update_stats_dequeue(cfs_rq, se);", 
      "if (sleep) {", 
      "-\t\tse->sleep_start_fair = cfs_rq->fair_clock;", 
      "#ifdef CONFIG_SCHEDSTATS", 
      "if (entity_is_task(se)) {", 
      "struct task_struct *tsk = task_of(se);", 
      "@@ -685,68 +562,64 @@ dequeue_entity(struct cfs_rq *cfs_rq, st", 
      "}", 
      "#endif", 
      "}", 
      "-\t__dequeue_entity(cfs_rq, se);", 
      "+", 
      "+\tif (se != cfs_rq->curr)", 
      "+\t\t__dequeue_entity(cfs_rq, se);", 
      "+\taccount_entity_dequeue(cfs_rq, se);", 
      "}", 
      "", 
      "/*", 
      "* Preempt the current task with a newly woken task if needed:", 
      "*/", 
      "static void", 
      "-__check_preempt_curr_fair(struct cfs_rq *cfs_rq, struct sched_entity *se,", 
      "-\t\t\t  struct sched_entity *curr, unsigned long granularity)", 
      "+check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)", 
      "{", 
      "-\ts64 __delta = curr->fair_key - se->fair_key;", 
      "unsigned long ideal_runtime, delta_exec;", 
      "", 
      "-\t/*", 
      "-\t * ideal_runtime is compared against sum_exec_runtime, which is", 
      "-\t * walltime, hence do not scale.", 
      "-\t */", 
      "-\tideal_runtime = max(sysctl_sched_latency / cfs_rq->nr_running,", 
      "-\t\t\t(unsigned long)sysctl_sched_min_granularity);", 
      "-", 
      "-\t/*", 
      "-\t * If we executed more than what the latency constraint suggests,", 
      "-\t * reduce the rescheduling granularity. This way the total latency", 
      "-\t * of how much a task is not scheduled converges to", 
      "-\t * sysctl_sched_latency:", 
      "-\t */", 
      "+\tideal_runtime = sched_slice(cfs_rq, curr);", 
      "delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;", 
      "if (delta_exec > ideal_runtime)", 
      "-\t\tgranularity = 0;", 
      "-", 
      "-\t/*", 
      "-\t * Take scheduling granularity into account - do not", 
      "-\t * preempt the current task unless the best task has", 
      "-\t * a larger than sched_granularity fairness advantage:", 
      "-\t *", 
      "-\t * scale granularity as key space is in fair_clock.", 
      "-\t */", 
      "-\tif (__delta > niced_granularity(curr, granularity))", 
      "resched_task(rq_of(cfs_rq)->curr);", 
      "}", 
      "", 
      "-static inline void", 
      "+static void", 
      "set_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)", 
      "{", 
      "-\t/*", 
      "-\t * Any task has to be enqueued before it get to execute on", 
      "-\t * a CPU. So account for the time it spent waiting on the", 
      "-\t * runqueue. (note, here we rely on pick_next_task() having", 
      "-\t * done a put_prev_task_fair() shortly before this, which", 
      "-\t * updated rq->fair_clock - used by update_stats_wait_end())", 
      "-\t */", 
      "-\tupdate_stats_wait_end(cfs_rq, se);", 
      "+\t/* 'current' is not kept within the tree. */", 
      "+\tif (se->on_rq) {", 
      "+\t\t/*", 
      "+\t\t * Any task has to be enqueued before it get to execute on", 
      "+\t\t * a CPU. So account for the time it spent waiting on the", 
      "+\t\t * runqueue.", 
      "+\t\t */", 
      "+\t\tupdate_stats_wait_end(cfs_rq, se);", 
      "+\t\t__dequeue_entity(cfs_rq, se);", 
      "+\t}", 
      "+", 
      "update_stats_curr_start(cfs_rq, se);", 
      "-\tset_cfs_rq_curr(cfs_rq, se);", 
      "+\tcfs_rq->curr = se;", 
      "+#ifdef CONFIG_SCHEDSTATS", 
      "+\t/*", 
      "+\t * Track our maximum slice length, if the CPU's load is at", 
      "+\t * least twice that of our own weight (i.e. dont track it", 
      "+\t * when there are only lesser-weight tasks around):", 
      "+\t */", 
      "+\tif (rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {", 
      "+\t\tse->slice_max = max(se->slice_max,", 
      "+\t\t\tse->sum_exec_runtime - se->prev_sum_exec_runtime);", 
      "+\t}", 
      "+#endif", 
      "se->prev_sum_exec_runtime = se->sum_exec_runtime;", 
      "}", 
      "", 
      "static struct sched_entity *pick_next_entity(struct cfs_rq *cfs_rq)", 
      "{", 
      "-\tstruct sched_entity *se = __pick_next_entity(cfs_rq);", 
      "+\tstruct sched_entity *se = NULL;", 
      "", 
      "-\tset_next_entity(cfs_rq, se);", 
      "+\tif (first_fair(cfs_rq)) {", 
      "+\t\tse = __pick_next_entity(cfs_rq);", 
      "+\t\tset_next_entity(cfs_rq, se);", 
      "+\t}", 
      "", 
      "return se;", 
      "}", 
      "@@ -760,33 +633,24 @@ static void put_prev_entity(struct cfs_r", 
      "if (prev->on_rq)", 
      "update_curr(cfs_rq);", 
      "", 
      "-\tupdate_stats_curr_end(cfs_rq, prev);", 
      "-", 
      "-\tif (prev->on_rq)", 
      "+\tcheck_spread(cfs_rq, prev);", 
      "+\tif (prev->on_rq) {", 
      "update_stats_wait_start(cfs_rq, prev);", 
      "-\tset_cfs_rq_curr(cfs_rq, NULL);", 
      "+\t\t/* Put 'current' back into the tree. */", 
      "+\t\t__enqueue_entity(cfs_rq, prev);", 
      "+\t}", 
      "+\tcfs_rq->curr = NULL;", 
      "}", 
      "", 
      "static void entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)", 
      "{", 
      "-\tstruct sched_entity *next;", 
      "-", 
      "-\t/*", 
      "-\t * Dequeue and enqueue the task to update its", 
      "-\t * position within the tree:", 
      "-\t */", 
      "-\tdequeue_entity(cfs_rq, curr, 0);", 
      "-\tenqueue_entity(cfs_rq, curr, 0);", 
      "-", 
      "/*", 
      "-\t * Reschedule if another task tops the current one.", 
      "+\t * Update run-time statistics of the 'current'.", 
      "*/", 
      "-\tnext = __pick_next_entity(cfs_rq);", 
      "-\tif (next == curr)", 
      "-\t\treturn;", 
      "+\tupdate_curr(cfs_rq);", 
      "", 
      "-\t__check_preempt_curr_fair(cfs_rq, next, curr,", 
      "-\t\t\tsched_granularity(cfs_rq));", 
      "+\tif (cfs_rq->nr_running > 1 || !sched_feat(WAKEUP_PREEMPT))", 
      "+\t\tcheck_preempt_tick(cfs_rq, curr);", 
      "}", 
      "", 
      "/**************************************************", 
      "@@ -821,23 +685,28 @@ static inline struct cfs_rq *group_cfs_r", 
      "*/", 
      "static inline struct cfs_rq *cpu_cfs_rq(struct cfs_rq *cfs_rq, int this_cpu)", 
      "{", 
      "-\t/* A later patch will take group into account */", 
      "-\treturn &cpu_rq(this_cpu)->cfs;", 
      "+\treturn cfs_rq->tg->cfs_rq[this_cpu];", 
      "}", 
      "", 
      "/* Iterate thr' all leaf cfs_rq's on a runqueue */", 
      "#define for_each_leaf_cfs_rq(rq, cfs_rq) \\", 
      "list_for_each_entry(cfs_rq, &rq->leaf_cfs_rq_list, leaf_cfs_rq_list)", 
      "", 
      "-/* Do the two (enqueued) tasks belong to the same group ? */", 
      "-static inline int is_same_group(struct task_struct *curr, struct task_struct *p)", 
      "+/* Do the two (enqueued) entities belong to the same group ? */", 
      "+static inline int", 
      "+is_same_group(struct sched_entity *se, struct sched_entity *pse)", 
      "{", 
      "-\tif (curr->se.cfs_rq == p->se.cfs_rq)", 
      "+\tif (se->cfs_rq == pse->cfs_rq)", 
      "return 1;", 
      "", 
      "return 0;", 
      "}", 
      "", 
      "+static inline struct sched_entity *parent_entity(struct sched_entity *se)", 
      "+{", 
      "+\treturn se->parent;", 
      "+}", 
      "+", 
      "#else\t/* CONFIG_FAIR_GROUP_SCHED */", 
      "", 
      "#define for_each_sched_entity(se) \\", 
      "@@ -870,11 +739,17 @@ static inline struct cfs_rq *cpu_cfs_rq(", 
      "#define for_each_leaf_cfs_rq(rq, cfs_rq) \\", 
      "for (cfs_rq = &rq->cfs; cfs_rq; cfs_rq = NULL)", 
      "", 
      "-static inline int is_same_group(struct task_struct *curr, struct task_struct *p)", 
      "+static inline int", 
      "+is_same_group(struct sched_entity *se, struct sched_entity *pse)", 
      "{", 
      "return 1;", 
      "}", 
      "", 
      "+static inline struct sched_entity *parent_entity(struct sched_entity *se)", 
      "+{", 
      "+\treturn NULL;", 
      "+}", 
      "+", 
      "#endif\t/* CONFIG_FAIR_GROUP_SCHED */", 
      "", 
      "/*", 
      "@@ -892,6 +767,7 @@ static void enqueue_task_fair(struct rq", 
      "break;", 
      "cfs_rq = cfs_rq_of(se);", 
      "enqueue_entity(cfs_rq, se, wakeup);", 
      "+\t\twakeup = 1;", 
      "}", 
      "}", 
      "", 
      "@@ -911,6 +787,7 @@ static void dequeue_task_fair(struct rq", 
      "/* Don't dequeue parent if it has other entities besides us */", 
      "if (cfs_rq->load.weight)", 
      "break;", 
      "+\t\tsleep = 1;", 
      "}", 
      "}", 
      "", 
      "@@ -919,12 +796,11 @@ static void dequeue_task_fair(struct rq", 
      "*", 
      "* If compat_yield is turned on then we requeue to the end of the tree.", 
      "*/", 
      "-static void yield_task_fair(struct rq *rq, struct task_struct *p)", 
      "+static void yield_task_fair(struct rq *rq)", 
      "{", 
      "-\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);", 
      "-\tstruct rb_node **link = &cfs_rq->tasks_timeline.rb_node;", 
      "-\tstruct sched_entity *rightmost, *se = &p->se;", 
      "-\tstruct rb_node *parent;", 
      "+\tstruct task_struct *curr = rq->curr;", 
      "+\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);", 
      "+\tstruct sched_entity *rightmost, *se = &curr->se;", 
      "", 
      "/*", 
      "* Are we the only task in the tree?", 
      "@@ -932,54 +808,41 @@ static void yield_task_fair(struct rq *r", 
      "if (unlikely(cfs_rq->nr_running == 1))", 
      "return;", 
      "", 
      "-\tif (likely(!sysctl_sched_compat_yield)) {", 
      "+\tif (likely(!sysctl_sched_compat_yield) && curr->policy != SCHED_BATCH) {", 
      "__update_rq_clock(rq);", 
      "/*", 
      "-\t\t * Dequeue and enqueue the task to update its", 
      "-\t\t * position within the tree:", 
      "+\t\t * Update run-time statistics of the 'current'.", 
      "*/", 
      "-\t\tdequeue_entity(cfs_rq, &p->se, 0);", 
      "-\t\tenqueue_entity(cfs_rq, &p->se, 0);", 
      "+\t\tupdate_curr(cfs_rq);", 
      "", 
      "return;", 
      "}", 
      "/*", 
      "* Find the rightmost entry in the rbtree:", 
      "*/", 
      "-\tdo {", 
      "-\t\tparent = *link;", 
      "-\t\tlink = &parent->rb_right;", 
      "-\t} while (*link);", 
      "-", 
      "-\trightmost = rb_entry(parent, struct sched_entity, run_node);", 
      "+\trightmost = __pick_last_entity(cfs_rq);", 
      "/*", 
      "* Already in the rightmost position?", 
      "*/", 
      "-\tif (unlikely(rightmost == se))", 
      "+\tif (unlikely(rightmost->vruntime < se->vruntime))", 
      "return;", 
      "", 
      "/*", 
      "* Minimally necessary key value to be last in the tree:", 
      "+\t * Upon rescheduling, sched_class::put_prev_task() will place", 
      "+\t * 'current' within the tree based on its new key value.", 
      "*/", 
      "-\tse->fair_key = rightmost->fair_key + 1;", 
      "-", 
      "-\tif (cfs_rq->rb_leftmost == &se->run_node)", 
      "-\t\tcfs_rq->rb_leftmost = rb_next(&se->run_node);", 
      "-\t/*", 
      "-\t * Relink the task to the rightmost position:", 
      "-\t */", 
      "-\trb_erase(&se->run_node, &cfs_rq->tasks_timeline);", 
      "-\trb_link_node(&se->run_node, parent, link);", 
      "-\trb_insert_color(&se->run_node, &cfs_rq->tasks_timeline);", 
      "+\tse->vruntime = rightmost->vruntime + 1;", 
      "}", 
      "", 
      "/*", 
      "* Preempt the current task with a newly woken task if needed:", 
      "*/", 
      "-static void check_preempt_curr_fair(struct rq *rq, struct task_struct *p)", 
      "+static void check_preempt_wakeup(struct rq *rq, struct task_struct *p)", 
      "{", 
      "struct task_struct *curr = rq->curr;", 
      "struct cfs_rq *cfs_rq = task_cfs_rq(curr);", 
      "+\tstruct sched_entity *se = &curr->se, *pse = &p->se;", 
      "unsigned long gran;", 
      "", 
      "if (unlikely(rt_prio(p->prio))) {", 
      "@@ -988,16 +851,27 @@ static void check_preempt_curr_fair(stru", 
      "resched_task(curr);", 
      "return;", 
      "}", 
      "-", 
      "-\tgran = sysctl_sched_wakeup_granularity;", 
      "/*", 
      "-\t * Batch tasks prefer throughput over latency:", 
      "+\t * Batch tasks do not preempt (their preemption is driven by", 
      "+\t * the tick):", 
      "*/", 
      "if (unlikely(p->policy == SCHED_BATCH))", 
      "-\t\tgran = sysctl_sched_batch_wakeup_granularity;", 
      "+\t\treturn;", 
      "+", 
      "+\tif (!sched_feat(WAKEUP_PREEMPT))", 
      "+\t\treturn;", 
      "+", 
      "+\twhile (!is_same_group(se, pse)) {", 
      "+\t\tse = parent_entity(se);", 
      "+\t\tpse = parent_entity(pse);", 
      "+\t}", 
      "", 
      "-\tif (is_same_group(curr, p))", 
      "-\t\t__check_preempt_curr_fair(cfs_rq, &p->se, &curr->se, gran);", 
      "+\tgran = sysctl_sched_wakeup_granularity;", 
      "+\tif (unlikely(se->load.weight != NICE_0_LOAD))", 
      "+\t\tgran = calc_delta_fair(gran, &se->load);", 
      "+", 
      "+\tif (pse->vruntime + gran < se->vruntime)", 
      "+\t\tresched_task(curr);", 
      "}", 
      "", 
      "static struct task_struct *pick_next_task_fair(struct rq *rq)", 
      "@@ -1030,6 +904,7 @@ static void put_prev_task_fair(struct rq", 
      "}", 
      "}", 
      "", 
      "+#ifdef CONFIG_SMP", 
      "/**************************************************", 
      "* Fair scheduling class load-balancing methods:", 
      "*/", 
      "@@ -1041,7 +916,7 @@ static void put_prev_task_fair(struct rq", 
      "* achieve that by always pre-iterating before returning", 
      "* the current task:", 
      "*/", 
      "-static inline struct task_struct *", 
      "+static struct task_struct *", 
      "__load_balance_iterator(struct cfs_rq *cfs_rq, struct rb_node *curr)", 
      "{", 
      "struct task_struct *p;", 
      "@@ -1078,7 +953,10 @@ static int cfs_rq_best_prio(struct cfs_r", 
      "if (!cfs_rq->nr_running)", 
      "return MAX_PRIO;", 
      "", 
      "-\tcurr = __pick_next_entity(cfs_rq);", 
      "+\tcurr = cfs_rq->curr;", 
      "+\tif (!curr)", 
      "+\t\tcurr = __pick_next_entity(cfs_rq);", 
      "+", 
      "p = task_of(curr);", 
      "", 
      "return p->prio;", 
      "@@ -1087,12 +965,11 @@ static int cfs_rq_best_prio(struct cfs_r", 
      "", 
      "static unsigned long", 
      "load_balance_fair(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "-\t\t  unsigned long max_nr_move, unsigned long max_load_move,", 
      "+\t\t  unsigned long max_load_move,", 
      "struct sched_domain *sd, enum cpu_idle_type idle,", 
      "int *all_pinned, int *this_best_prio)", 
      "{", 
      "struct cfs_rq *busy_cfs_rq;", 
      "-\tunsigned long load_moved, total_nr_moved = 0, nr_moved;", 
      "long rem_load_move = max_load_move;", 
      "struct rq_iterator cfs_rq_iterator;", 
      "", 
      "@@ -1120,25 +997,48 @@ load_balance_fair(struct rq *this_rq, in", 
      "#else", 
      "# define maxload rem_load_move", 
      "#endif", 
      "-\t\t/* pass busy_cfs_rq argument into", 
      "+\t\t/*", 
      "+\t\t * pass busy_cfs_rq argument into", 
      "* load_balance_[start|next]_fair iterators", 
      "*/", 
      "cfs_rq_iterator.arg = busy_cfs_rq;", 
      "-\t\tnr_moved = balance_tasks(this_rq, this_cpu, busiest,", 
      "-\t\t\t\tmax_nr_move, maxload, sd, idle, all_pinned,", 
      "-\t\t\t\t&load_moved, this_best_prio, &cfs_rq_iterator);", 
      "-", 
      "-\t\ttotal_nr_moved += nr_moved;", 
      "-\t\tmax_nr_move -= nr_moved;", 
      "-\t\trem_load_move -= load_moved;", 
      "+\t\trem_load_move -= balance_tasks(this_rq, this_cpu, busiest,", 
      "+\t\t\t\t\t       maxload, sd, idle, all_pinned,", 
      "+\t\t\t\t\t       this_best_prio,", 
      "+\t\t\t\t\t       &cfs_rq_iterator);", 
      "", 
      "-\t\tif (max_nr_move <= 0 || rem_load_move <= 0)", 
      "+\t\tif (rem_load_move <= 0)", 
      "break;", 
      "}", 
      "", 
      "return max_load_move - rem_load_move;", 
      "}", 
      "", 
      "+static int", 
      "+move_one_task_fair(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "+\t\t   struct sched_domain *sd, enum cpu_idle_type idle)", 
      "+{", 
      "+\tstruct cfs_rq *busy_cfs_rq;", 
      "+\tstruct rq_iterator cfs_rq_iterator;", 
      "+", 
      "+\tcfs_rq_iterator.start = load_balance_start_fair;", 
      "+\tcfs_rq_iterator.next = load_balance_next_fair;", 
      "+", 
      "+\tfor_each_leaf_cfs_rq(busiest, busy_cfs_rq) {", 
      "+\t\t/*", 
      "+\t\t * pass busy_cfs_rq argument into", 
      "+\t\t * load_balance_[start|next]_fair iterators", 
      "+\t\t */", 
      "+\t\tcfs_rq_iterator.arg = busy_cfs_rq;", 
      "+\t\tif (iter_move_one_task(this_rq, this_cpu, busiest, sd, idle,", 
      "+\t\t\t\t       &cfs_rq_iterator))", 
      "+\t\t    return 1;", 
      "+\t}", 
      "+", 
      "+\treturn 0;", 
      "+}", 
      "+#endif", 
      "+", 
      "/*", 
      "* scheduler tick hitting a task of our scheduling class:", 
      "*/", 
      "@@ -1153,6 +1053,8 @@ static void task_tick_fair(struct rq *rq", 
      "}", 
      "}", 
      "", 
      "+#define swap(a, b) do { typeof(a) tmp = (a); (a) = (b); (b) = tmp; } while (0)", 
      "+", 
      "/*", 
      "* Share the fairness runtime between parent and child, thus the", 
      "* total amount of pressure for CPU stays equal - new tasks", 
      "@@ -1163,37 +1065,28 @@ static void task_tick_fair(struct rq *rq", 
      "static void task_new_fair(struct rq *rq, struct task_struct *p)", 
      "{", 
      "struct cfs_rq *cfs_rq = task_cfs_rq(p);", 
      "-\tstruct sched_entity *se = &p->se, *curr = cfs_rq_curr(cfs_rq);", 
      "+\tstruct sched_entity *se = &p->se, *curr = cfs_rq->curr;", 
      "+\tint this_cpu = smp_processor_id();", 
      "", 
      "sched_info_queued(p);", 
      "", 
      "update_curr(cfs_rq);", 
      "-\tupdate_stats_enqueue(cfs_rq, se);", 
      "-\t/*", 
      "-\t * Child runs first: we let it run before the parent", 
      "-\t * until it reschedules once. We set up the key so that", 
      "-\t * it will preempt the parent:", 
      "-\t */", 
      "-\tse->fair_key = curr->fair_key -", 
      "-\t\tniced_granularity(curr, sched_granularity(cfs_rq)) - 1;", 
      "-\t/*", 
      "-\t * The first wait is dominated by the child-runs-first logic,", 
      "-\t * so do not credit it with that waiting time yet:", 
      "-\t */", 
      "-\tif (sysctl_sched_features & SCHED_FEAT_SKIP_INITIAL)", 
      "-\t\tse->wait_start_fair = 0;", 
      "+\tplace_entity(cfs_rq, se, 1);", 
      "", 
      "-\t/*", 
      "-\t * The statistical average of wait_runtime is about", 
      "-\t * -granularity/2, so initialize the task with that:", 
      "-\t */", 
      "-\tif (sysctl_sched_features & SCHED_FEAT_START_DEBIT)", 
      "-\t\tse->wait_runtime = -(sched_granularity(cfs_rq) / 2);", 
      "+\t/* 'curr' will be NULL if the child belongs to a different group */", 
      "+\tif (sysctl_sched_child_runs_first && this_cpu == task_cpu(p) &&", 
      "+\t\t\tcurr && curr->vruntime < se->vruntime) {", 
      "+\t\t/*", 
      "+\t\t * Upon rescheduling, sched_class::put_prev_task() will place", 
      "+\t\t * 'current' within the tree based on its new key value.", 
      "+\t\t */", 
      "+\t\tswap(curr->vruntime, se->vruntime);", 
      "+\t}", 
      "", 
      "-\t__enqueue_entity(cfs_rq, se);", 
      "+\tenqueue_task_fair(rq, p, 0);", 
      "+\tresched_task(rq->curr);", 
      "}", 
      "", 
      "-#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "/* Account for a task changing its policy or group.", 
      "*", 
      "* This routine is mostly called to set cfs_rq->curr field when a task", 
      "@@ -1206,26 +1099,25 @@ static void set_curr_task_fair(struct rq", 
      "for_each_sched_entity(se)", 
      "set_next_entity(cfs_rq_of(se), se);", 
      "}", 
      "-#else", 
      "-static void set_curr_task_fair(struct rq *rq)", 
      "-{", 
      "-}", 
      "-#endif", 
      "", 
      "/*", 
      "* All the scheduling class methods:", 
      "*/", 
      "-struct sched_class fair_sched_class __read_mostly = {", 
      "+static const struct sched_class fair_sched_class = {", 
      "+\t.next\t\t\t= &idle_sched_class,", 
      ".enqueue_task\t\t= enqueue_task_fair,", 
      ".dequeue_task\t\t= dequeue_task_fair,", 
      ".yield_task\t\t= yield_task_fair,", 
      "", 
      "-\t.check_preempt_curr\t= check_preempt_curr_fair,", 
      "+\t.check_preempt_curr\t= check_preempt_wakeup,", 
      "", 
      ".pick_next_task\t\t= pick_next_task_fair,", 
      ".put_prev_task\t\t= put_prev_task_fair,", 
      "", 
      "+#ifdef CONFIG_SMP", 
      ".load_balance\t\t= load_balance_fair,", 
      "+\t.move_one_task\t\t= move_one_task_fair,", 
      "+#endif", 
      "", 
      ".set_curr_task          = set_curr_task_fair,", 
      ".task_tick\t\t= task_tick_fair,", 
      "@@ -1237,6 +1129,9 @@ static void print_cfs_stats(struct seq_f", 
      "{", 
      "struct cfs_rq *cfs_rq;", 
      "", 
      "+#ifdef CONFIG_FAIR_GROUP_SCHED", 
      "+\tprint_cfs_rq(m, cpu, &cpu_rq(cpu)->cfs);", 
      "+#endif", 
      "for_each_leaf_cfs_rq(cpu_rq(cpu), cfs_rq)", 
      "print_cfs_rq(m, cpu, cfs_rq);", 
      "}"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/sched_idletask.c", 
    "linux-2.6.23/kernel/sched_idletask.c", 
    [
      "Index: linux-2.6.23/kernel/sched_idletask.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/sched_idletask.c", 
      "+++ linux-2.6.23/kernel/sched_idletask.c", 
      "@@ -37,23 +37,37 @@ static void put_prev_task_idle(struct rq", 
      "{", 
      "}", 
      "", 
      "+#ifdef CONFIG_SMP", 
      "static unsigned long", 
      "load_balance_idle(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "-\t\t\tunsigned long max_nr_move, unsigned long max_load_move,", 
      "-\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,", 
      "-\t\t\tint *all_pinned, int *this_best_prio)", 
      "+\t\t  unsigned long max_load_move,", 
      "+\t\t  struct sched_domain *sd, enum cpu_idle_type idle,", 
      "+\t\t  int *all_pinned, int *this_best_prio)", 
      "{", 
      "return 0;", 
      "}", 
      "", 
      "+static int", 
      "+move_one_task_idle(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "+\t\t   struct sched_domain *sd, enum cpu_idle_type idle)", 
      "+{", 
      "+\treturn 0;", 
      "+}", 
      "+#endif", 
      "+", 
      "static void task_tick_idle(struct rq *rq, struct task_struct *curr)", 
      "{", 
      "}", 
      "", 
      "+static void set_curr_task_idle(struct rq *rq)", 
      "+{", 
      "+}", 
      "+", 
      "/*", 
      "* Simple, special scheduling class for the per-CPU idle tasks:", 
      "*/", 
      "-static struct sched_class idle_sched_class __read_mostly = {", 
      "+const struct sched_class idle_sched_class = {", 
      "+\t/* .next is NULL */", 
      "/* no enqueue/yield_task for idle tasks */", 
      "", 
      "/* dequeue is not valid, we print a debug message there: */", 
      "@@ -64,8 +78,12 @@ static struct sched_class idle_sched_cla", 
      ".pick_next_task\t\t= pick_next_task_idle,", 
      ".put_prev_task\t\t= put_prev_task_idle,", 
      "", 
      "+#ifdef CONFIG_SMP", 
      ".load_balance\t\t= load_balance_idle,", 
      "+\t.move_one_task\t\t= move_one_task_idle,", 
      "+#endif", 
      "", 
      "+\t.set_curr_task          = set_curr_task_idle,", 
      ".task_tick\t\t= task_tick_idle,", 
      "/* no .task_new for idle tasks */", 
      "};"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/sched_rt.c", 
    "linux-2.6.23/kernel/sched_rt.c", 
    [
      "Index: linux-2.6.23/kernel/sched_rt.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/sched_rt.c", 
      "+++ linux-2.6.23/kernel/sched_rt.c", 
      "@@ -7,7 +7,7 @@", 
      "* Update the current task's runtime statistics. Skip current tasks that", 
      "* are not in our scheduling class.", 
      "*/", 
      "-static inline void update_curr_rt(struct rq *rq)", 
      "+static void update_curr_rt(struct rq *rq)", 
      "{", 
      "struct task_struct *curr = rq->curr;", 
      "u64 delta_exec;", 
      "@@ -23,6 +23,7 @@ static inline void update_curr_rt(struct", 
      "", 
      "curr->se.sum_exec_runtime += delta_exec;", 
      "curr->se.exec_start = rq->clock;", 
      "+\tcpuacct_charge(curr, delta_exec);", 
      "}", 
      "", 
      "static void enqueue_task_rt(struct rq *rq, struct task_struct *p, int wakeup)", 
      "@@ -59,9 +60,9 @@ static void requeue_task_rt(struct rq *r", 
      "}", 
      "", 
      "static void", 
      "-yield_task_rt(struct rq *rq, struct task_struct *p)", 
      "+yield_task_rt(struct rq *rq)", 
      "{", 
      "-\trequeue_task_rt(rq, p);", 
      "+\trequeue_task_rt(rq, rq->curr);", 
      "}", 
      "", 
      "/*", 
      "@@ -98,6 +99,7 @@ static void put_prev_task_rt(struct rq *", 
      "p->se.exec_start = 0;", 
      "}", 
      "", 
      "+#ifdef CONFIG_SMP", 
      "/*", 
      "* Load-balancing iterator. Note: while the runqueue stays locked", 
      "* during the whole iteration, the current task might be", 
      "@@ -172,13 +174,11 @@ static struct task_struct *load_balance_", 
      "", 
      "static unsigned long", 
      "load_balance_rt(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "-\t\t\tunsigned long max_nr_move, unsigned long max_load_move,", 
      "-\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,", 
      "-\t\t\tint *all_pinned, int *this_best_prio)", 
      "+\t\tunsigned long max_load_move,", 
      "+\t\tstruct sched_domain *sd, enum cpu_idle_type idle,", 
      "+\t\tint *all_pinned, int *this_best_prio)", 
      "{", 
      "-\tint nr_moved;", 
      "struct rq_iterator rt_rq_iterator;", 
      "-\tunsigned long load_moved;", 
      "", 
      "rt_rq_iterator.start = load_balance_start_rt;", 
      "rt_rq_iterator.next = load_balance_next_rt;", 
      "@@ -187,15 +187,29 @@ load_balance_rt(struct rq *this_rq, int", 
      "*/", 
      "rt_rq_iterator.arg = busiest;", 
      "", 
      "-\tnr_moved = balance_tasks(this_rq, this_cpu, busiest, max_nr_move,", 
      "-\t\t\tmax_load_move, sd, idle, all_pinned, &load_moved,", 
      "-\t\t\tthis_best_prio, &rt_rq_iterator);", 
      "+\treturn balance_tasks(this_rq, this_cpu, busiest, max_load_move, sd,", 
      "+\t\t\t     idle, all_pinned, this_best_prio, &rt_rq_iterator);", 
      "+}", 
      "+", 
      "+static int", 
      "+move_one_task_rt(struct rq *this_rq, int this_cpu, struct rq *busiest,", 
      "+\t\t struct sched_domain *sd, enum cpu_idle_type idle)", 
      "+{", 
      "+\tstruct rq_iterator rt_rq_iterator;", 
      "+", 
      "+\trt_rq_iterator.start = load_balance_start_rt;", 
      "+\trt_rq_iterator.next = load_balance_next_rt;", 
      "+\trt_rq_iterator.arg = busiest;", 
      "", 
      "-\treturn load_moved;", 
      "+\treturn iter_move_one_task(this_rq, this_cpu, busiest, sd, idle,", 
      "+\t\t\t\t  &rt_rq_iterator);", 
      "}", 
      "+#endif", 
      "", 
      "static void task_tick_rt(struct rq *rq, struct task_struct *p)", 
      "{", 
      "+\tupdate_curr_rt(rq);", 
      "+", 
      "/*", 
      "* RR tasks need a special form of timeslice management.", 
      "* FIFO tasks have no timeslices.", 
      "@@ -206,7 +220,7 @@ static void task_tick_rt(struct rq *rq,", 
      "if (--p->time_slice)", 
      "return;", 
      "", 
      "-\tp->time_slice = static_prio_timeslice(p->static_prio);", 
      "+\tp->time_slice = DEF_TIMESLICE;", 
      "", 
      "/*", 
      "* Requeue to the end of queue if we are not the only element", 
      "@@ -218,7 +232,15 @@ static void task_tick_rt(struct rq *rq,", 
      "}", 
      "}", 
      "", 
      "-static struct sched_class rt_sched_class __read_mostly = {", 
      "+static void set_curr_task_rt(struct rq *rq)", 
      "+{", 
      "+\tstruct task_struct *p = rq->curr;", 
      "+", 
      "+\tp->se.exec_start = rq->clock;", 
      "+}", 
      "+", 
      "+const struct sched_class rt_sched_class = {", 
      "+\t.next\t\t\t= &fair_sched_class,", 
      ".enqueue_task\t\t= enqueue_task_rt,", 
      ".dequeue_task\t\t= dequeue_task_rt,", 
      ".yield_task\t\t= yield_task_rt,", 
      "@@ -228,7 +250,11 @@ static struct sched_class rt_sched_class", 
      ".pick_next_task\t\t= pick_next_task_rt,", 
      ".put_prev_task\t\t= put_prev_task_rt,", 
      "", 
      "+#ifdef CONFIG_SMP", 
      ".load_balance\t\t= load_balance_rt,", 
      "+\t.move_one_task\t\t= move_one_task_rt,", 
      "+#endif", 
      "", 
      "+\t.set_curr_task          = set_curr_task_rt,", 
      ".task_tick\t\t= task_tick_rt,", 
      "};"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/sched_stats.h", 
    "linux-2.6.23/kernel/sched_stats.h", 
    [
      "Index: linux-2.6.23/kernel/sched_stats.h", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/sched_stats.h", 
      "+++ linux-2.6.23/kernel/sched_stats.h", 
      "@@ -16,18 +16,18 @@ static int show_schedstat(struct seq_fil", 
      "struct rq *rq = cpu_rq(cpu);", 
      "#ifdef CONFIG_SMP", 
      "struct sched_domain *sd;", 
      "-\t\tint dcnt = 0;", 
      "+\t\tint dcount = 0;", 
      "#endif", 
      "", 
      "/* runqueue-specific stats */", 
      "seq_printf(seq,", 
      "-\t\t    \"cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %llu %llu %lu\",", 
      "+\t\t    \"cpu%d %u %u %u %u %u %u %u %u %u %llu %llu %lu\",", 
      "cpu, rq->yld_both_empty,", 
      "-\t\t    rq->yld_act_empty, rq->yld_exp_empty, rq->yld_cnt,", 
      "-\t\t    rq->sched_switch, rq->sched_cnt, rq->sched_goidle,", 
      "-\t\t    rq->ttwu_cnt, rq->ttwu_local,", 
      "+\t\t    rq->yld_act_empty, rq->yld_exp_empty, rq->yld_count,", 
      "+\t\t    rq->sched_switch, rq->sched_count, rq->sched_goidle,", 
      "+\t\t    rq->ttwu_count, rq->ttwu_local,", 
      "rq->rq_sched_info.cpu_time,", 
      "-\t\t    rq->rq_sched_info.run_delay, rq->rq_sched_info.pcnt);", 
      "+\t\t    rq->rq_sched_info.run_delay, rq->rq_sched_info.pcount);", 
      "", 
      "seq_printf(seq, \"\\n\");", 
      "", 
      "@@ -39,12 +39,11 @@ static int show_schedstat(struct seq_fil", 
      "char mask_str[NR_CPUS];", 
      "", 
      "cpumask_scnprintf(mask_str, NR_CPUS, sd->span);", 
      "-\t\t\tseq_printf(seq, \"domain%d %s\", dcnt++, mask_str);", 
      "+\t\t\tseq_printf(seq, \"domain%d %s\", dcount++, mask_str);", 
      "for (itype = CPU_IDLE; itype < CPU_MAX_IDLE_TYPES;", 
      "itype++) {", 
      "-\t\t\t\tseq_printf(seq, \" %lu %lu %lu %lu %lu %lu %lu \"", 
      "-\t\t\t\t\t\t\"%lu\",", 
      "-\t\t\t\t    sd->lb_cnt[itype],", 
      "+\t\t\t\tseq_printf(seq, \" %u %u %u %u %u %u %u %u\",", 
      "+\t\t\t\t    sd->lb_count[itype],", 
      "sd->lb_balanced[itype],", 
      "sd->lb_failed[itype],", 
      "sd->lb_imbalance[itype],", 
      "@@ -53,11 +52,11 @@ static int show_schedstat(struct seq_fil", 
      "sd->lb_nobusyq[itype],", 
      "sd->lb_nobusyg[itype]);", 
      "}", 
      "-\t\t\tseq_printf(seq, \" %lu %lu %lu %lu %lu %lu %lu %lu %lu\"", 
      "-\t\t\t    \" %lu %lu %lu\\n\",", 
      "-\t\t\t    sd->alb_cnt, sd->alb_failed, sd->alb_pushed,", 
      "-\t\t\t    sd->sbe_cnt, sd->sbe_balanced, sd->sbe_pushed,", 
      "-\t\t\t    sd->sbf_cnt, sd->sbf_balanced, sd->sbf_pushed,", 
      "+\t\t\tseq_printf(seq,", 
      "+\t\t\t\t   \" %u %u %u %u %u %u %u %u %u %u %u %u\\n\",", 
      "+\t\t\t    sd->alb_count, sd->alb_failed, sd->alb_pushed,", 
      "+\t\t\t    sd->sbe_count, sd->sbe_balanced, sd->sbe_pushed,", 
      "+\t\t\t    sd->sbf_count, sd->sbf_balanced, sd->sbf_pushed,", 
      "sd->ttwu_wake_remote, sd->ttwu_move_affine,", 
      "sd->ttwu_move_balance);", 
      "}", 
      "@@ -101,7 +100,7 @@ rq_sched_info_arrive(struct rq *rq, unsi", 
      "{", 
      "if (rq) {", 
      "rq->rq_sched_info.run_delay += delta;", 
      "-\t\trq->rq_sched_info.pcnt++;", 
      "+\t\trq->rq_sched_info.pcount++;", 
      "}", 
      "}", 
      "", 
      "@@ -157,14 +156,14 @@ static inline void sched_info_dequeued(s", 
      "*/", 
      "static void sched_info_arrive(struct task_struct *t)", 
      "{", 
      "-\tunsigned long long now = sched_clock(), delta = 0;", 
      "+\tunsigned long long now = task_rq(t)->clock, delta = 0;", 
      "", 
      "if (t->sched_info.last_queued)", 
      "delta = now - t->sched_info.last_queued;", 
      "sched_info_dequeued(t);", 
      "t->sched_info.run_delay += delta;", 
      "t->sched_info.last_arrival = now;", 
      "-\tt->sched_info.pcnt++;", 
      "+\tt->sched_info.pcount++;", 
      "", 
      "rq_sched_info_arrive(task_rq(t), delta);", 
      "}", 
      "@@ -188,7 +187,7 @@ static inline void sched_info_queued(str", 
      "{", 
      "if (unlikely(sched_info_on()))", 
      "if (!t->sched_info.last_queued)", 
      "-\t\t\tt->sched_info.last_queued = sched_clock();", 
      "+\t\t\tt->sched_info.last_queued = task_rq(t)->clock;", 
      "}", 
      "", 
      "/*", 
      "@@ -197,7 +196,8 @@ static inline void sched_info_queued(str", 
      "*/", 
      "static inline void sched_info_depart(struct task_struct *t)", 
      "{", 
      "-\tunsigned long long delta = sched_clock() - t->sched_info.last_arrival;", 
      "+\tunsigned long long delta = task_rq(t)->clock -", 
      "+\t\t\t\t\tt->sched_info.last_arrival;", 
      "", 
      "t->sched_info.cpu_time += delta;", 
      "rq_sched_info_depart(task_rq(t), delta);"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/sysctl.c", 
    "linux-2.6.23/kernel/sysctl.c", 
    [
      "Index: linux-2.6.23/kernel/sysctl.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/sysctl.c", 
      "+++ linux-2.6.23/kernel/sysctl.c", 
      "@@ -216,12 +216,12 @@ static ctl_table root_table[] = {", 
      "", 
      "#ifdef CONFIG_SCHED_DEBUG", 
      "static unsigned long min_sched_granularity_ns = 100000;\t\t/* 100 usecs */", 
      "-static unsigned long max_sched_granularity_ns = 1000000000;\t/* 1 second */", 
      "+static unsigned long max_sched_granularity_ns = NSEC_PER_SEC;\t/* 1 second */", 
      "static unsigned long min_wakeup_granularity_ns;\t\t\t/* 0 usecs */", 
      "-static unsigned long max_wakeup_granularity_ns = 1000000000;\t/* 1 second */", 
      "+static unsigned long max_wakeup_granularity_ns = NSEC_PER_SEC;\t/* 1 second */", 
      "#endif", 
      "", 
      "-static ctl_table kern_table[] = {", 
      "+static struct ctl_table kern_table[] = {", 
      "#ifdef CONFIG_SCHED_DEBUG", 
      "{", 
      ".ctl_name\t= CTL_UNNUMBERED,", 
      "@@ -229,7 +229,7 @@ static ctl_table kern_table[] = {", 
      ".data\t\t= &sysctl_sched_min_granularity,", 
      ".maxlen\t\t= sizeof(unsigned int),", 
      ".mode\t\t= 0644,", 
      "-\t\t.proc_handler\t= &proc_dointvec_minmax,", 
      "+\t\t.proc_handler\t= &sched_nr_latency_handler,", 
      ".strategy\t= &sysctl_intvec,", 
      ".extra1\t\t= &min_sched_granularity_ns,", 
      ".extra2\t\t= &max_sched_granularity_ns,", 
      "@@ -240,7 +240,7 @@ static ctl_table kern_table[] = {", 
      ".data\t\t= &sysctl_sched_latency,", 
      ".maxlen\t\t= sizeof(unsigned int),", 
      ".mode\t\t= 0644,", 
      "-\t\t.proc_handler\t= &proc_dointvec_minmax,", 
      "+\t\t.proc_handler\t= &sched_nr_latency_handler,", 
      ".strategy\t= &sysctl_intvec,", 
      ".extra1\t\t= &min_sched_granularity_ns,", 
      ".extra2\t\t= &max_sched_granularity_ns,", 
      "@@ -269,43 +269,39 @@ static ctl_table kern_table[] = {", 
      "},", 
      "{", 
      ".ctl_name\t= CTL_UNNUMBERED,", 
      "-\t\t.procname\t= \"sched_stat_granularity_ns\",", 
      "-\t\t.data\t\t= &sysctl_sched_stat_granularity,", 
      "+\t\t.procname\t= \"sched_child_runs_first\",", 
      "+\t\t.data\t\t= &sysctl_sched_child_runs_first,", 
      ".maxlen\t\t= sizeof(unsigned int),", 
      ".mode\t\t= 0644,", 
      "-\t\t.proc_handler\t= &proc_dointvec_minmax,", 
      "-\t\t.strategy\t= &sysctl_intvec,", 
      "-\t\t.extra1\t\t= &min_wakeup_granularity_ns,", 
      "-\t\t.extra2\t\t= &max_wakeup_granularity_ns,", 
      "+\t\t.proc_handler\t= &proc_dointvec,", 
      "},", 
      "{", 
      ".ctl_name\t= CTL_UNNUMBERED,", 
      "-\t\t.procname\t= \"sched_runtime_limit_ns\",", 
      "-\t\t.data\t\t= &sysctl_sched_runtime_limit,", 
      "+\t\t.procname\t= \"sched_features\",", 
      "+\t\t.data\t\t= &sysctl_sched_features,", 
      ".maxlen\t\t= sizeof(unsigned int),", 
      ".mode\t\t= 0644,", 
      "-\t\t.proc_handler\t= &proc_dointvec_minmax,", 
      "-\t\t.strategy\t= &sysctl_intvec,", 
      "-\t\t.extra1\t\t= &min_sched_granularity_ns,", 
      "-\t\t.extra2\t\t= &max_sched_granularity_ns,", 
      "+\t\t.proc_handler\t= &proc_dointvec,", 
      "},", 
      "{", 
      ".ctl_name\t= CTL_UNNUMBERED,", 
      "-\t\t.procname\t= \"sched_child_runs_first\",", 
      "-\t\t.data\t\t= &sysctl_sched_child_runs_first,", 
      "+\t\t.procname\t= \"sched_migration_cost\",", 
      "+\t\t.data\t\t= &sysctl_sched_migration_cost,", 
      ".maxlen\t\t= sizeof(unsigned int),", 
      ".mode\t\t= 0644,", 
      ".proc_handler\t= &proc_dointvec,", 
      "},", 
      "+#if defined(CONFIG_FAIR_GROUP_SCHED) && defined(CONFIG_SMP)", 
      "{", 
      ".ctl_name\t= CTL_UNNUMBERED,", 
      "-\t\t.procname\t= \"sched_features\",", 
      "-\t\t.data\t\t= &sysctl_sched_features,", 
      "+\t\t.procname\t= \"sched_nr_migrate\",", 
      "+\t\t.data\t\t= &sysctl_sched_nr_migrate,", 
      ".maxlen\t\t= sizeof(unsigned int),", 
      "-\t\t.mode\t\t= 0644,", 
      "+\t\t.mode\t\t= 644,", 
      ".proc_handler\t= &proc_dointvec,", 
      "},", 
      "#endif", 
      "+#endif", 
      "{", 
      ".ctl_name\t= CTL_UNNUMBERED,", 
      ".procname\t= \"sched_compat_yield\","
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/timer.c", 
    "linux-2.6.23/kernel/timer.c", 
    [
      "Index: linux-2.6.23/kernel/timer.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/timer.c", 
      "+++ linux-2.6.23/kernel/timer.c", 
      "@@ -830,10 +830,13 @@ void update_process_times(int user_tick)", 
      "int cpu = smp_processor_id();", 
      "", 
      "/* Note: this timer irq context must be accounted for as well. */", 
      "-\tif (user_tick)", 
      "+\tif (user_tick) {", 
      "account_user_time(p, jiffies_to_cputime(1));", 
      "-\telse", 
      "+\t\taccount_user_time_scaled(p, jiffies_to_cputime(1));", 
      "+\t} else {", 
      "account_system_time(p, HARDIRQ_OFFSET, jiffies_to_cputime(1));", 
      "+\t\taccount_system_time_scaled(p, jiffies_to_cputime(1));", 
      "+\t}", 
      "run_local_timers();", 
      "if (rcu_pending(cpu))", 
      "rcu_check_callbacks(cpu, user_tick);"
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/tsacct.c", 
    "linux-2.6.23/kernel/tsacct.c", 
    [
      "Index: linux-2.6.23/kernel/tsacct.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/tsacct.c", 
      "+++ linux-2.6.23/kernel/tsacct.c", 
      "@@ -62,6 +62,10 @@ void bacct_add_tsk(struct taskstats *sta", 
      "rcu_read_unlock();", 
      "stats->ac_utime\t = cputime_to_msecs(tsk->utime) * USEC_PER_MSEC;", 
      "stats->ac_stime\t = cputime_to_msecs(tsk->stime) * USEC_PER_MSEC;", 
      "+\tstats->ac_utimescaled =", 
      "+\t\tcputime_to_msecs(tsk->utimescaled) * USEC_PER_MSEC;", 
      "+\tstats->ac_stimescaled =", 
      "+\t\tcputime_to_msecs(tsk->stimescaled) * USEC_PER_MSEC;", 
      "stats->ac_minflt = tsk->min_flt;", 
      "stats->ac_majflt = tsk->maj_flt;", 
      ""
    ]
  ], 
  [
    "linux-2.6.23.orig/kernel/user.c", 
    "linux-2.6.23/kernel/user.c", 
    [
      "Index: linux-2.6.23/kernel/user.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/kernel/user.c", 
      "+++ linux-2.6.23/kernel/user.c", 
      "@@ -50,12 +50,16 @@ struct user_struct root_user = {", 
      ".uid_keyring\t= &root_user_keyring,", 
      ".session_keyring = &root_session_keyring,", 
      "#endif", 
      "+#ifdef CONFIG_FAIR_USER_SCHED", 
      "+\t.tg\t\t= &init_task_group,", 
      "+#endif", 
      "};", 
      "", 
      "/*", 
      "* These routines must be called with the uidhash spinlock held!", 
      "*/", 
      "-static inline void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)", 
      "+static inline void uid_hash_insert(struct user_struct *up,", 
      "+\t\t\t\t\t\tstruct hlist_head *hashent)", 
      "{", 
      "hlist_add_head(&up->uidhash_node, hashent);", 
      "}", 
      "@@ -65,13 +69,14 @@ static inline void uid_hash_remove(struc", 
      "hlist_del_init(&up->uidhash_node);", 
      "}", 
      "", 
      "-static inline struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)", 
      "+static inline struct user_struct *uid_hash_find(uid_t uid,", 
      "+\t\t\t\t\t\tstruct hlist_head *hashent)", 
      "{", 
      "struct user_struct *user;", 
      "struct hlist_node *h;", 
      "", 
      "hlist_for_each_entry(user, h, hashent, uidhash_node) {", 
      "-\t\tif(user->uid == uid) {", 
      "+\t\tif (user->uid == uid) {", 
      "atomic_inc(&user->__count);", 
      "return user;", 
      "}", 
      "@@ -80,6 +85,203 @@ static inline struct user_struct *uid_ha", 
      "return NULL;", 
      "}", 
      "", 
      "+#ifdef CONFIG_FAIR_USER_SCHED", 
      "+", 
      "+static struct kobject uids_kobject; /* represents /sys/kernel/uids directory */", 
      "+static DEFINE_MUTEX(uids_mutex);", 
      "+", 
      "+static void sched_destroy_user(struct user_struct *up)", 
      "+{", 
      "+\tsched_destroy_group(up->tg);", 
      "+}", 
      "+", 
      "+static int sched_create_user(struct user_struct *up)", 
      "+{", 
      "+\tint rc = 0;", 
      "+", 
      "+\tup->tg = sched_create_group();", 
      "+\tif (IS_ERR(up->tg))", 
      "+\t\trc = -ENOMEM;", 
      "+", 
      "+\treturn rc;", 
      "+}", 
      "+", 
      "+static void sched_switch_user(struct task_struct *p)", 
      "+{", 
      "+\tsched_move_task(p);", 
      "+}", 
      "+", 
      "+static inline void uids_mutex_lock(void)", 
      "+{", 
      "+\tmutex_lock(&uids_mutex);", 
      "+}", 
      "+", 
      "+static inline void uids_mutex_unlock(void)", 
      "+{", 
      "+\tmutex_unlock(&uids_mutex);", 
      "+}", 
      "+", 
      "+/* return cpu shares held by the user */", 
      "+ssize_t cpu_shares_show(struct kset *kset, char *buffer)", 
      "+{", 
      "+\tstruct user_struct *up = container_of(kset, struct user_struct, kset);", 
      "+", 
      "+\treturn sprintf(buffer, \"%lu\\n\", sched_group_shares(up->tg));", 
      "+}", 
      "+", 
      "+/* modify cpu shares held by the user */", 
      "+ssize_t cpu_shares_store(struct kset *kset, const char *buffer, size_t size)", 
      "+{", 
      "+\tstruct user_struct *up = container_of(kset, struct user_struct, kset);", 
      "+\tunsigned long shares;", 
      "+\tint rc;", 
      "+", 
      "+\tsscanf(buffer, \"%lu\", &shares);", 
      "+", 
      "+\trc = sched_group_set_shares(up->tg, shares);", 
      "+", 
      "+\treturn (rc ? rc : size);", 
      "+}", 
      "+", 
      "+static void user_attr_init(struct subsys_attribute *sa, char *name, int mode)", 
      "+{", 
      "+\tsa->attr.name = name; sa->attr.owner = NULL;", 
      "+\tsa->attr.mode = mode;", 
      "+\tsa->show = cpu_shares_show;", 
      "+\tsa->store = cpu_shares_store;", 
      "+}", 
      "+", 
      "+/* Create \"/sys/kernel/uids/<uid>\" directory and", 
      "+ *  \"/sys/kernel/uids/<uid>/cpu_share\" file for this user.", 
      "+ */", 
      "+static int user_kobject_create(struct user_struct *up)", 
      "+{", 
      "+\tstruct kset *kset = &up->kset;", 
      "+\tstruct kobject *kobj = &kset->kobj;", 
      "+\tint error;", 
      "+", 
      "+\tmemset(kset, 0, sizeof(struct kset));", 
      "+\tkobj->parent = &uids_kobject;\t/* create under /sys/kernel/uids dir */", 
      "+\tkobject_set_name(kobj, \"%d\", up->uid);", 
      "+\tkset_init(kset);", 
      "+\tuser_attr_init(&up->user_attr, \"cpu_share\", 0644);", 
      "+", 
      "+\terror = kobject_add(kobj);", 
      "+\tif (error)", 
      "+\t\tgoto done;", 
      "+", 
      "+\terror = sysfs_create_file(kobj, &up->user_attr.attr);", 
      "+\tif (error)", 
      "+\t\tkobject_del(kobj);", 
      "+", 
      "+\tkobject_uevent(kobj, KOBJ_ADD);", 
      "+", 
      "+done:", 
      "+\treturn error;", 
      "+}", 
      "+", 
      "+/* create these in sysfs filesystem:", 
      "+ * \t\"/sys/kernel/uids\" directory", 
      "+ * \t\"/sys/kernel/uids/0\" directory (for root user)", 
      "+ * \t\"/sys/kernel/uids/0/cpu_share\" file (for root user)", 
      "+ */", 
      "+int __init uids_kobject_init(void)", 
      "+{", 
      "+\tint error;", 
      "+", 
      "+\t/* create under /sys/kernel dir */", 
      "+\tuids_kobject.parent = &kernel_subsys.kobj;", 
      "+\tuids_kobject.kset = &kernel_subsys;", 
      "+\tkobject_set_name(&uids_kobject, \"uids\");", 
      "+\tkobject_init(&uids_kobject);", 
      "+", 
      "+\terror = kobject_add(&uids_kobject);", 
      "+\tif (!error)", 
      "+\t\terror = user_kobject_create(&root_user);", 
      "+", 
      "+\treturn error;", 
      "+}", 
      "+", 
      "+/* work function to remove sysfs directory for a user and free up", 
      "+ * corresponding structures.", 
      "+ */", 
      "+static void remove_user_sysfs_dir(struct work_struct *w)", 
      "+{", 
      "+\tstruct user_struct *up = container_of(w, struct user_struct, work);", 
      "+\tstruct kobject *kobj = &up->kset.kobj;", 
      "+\tunsigned long flags;", 
      "+\tint remove_user = 0;", 
      "+", 
      "+\t/* Make uid_hash_remove() + sysfs_remove_file() + kobject_del()", 
      "+\t * atomic.", 
      "+\t */", 
      "+\tuids_mutex_lock();", 
      "+", 
      "+\tlocal_irq_save(flags);", 
      "+", 
      "+\tif (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {", 
      "+\t\tuid_hash_remove(up);", 
      "+\t\tremove_user = 1;", 
      "+\t\tspin_unlock_irqrestore(&uidhash_lock, flags);", 
      "+\t} else {", 
      "+\t\tlocal_irq_restore(flags);", 
      "+\t}", 
      "+", 
      "+\tif (!remove_user)", 
      "+\t\tgoto done;", 
      "+", 
      "+\tsysfs_remove_file(kobj, &up->user_attr.attr);", 
      "+\tkobject_uevent(kobj, KOBJ_REMOVE);", 
      "+\tkobject_del(kobj);", 
      "+", 
      "+\tsched_destroy_user(up);", 
      "+\tkey_put(up->uid_keyring);", 
      "+\tkey_put(up->session_keyring);", 
      "+\tkmem_cache_free(uid_cachep, up);", 
      "+", 
      "+done:", 
      "+\tuids_mutex_unlock();", 
      "+}", 
      "+", 
      "+/* IRQs are disabled and uidhash_lock is held upon function entry.", 
      "+ * IRQ state (as stored in flags) is restored and uidhash_lock released", 
      "+ * upon function exit.", 
      "+ */", 
      "+static inline void free_user(struct user_struct *up, unsigned long flags)", 
      "+{", 
      "+\t/* restore back the count */", 
      "+\tatomic_inc(&up->__count);", 
      "+\tspin_unlock_irqrestore(&uidhash_lock, flags);", 
      "+", 
      "+\tINIT_WORK(&up->work, remove_user_sysfs_dir);", 
      "+\tschedule_work(&up->work);", 
      "+}", 
      "+", 
      "+#else\t/* CONFIG_FAIR_USER_SCHED */", 
      "+", 
      "+static void sched_destroy_user(struct user_struct *up) { }", 
      "+static int sched_create_user(struct user_struct *up) { return 0; }", 
      "+static void sched_switch_user(struct task_struct *p) { }", 
      "+static inline int user_kobject_create(struct user_struct *up) { return 0; }", 
      "+static inline void uids_mutex_lock(void) { }", 
      "+static inline void uids_mutex_unlock(void) { }", 
      "+", 
      "+/* IRQs are disabled and uidhash_lock is held upon function entry.", 
      "+ * IRQ state (as stored in flags) is restored and uidhash_lock released", 
      "+ * upon function exit.", 
      "+ */", 
      "+static inline void free_user(struct user_struct *up, unsigned long flags)", 
      "+{", 
      "+\tuid_hash_remove(up);", 
      "+\tspin_unlock_irqrestore(&uidhash_lock, flags);", 
      "+\tsched_destroy_user(up);", 
      "+\tkey_put(up->uid_keyring);", 
      "+\tkey_put(up->session_keyring);", 
      "+\tkmem_cache_free(uid_cachep, up);", 
      "+}", 
      "+", 
      "+#endif\t/* CONFIG_FAIR_USER_SCHED */", 
      "+", 
      "/*", 
      "* Locate the user_struct for the passed UID.  If found, take a ref on it.  The", 
      "* caller must undo that ref with free_uid().", 
      "@@ -106,15 +308,10 @@ void free_uid(struct user_struct *up)", 
      "return;", 
      "", 
      "local_irq_save(flags);", 
      "-\tif (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {", 
      "-\t\tuid_hash_remove(up);", 
      "-\t\tspin_unlock_irqrestore(&uidhash_lock, flags);", 
      "-\t\tkey_put(up->uid_keyring);", 
      "-\t\tkey_put(up->session_keyring);", 
      "-\t\tkmem_cache_free(uid_cachep, up);", 
      "-\t} else {", 
      "+\tif (atomic_dec_and_lock(&up->__count, &uidhash_lock))", 
      "+\t\tfree_user(up, flags);", 
      "+\telse", 
      "local_irq_restore(flags);", 
      "-\t}", 
      "}", 
      "", 
      "struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)", 
      "@@ -122,6 +319,11 @@ struct user_struct * alloc_uid(struct us", 
      "struct hlist_head *hashent = uidhashentry(ns, uid);", 
      "struct user_struct *up;", 
      "", 
      "+\t/* Make uid_hash_find() + user_kobject_create() + uid_hash_insert()", 
      "+\t * atomic.", 
      "+\t */", 
      "+\tuids_mutex_lock();", 
      "+", 
      "spin_lock_irq(&uidhash_lock);", 
      "up = uid_hash_find(uid, hashent);", 
      "spin_unlock_irq(&uidhash_lock);", 
      "@@ -150,6 +352,22 @@ struct user_struct * alloc_uid(struct us", 
      "return NULL;", 
      "}", 
      "", 
      "+\t\tif (sched_create_user(new) < 0) {", 
      "+\t\t\tkey_put(new->uid_keyring);", 
      "+\t\t\tkey_put(new->session_keyring);", 
      "+\t\t\tkmem_cache_free(uid_cachep, new);", 
      "+\t\t\treturn NULL;", 
      "+\t\t}", 
      "+", 
      "+\t\tif (user_kobject_create(new)) {", 
      "+\t\t\tsched_destroy_user(new);", 
      "+\t\t\tkey_put(new->uid_keyring);", 
      "+\t\t\tkey_put(new->session_keyring);", 
      "+\t\t\tkmem_cache_free(uid_cachep, new);", 
      "+\t\t\tuids_mutex_unlock();", 
      "+\t\t\treturn NULL;", 
      "+\t\t}", 
      "+", 
      "/*", 
      "* Before adding this, check whether we raced", 
      "* on adding the same user already..", 
      "@@ -157,6 +375,11 @@ struct user_struct * alloc_uid(struct us", 
      "spin_lock_irq(&uidhash_lock);", 
      "up = uid_hash_find(uid, hashent);", 
      "if (up) {", 
      "+\t\t\t/* This case is not possible when CONFIG_FAIR_USER_SCHED", 
      "+\t\t\t * is defined, since we serialize alloc_uid() using", 
      "+\t\t\t * uids_mutex. Hence no need to call", 
      "+\t\t\t * sched_destroy_user() or remove_user_sysfs_dir().", 
      "+\t\t\t */", 
      "key_put(new->uid_keyring);", 
      "key_put(new->session_keyring);", 
      "kmem_cache_free(uid_cachep, new);", 
      "@@ -167,6 +390,9 @@ struct user_struct * alloc_uid(struct us", 
      "spin_unlock_irq(&uidhash_lock);", 
      "", 
      "}", 
      "+", 
      "+\tuids_mutex_unlock();", 
      "+", 
      "return up;", 
      "}", 
      "", 
      "@@ -184,6 +410,7 @@ void switch_uid(struct user_struct *new_", 
      "atomic_dec(&old_user->processes);", 
      "switch_uid_keyring(new_user);", 
      "current->user = new_user;", 
      "+\tsched_switch_user(current);", 
      "", 
      "/*", 
      "* We need to synchronize with __sigqueue_alloc()"
    ]
  ], 
  [
    "linux-2.6.23.orig/mm/memory_hotplug.c", 
    "linux-2.6.23/mm/memory_hotplug.c", 
    [
      "Index: linux-2.6.23/mm/memory_hotplug.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/mm/memory_hotplug.c", 
      "+++ linux-2.6.23/mm/memory_hotplug.c", 
      "@@ -217,6 +217,10 @@ int online_pages(unsigned long pfn, unsi", 
      "zone->zone_pgdat->node_present_pages += onlined_pages;", 
      "", 
      "setup_per_zone_pages_min();", 
      "+\tif (onlined_pages) {", 
      "+\t\tkswapd_run(zone_to_nid(zone));", 
      "+\t\tnode_set_state(zone_to_nid(zone), N_HIGH_MEMORY);", 
      "+\t}", 
      "", 
      "if (need_zonelists_rebuild)", 
      "build_all_zonelists();", 
      "@@ -271,9 +275,6 @@ int add_memory(int nid, u64 start, u64 s", 
      "if (!pgdat)", 
      "return -ENOMEM;", 
      "new_pgdat = 1;", 
      "-\t\tret = kswapd_run(nid);", 
      "-\t\tif (ret)", 
      "-\t\t\tgoto error;", 
      "}", 
      "", 
      "/* call arch's memory hotadd */"
    ]
  ], 
  [
    "linux-2.6.23.orig/mm/page_alloc.c", 
    "linux-2.6.23/mm/page_alloc.c", 
    [
      "Index: linux-2.6.23/mm/page_alloc.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/mm/page_alloc.c", 
      "+++ linux-2.6.23/mm/page_alloc.c", 
      "@@ -47,13 +47,21 @@", 
      "#include \"internal.h\"", 
      "", 
      "/*", 
      "- * MCD - HACK: Find somewhere to initialize this EARLY, or make this", 
      "- * initializer cleaner", 
      "+ * Array of node states.", 
      "*/", 
      "-nodemask_t node_online_map __read_mostly = { { [0] = 1UL } };", 
      "-EXPORT_SYMBOL(node_online_map);", 
      "-nodemask_t node_possible_map __read_mostly = NODE_MASK_ALL;", 
      "-EXPORT_SYMBOL(node_possible_map);", 
      "+nodemask_t node_states[NR_NODE_STATES] __read_mostly = {", 
      "+\t[N_POSSIBLE] = NODE_MASK_ALL,", 
      "+\t[N_ONLINE] = { { [0] = 1UL } },", 
      "+#ifndef CONFIG_NUMA", 
      "+\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },", 
      "+#ifdef CONFIG_HIGHMEM", 
      "+\t[N_HIGH_MEMORY] = { { [0] = 1UL } },", 
      "+#endif", 
      "+\t[N_CPU] = { { [0] = 1UL } },", 
      "+#endif\t/* NUMA */", 
      "+};", 
      "+EXPORT_SYMBOL(node_states);", 
      "+", 
      "unsigned long totalram_pages __read_mostly;", 
      "unsigned long totalreserve_pages __read_mostly;", 
      "long nr_swap_pages;", 
      "@@ -2077,14 +2085,35 @@ static void build_zonelist_cache(pg_data", 
      "", 
      "#endif\t/* CONFIG_NUMA */", 
      "", 
      "+/* Any regular memory on that node ? */", 
      "+static void check_for_regular_memory(pg_data_t *pgdat)", 
      "+{", 
      "+#ifdef CONFIG_HIGHMEM", 
      "+\tenum zone_type zone_type;", 
      "+", 
      "+\tfor (zone_type = 0; zone_type <= ZONE_NORMAL; zone_type++) {", 
      "+\t\tstruct zone *zone = &pgdat->node_zones[zone_type];", 
      "+\t\tif (zone->present_pages)", 
      "+\t\t\tnode_set_state(zone_to_nid(zone), N_NORMAL_MEMORY);", 
      "+\t}", 
      "+#endif", 
      "+}", 
      "+", 
      "/* return values int ....just for stop_machine_run() */", 
      "static int __build_all_zonelists(void *dummy)", 
      "{", 
      "int nid;", 
      "", 
      "for_each_online_node(nid) {", 
      "-\t\tbuild_zonelists(NODE_DATA(nid));", 
      "-\t\tbuild_zonelist_cache(NODE_DATA(nid));", 
      "+\t\tpg_data_t *pgdat = NODE_DATA(nid);", 
      "+", 
      "+\t\tbuild_zonelists(pgdat);", 
      "+\t\tbuild_zonelist_cache(pgdat);", 
      "+", 
      "+\t\t/* Any memory on that node */", 
      "+\t\tif (pgdat->node_present_pages)", 
      "+\t\t\tnode_set_state(nid, N_HIGH_MEMORY);", 
      "+\t\tcheck_for_regular_memory(pgdat);", 
      "}", 
      "return 0;", 
      "}", 
      "@@ -2329,6 +2358,9 @@ static struct per_cpu_pageset boot_pages", 
      "static int __cpuinit process_zones(int cpu)", 
      "{", 
      "struct zone *zone, *dzone;", 
      "+\tint node = cpu_to_node(cpu);", 
      "+", 
      "+\tnode_set_state(node, N_CPU);\t/* this node has a cpu */", 
      "", 
      "for_each_zone(zone) {", 
      "", 
      "@@ -2336,7 +2368,7 @@ static int __cpuinit process_zones(int c", 
      "continue;", 
      "", 
      "zone_pcp(zone, cpu) = kmalloc_node(sizeof(struct per_cpu_pageset),", 
      "-\t\t\t\t\t GFP_KERNEL, cpu_to_node(cpu));", 
      "+\t\t\t\t\t GFP_KERNEL, node);", 
      "if (!zone_pcp(zone, cpu))", 
      "goto bad;", 
      ""
    ]
  ], 
  [
    "linux-2.6.23.orig/mm/vmscan.c", 
    "linux-2.6.23/mm/vmscan.c", 
    [
      "Index: linux-2.6.23/mm/vmscan.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/mm/vmscan.c", 
      "+++ linux-2.6.23/mm/vmscan.c", 
      "@@ -1847,7 +1847,6 @@ static int __zone_reclaim(struct zone *z", 
      "", 
      "int zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)", 
      "{", 
      "-\tcpumask_t mask;", 
      "int node_id;", 
      "", 
      "/*", 
      "@@ -1884,8 +1883,7 @@ int zone_reclaim(struct zone *zone, gfp_", 
      "* as wide as possible.", 
      "*/", 
      "node_id = zone_to_nid(zone);", 
      "-\tmask = node_to_cpumask(node_id);", 
      "-\tif (!cpus_empty(mask) && node_id != numa_node_id())", 
      "+\tif (node_state(node_id, N_CPU) && node_id != numa_node_id())", 
      "return 0;", 
      "return __zone_reclaim(zone, gfp_mask, order);", 
      "}"
    ]
  ], 
  [
    "linux-2.6.23.orig/net/unix/af_unix.c", 
    "linux-2.6.23/net/unix/af_unix.c", 
    [
      "Index: linux-2.6.23/net/unix/af_unix.c", 
      "===================================================================", 
      "--- linux-2.6.23.orig/net/unix/af_unix.c", 
      "+++ linux-2.6.23/net/unix/af_unix.c", 
      "@@ -333,7 +333,7 @@ static void unix_write_space(struct sock", 
      "read_lock(&sk->sk_callback_lock);", 
      "if (unix_writable(sk)) {", 
      "if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))", 
      "-\t\t\twake_up_interruptible(sk->sk_sleep);", 
      "+\t\t\twake_up_interruptible_sync(sk->sk_sleep);", 
      "sk_wake_async(sk, 2, POLL_OUT);", 
      "}", 
      "read_unlock(&sk->sk_callback_lock);", 
      "@@ -1642,7 +1642,7 @@ static int unix_dgram_recvmsg(struct kio", 
      "goto out_unlock;", 
      "}", 
      "", 
      "-\twake_up_interruptible(&u->peer_wait);", 
      "+\twake_up_interruptible_sync(&u->peer_wait);", 
      "", 
      "if (msg->msg_name)", 
      "unix_copy_addr(msg, skb->sk);"
    ]
  ]
]